{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"10\">CSIC 5011 Q3 - Q5</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Ng Yui Hong (20434594)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">3. Positive Semi-definiteness</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Recall the definitioin of eigenvalues <br><br>\n",
    "$ A\\vec v = \\lambda \\vec v$ <br> <br>\n",
    "Recall that if a matrix is positive semi-definite, then for all $v$ <br><br>\n",
    "$v^TAv ≥ 0$   <br><br>\n",
    "Combining the two equation <br><br>\n",
    "$v^T\\lambda v ≥ 0$  <br>\n",
    "$\\lambda v^T v ≥ 0$ <br>\n",
    "$\\lambda ||v||^2 ≥ 0$ <br>\n",
    "$\\lambda ≥ 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment: how about the converse (if all e-values are positive, then...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "We First need to prove that for any positive semi definite matrix K, <br><br>\n",
    "$K=R^TR$  <br><br>\n",
    "prove:    <br><br>\n",
    "$ K = V diag(\\lambda_1, \\lambda_2,......)V^T$ &emsp;&emsp;&emsp;&emsp;&emsp; as $K$ is symmetric by eigenvalue decomposition<br>\n",
    "$ K = V diag(\\sqrt\\lambda_1, \\sqrt\\lambda_2,......)diag(\\sqrt\\lambda_1, \\sqrt\\lambda_2,......)V^T$ <br>\n",
    "$ K = [V diag(\\sqrt\\lambda_1, \\sqrt\\lambda_2,......)][Vdiag(\\sqrt\\lambda_1, \\sqrt\\lambda_2,......)]^T$ <br>\n",
    "$K=R^TR$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; where  $R = Vdiag(\\sqrt\\lambda_1, \\sqrt\\lambda_2,......)$ <br><br>\n",
    "\n",
    "Therefore suppose $K=R^TR$<br><br>\n",
    "\n",
    "$K_{ii} = u_i^Tu_i $ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; where $u_i$ is the $i$ th column of $R$ <br>\n",
    "$K_{jj} = u_j^Tu_j $ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; where $u_j$ is the $j$ th column of $R$ <br>\n",
    "$K_{ij} = u_i^Tu_j = u_j^Tu_i$ &nbsp;&emsp;&emsp;&emsp; where $u_i$ and $u_j$ are the $i$ th and $j$ th column of $R$ <br>\n",
    "$K_{ii} + K_{jj} -2K_{ij} = u_i^Tu_i + u_j^Tu_j - u_i^Tu_j - u_j^Tu_i$ <br>\n",
    "$K_{ii} + K_{jj} -2K_{ij} = (u_i^T - u_j^T) (u_i - u_j) = (u_i- u_j)^T (u_i - u_j)$ <br>\n",
    "$K_{ii} + K_{jj} -2K_{ij} = ||u_i - u_j||^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Consider <br><br>\n",
    "$B_\\alpha = - \\frac{1}{2} H_\\alpha D H^T_\\alpha$ <br>\n",
    "$x^TB_\\alpha x = - \\frac{1}{2} x^TH_\\alpha D H^T_\\alpha x$ <br>\n",
    "$x^TB_\\alpha x = - \\frac{1}{2} (H^T_\\alpha x)^T D (H^T_\\alpha x)$ <br>\n",
    "$x^TB_\\alpha x = - \\frac{1}{2} y^T D y$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; where $y = H^T_\\alpha x$<br><br>\n",
    "It remains to prove that $D$ is negative definite, i.e. $D\\preceq 0$ <br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment: try using the definition of H_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) We first prove that summation of positive semi definite matrix is positive semi definite <br><br>\n",
    "$u^TAu \\geq 0$ &emsp;&emsp; and &emsp;&emsp; $u^TBu \\geq 0$ <br> \n",
    "$u^T(A+B)u = u^TAu + u^TBu \\geq 0$ <br><br>\n",
    "Therefore $A+B \\succeq 0$ <br>\n",
    "Then we will prove the Hadamard product of positive semi definite matrix is positive semi definite <br>\n",
    "We first prove the following Hadamard product equality for any vector $a,b$ <br>\n",
    "$ {\\displaystyle (aa^T) \\circ (bb^T) = (a \\circ b)(a \\circ b)^T}$ <br>\n",
    "Prove: <br><br>\n",
    "The $i,j$th entry of LHS of equality is $(a_ia_j)(b_ib_j)$<br>\n",
    "The $i,j$th entry of RHS of equality is $(a_ib_i)(a_jb_j)$<br>\n",
    "The equality thus hold <br><br>\n",
    "Let the eigen-decomposition of $ {\\displaystyle A=\\sum \\mu _{i}a_{i}a_{i}^{\\textsf {T}}}$ and ${\\displaystyle B=\\sum \\nu _{i}b_{i}b_{i}^{\\textsf {T}}}$<br><br>\n",
    "${\\displaystyle A\\circ B}$<br>\n",
    "${\\displaystyle =\\sum _{ij}\\mu _{i}\\nu _{j}\\left(a_{i}a_{i}^{\\textsf {T}}\\right)\\circ \\left(b_{j}b_{j}^{\\textsf {T}}\\right)}$ &emsp;&emsp;&emsp;&nbsp; (Distributivity of Hadamard Product)<br>\n",
    "${\\displaystyle =\\sum _{ij}\\mu _{i}\\nu _{j}\\left(a_{i}\\circ b_{j}\\right)\\left(a_{i}\\circ b_{j}\\right)^{\\textsf {T}}}$ &emsp;&emsp; (above proved equality) <br><br>\n",
    "${\\displaystyle x^T (A\\circ B )x}$<br>\n",
    "${\\displaystyle  =\\sum _{ij}\\mu _{i}\\nu _{j}x^T\\left(a_{i}\\circ b_{j}\\right)\\left(a_{i}\\circ b_{j}\\right)^{\\textsf {T}}x}$<br>\n",
    "${\\displaystyle =\\sum _{ij}\\mu _{i}\\nu _{j} (x^T(a_{i}\\circ b_{j}))^2}$<br>\n",
    "${\\displaystyle \\geq 0}$ <br><br>\n",
    "Thus, Hadamard Product of positve semi definite matrix is posiitive semi definite <br>\n",
    "Reamining prove is skip as it is trival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">4. Square Distance Function</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) No it is not. Consider the Euclidean distance function <br>\n",
    "Take $x=0, y=1/2 and z=1$. Then $|x−z|^2=1$ but $|x−y|^2+|y−z|^2=1/2$. <br>\n",
    "The triangle inequality criteria fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Yes <br>\n",
    "1. &emsp;&emsp;&emsp;\t${\\displaystyle d(x,y)=0\\Leftrightarrow x=y}$ <br>\n",
    "$\\implies {\\displaystyle \\sqrt{d(x,y)}=0\\Leftrightarrow x=y} $ <br><br>\n",
    "2. &emsp;&emsp;&emsp; ${\\displaystyle d(x,y)=d(y,x)}$ <br>\n",
    "$\\implies {\\displaystyle \\sqrt{d(x,y)}=\\sqrt{d(y,x)}}$ <br><br>\n",
    "3. &emsp;&emsp;&emsp; ${\\displaystyle d(x,y)\\leq d(x,z)+d(z,y)}$ <br>\n",
    "$\\implies {\\displaystyle d(x,y)\\leq d(x,z)+d(z,y) + 2 \\sqrt{d(x,z)}}\\sqrt{d(z,y)}$ <br>\n",
    "$\\implies {\\displaystyle \\sqrt{d(x,y)}^2 \\leq (\\sqrt{d(x,z)}+\\sqrt{d(z,y)})^2}$ <br>\n",
    "$\\implies {\\displaystyle \\sqrt{d(x,y)} \\leq \\sqrt{d(x,z)}+\\sqrt{d(z,y)}}$ <br><br>\n",
    "Therefore, $\\sqrt{d(x,y)}$ is a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">5. Singular Value Decomposition</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Existence: <br>\n",
    "We construct a $m*m$ symmetric matrix by $A_1 = AA^T$ and a $n*n$ symmetric matrix by $A_2 = A^TA$ <br>\n",
    "By spectral theorem, <br><br>\n",
    "$ A^TA = V\\Lambda V^T$    <br>\n",
    "where $V$ are Unitary  matrix , $v_{1},v_{2},...v_{r}$ are <b>nornalized</b> orthogonal eigenvectors with non zero eigenvalues  and $\\lambda_{1}, \\lambda_{2} ...\\lambda_{r}$ are corresponding eigenvalues  <br>\n",
    "\n",
    "It is easy to see aforementioned $r$ is just the rank of $A^T A$, which is also the rank of $A$. <br><br>\n",
    "Define ${\\displaystyle u_{i} = \\frac{Av_{i}}{\\sqrt{\\lambda_{i}}}}$ <br>\n",
    "It is easy to show that $u_{i}$ are orthogonal and normalized  by following <br><br>\n",
    "${\\displaystyle u_{i}^T u_{j} = \\frac {(Av_{i})^T(Av_{j})}{\\sqrt{\\lambda_i \\lambda_j}} = \\frac {v_{i}^TA^TAv_{j}} {{\\sqrt{\\lambda_i \\lambda_j}}}  =\\lambda_i \\frac{v^T_{i}v_{j}}{\\sqrt{\\lambda_i \\lambda_j}}} $ <br><br> Also <br><br>\n",
    "${\\displaystyle u_i^TAv_i = (\\frac{Av_{i}}{\\sqrt{\\lambda_{i}}})^T Av_j =  \\frac{v^T_{i} A^T A v_j}{\\sqrt{\\lambda_{i}}} =\\frac{\\lambda_j v_i^T v_j}{\\sqrt{\\lambda_i}}}$ <br><br>\n",
    "Therefore, when $i \\neq j$, $u_i^TAv_i$ is zero. When $i = j$, $u_i^TAv_i$ is $\\sqrt{\\lambda_i}$. <br><br>\n",
    "<br><br>\n",
    "Then we try to compute the $\\Sigma$ matrix in SVD <br><br>\n",
    "$\\begin{bmatrix} u^T_1 \\\\ .\\\\.\\\\. \\\\ u^T_r \\end{bmatrix} A \\begin{bmatrix} v_1 & ... & v_r \\end{bmatrix} = diag(\\sigma_1, \\sigma_2, ...,\\sigma_r) = \\Sigma$ <br><br>\n",
    "Pick $u_{r+1}..., u_n$ that emsure orthgonality of all $u_i$. Pick $v_{r+1}..., v_n$ that emsure orthgonality of all $v_i$. Normalized all $u_i$ and $v_i$<br><br>\n",
    "$\\begin{bmatrix} u^T_1 \\\\ .\\\\.\\\\. \\\\ u^T_m \\end{bmatrix} A \\begin{bmatrix} v_1 & ... & v_m \\end{bmatrix} = diag(\\sigma_1, \\sigma_2, ...,\\sigma_m) = \\Sigma$ <br><br>\n",
    "As $UU^T = U^TU = I $ and $VV^T = V^TV = I$ <br><br>\n",
    "$A = U\\Sigma V^T$ <br><br>\n",
    "From the process of construction, it is easy to see that SVD is unique iff all eigenvalues of A are not the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Best rank-k approximation - operator norm: <br>\n",
    "The required result is the well known Eckhart-Young Theorem<br>\n",
    "See part e for generalized result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Best rank-k approximation - Frobenius norm: <br>\n",
    "See part e for generalized result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Schatten p-norms: <bd>\n",
    "The claim can be prove by showing that singular value preserve when A is multiplied by orthgonal matrix.\n",
    "$QAZ = QU\\Sigma V^TZ = (QU) \\Sigma (ZV^T)^T$ = $U'\\Sigma V'^{T}$ <br>\n",
    "$U'=QU$ is orthogonal as $QU(QU)^T = QUU^TQ^T = QQ^T = I$ <br>\n",
    "$V' = ZV^T$ is orthogonal as  $ZV^T(ZV^T)^T = ZV^TVZ^T = ZZ^T = I$  <br>\n",
    "$\\Sigma$ is the singular value matrix of $QAZ$<br>\n",
    "$||A||_p = ||QAZ||_p$ <br>\n",
    "Schatten p-norm is unitarily invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Best rank-k approximation - unitarily invariant norms: <br>\n",
    "To prove the stated result, we will use Ky Fan's dominance Theorem without prove <br><br>\n",
    "<b>Ky Fan's dominance Theorem</b> (on K - norm)<br><br>\n",
    "$||A||_{[k]} = \\sum_k \\sigma_k(A) $ &emsp;&emsp; where $k = 1, . . . , q = min(m, n)$  and $A ∈ M_{m,n}$<br><br>\n",
    "We also assume the following result without prove:<br><br>\n",
    "$\\lambda(A - B)$ majorizes $\\lambda(A)^↓ - \\lambda(B)^↓$<br><br>\n",
    "With that, We can show the following <br><br>\n",
    "$A , B  ∈ M_{m,n}$<br>\n",
    "for $ A = V_1\\Sigma(A)W_1 and B = V_2\\Sigma(B)W_2$ <br>\n",
    "if singular values in $\\Sigma(A)$ and $\\Sigma(B)$ are ordered in a non-increasing way <br>\n",
    "Then $||A − B|| ≥ ||\\Sigma(A) − \\Sigma(B)||$ for every unitarily invariant norm $||*||$ <br>\n",
    "As a consequence,  SVD provides the best $rank-k$ approximation<br><br>\n",
    "Prove: <br><br>\n",
    "$σ_1(A) ≥ · · · ≥ σ_q (A) ≥  \\underset{|m-n|}{\\underbrace{{0 = ··· = 0}}} ≥ −σ_q (A) ≥ · · · ≥ −σ_1(A)$ <br><br>\n",
    "Similarly eigenvalues of $B$ and $A − B$ can be ordered. <br>\n",
    "The differences of the respective ordered eigenvalues of $A$ and $B$ are<br><br>\n",
    "$±(σ_1(A) − σ_1(B)), . . . ,±(σ_q (A) − σ_q (B)), \\underset{|m-n|} {\\underbrace{0 , 0 , ... , 0}}$ <br><br>\n",
    "Although it is not clear how to order these values algebraically, the $q$ algebraically largest values\n",
    "are <br><br>\n",
    "$|σ_1(A) − σ_1(B)|, . . . , |σ_q (A) − σ_q (B)|$<br><br>\n",
    "Together with the result that $\\lambda(A - B)$ majorizes $\\lambda(A)^↓ - \\lambda(B)^↓$, We can see<br><br>\n",
    "$\\sum \\limits _{i=1} ^{k} σ_i (A − B) ≥  \\underset{1≤i_1<···<i_k≤q} {max} \\sum \\limits _{j=1} ^{k} |σ_{i_j} (A) − σ_{i_j} (B)|,$ &emsp;&emsp; $k = 1, . . . , q$<br><br>\n",
    "With the help of Ky Fan's dominance Theorem <br><br>\n",
    "$||A − B||_{[k]} ≥ ||\\Sigma(A) − \\Sigma(B)||_{[k]},$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $k = 1, . . . , q$<br><br>\n",
    "$||A − B|| ≥ ||\\Sigma(A) − \\Sigma(B)||,$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $||*||$ is unitarily invariant norm, $k = 1, . . . , q$<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;$= ||diag(σ_1(A) − σ_1(B), . . . , σ_k (A) − σ_k (B), σ_{k+1}(A), . . . , σ_q (A))||$ <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;$≥ ||diag(0, . . . , 0, σ_{k+1}(A), . . . , σ_q (A))||$ <br><br>\n",
    "\n",
    "The equalty in the above holds with $B = V\\Sigma_0W^T$ <br>\n",
    "where <br>\n",
    "1. $B ∈ M_{m,n}$ , $rank(B) = k$ <br>\n",
    "2. SVD of $A$ being $A = V\\Sigma(A)W^T$ <br>\n",
    "3. $\\Sigma_0 ∈ M_{m,n}$ is a nonnegative diagonal matrix with diagonal entries $σ_1(A), . . . , σ_k (A)$, and $q − k$ zeroes.<br><br>\n",
    "\n",
    "In other words. <br><br>\n",
    "$||A − V\\Sigma_0W^T|| = min ${$||A − B || : B ∈ M_{m×n}$ and has rank at most $k$} <br><br>\n",
    "i.e. The SVD provides the best rank-k approximation for any unitarily invariant norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Closest rotation: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment: did a good job in (e), but where is (f)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
