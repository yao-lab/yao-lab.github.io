{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-13T22:08:09.817695Z","iopub.execute_input":"2021-11-13T22:08:09.818041Z","iopub.status.idle":"2021-11-13T22:08:10.487216Z","shell.execute_reply.started":"2021-11-13T22:08:09.817957Z","shell.execute_reply":"2021-11-13T22:08:10.486476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crypto_df = pd.read_csv(\"../input/g-research-crypto-forecasting/\" + 'train.csv')\nassets = pd.read_csv('../input/g-research-crypto-forecasting/asset_details.csv')\nassets_names = dict(zip(assets.Asset_ID, assets.Asset_Name))\nassets_order = pd.read_csv('../input/g-research-crypto-forecasting/supplemental_train.csv').Asset_ID[:14]\nassets_order = dict((t,i) for i,t in enumerate(assets_order))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-13T22:08:10.488884Z","iopub.execute_input":"2021-11-13T22:08:10.489146Z","iopub.status.idle":"2021-11-13T22:09:10.911996Z","shell.execute_reply.started":"2021-11-13T22:08:10.489089Z","shell.execute_reply":"2021-11-13T22:09:10.911259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = crypto_df.copy().set_index(\"timestamp\")\ntrain['assets'] = 1\ntrain['assets'] = train.groupby(by = train.index)['assets'].sum()\ntrain['asset_name'] = train.Asset_ID.map(assets_names)\ntrain['asset_name'].value_counts()\nall_same_time = train[train['assets'] == 14][['Asset_ID', 'Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'asset_name','VWAP','Target']]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-13T22:09:10.913306Z","iopub.execute_input":"2021-11-13T22:09:10.913561Z","iopub.status.idle":"2021-11-13T22:09:18.147913Z","shell.execute_reply.started":"2021-11-13T22:09:10.913528Z","shell.execute_reply":"2021-11-13T22:09:18.147167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport traceback\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport gresearch_crypto #note: this notebook have to run on Kaggle, as this package is provided by the competition organizor and only available on Kaggle\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport pandas as pd, numpy as np\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\npd.set_option('display.max_columns', None)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T22:09:18.1499Z","iopub.execute_input":"2021-11-13T22:09:18.150126Z","iopub.status.idle":"2021-11-13T22:09:24.512872Z","shell.execute_reply.started":"2021-11-13T22:09:18.150085Z","shell.execute_reply":"2021-11-13T22:09:24.512164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = \"GPU\" \nSEED = 50\n\nEPOCHS = 1000\nDEBUG = False\nN_ASSETS = 14\nWINDOW_SIZE = 15\nBATCH_SIZE = 1024\nPCT_VALIDATION = 10 ","metadata":{"execution":{"iopub.status.busy":"2021-11-13T22:09:24.514245Z","iopub.execute_input":"2021-11-13T22:09:24.514482Z","iopub.status.idle":"2021-11-13T22:09:24.521392Z","shell.execute_reply.started":"2021-11-13T22:09:24.514448Z","shell.execute_reply":"2021-11-13T22:09:24.5198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.get_strategy()\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-13T22:13:05.040245Z","iopub.execute_input":"2021-11-13T22:13:05.040799Z","iopub.status.idle":"2021-11-13T22:13:05.053787Z","shell.execute_reply.started":"2021-11-13T22:13:05.04076Z","shell.execute_reply":"2021-11-13T22:13:05.052953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '../input/g-research-crypto-forecasting/'\norig_df_train = pd.read_csv('/kaggle/input/g-research-crypto-forecasting/train.csv')\nsupp_df_train = pd.read_csv('../input/g-research-crypto-forecasting/supplemental_train.csv')\ndf_asset_details = pd.read_csv('/kaggle/input/g-research-crypto-forecasting/asset_details.csv').sort_values(\"Asset_ID\")\n\n\ndef load_training_data_for_asset(asset_id):\n    dfs = []        \n    dfs.append(orig_df_train[orig_df_train[\"Asset_ID\"] == asset_id].copy())\n    df = pd.concat(dfs, axis = 0) if len(dfs) > 1 else dfs[0]\n    df['date'] = pd.to_datetime(df['timestamp'], unit = 's')        \n    df = df.sort_values('date')\n    return df\n\ndef load_data_for_all_assets():\n    dfs = []\n    for asset_id in range(14): dfs.append(load_training_data_for_asset(asset_id))\n    return pd.concat(dfs) \n\ntrain = load_data_for_all_assets().sort_values('timestamp').set_index(\"timestamp\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T22:13:08.91065Z","iopub.execute_input":"2021-11-13T22:13:08.910904Z","iopub.status.idle":"2021-11-13T22:14:01.418557Z","shell.execute_reply.started":"2021-11-13T22:13:08.910875Z","shell.execute_reply":"2021-11-13T22:14:01.417126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(data_path + 'example_test.csv')\nsample_prediction_df = pd.read_csv(data_path + 'example_sample_submission.csv')\nassets = pd.read_csv('../input/g-research-crypto-forecasting/asset_details.csv')\nassets_order = pd.read_csv('../input/g-research-crypto-forecasting/supplemental_train.csv').Asset_ID[:N_ASSETS]\nassets_order = dict((t,i) for i,t in enumerate(assets_order))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T22:16:52.457759Z","iopub.execute_input":"2021-11-13T22:16:52.458839Z","iopub.status.idle":"2021-11-13T22:16:54.914502Z","shell.execute_reply.started":"2021-11-13T22:16:52.458788Z","shell.execute_reply":"2021-11-13T22:16:54.91373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n        \n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-13T22:16:54.916144Z","iopub.execute_input":"2021-11-13T22:16:54.916418Z","iopub.status.idle":"2021-11-13T22:16:54.932788Z","shell.execute_reply.started":"2021-11-13T22:16:54.916378Z","shell.execute_reply":"2021-11-13T22:16:54.932036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_features(df):\n    df['Upper_Shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n    df['Lower_Shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n    df['spread'] = df['High'] - df['Low']\n    df['mean_trade'] = df['Volume']/df['Count']\n    df['log_price_change'] = np.log(df['Close']/df['Open'])\n    df['close_vwap_diff'] = (df['Close'] - df['VWAP']).abs()\n    return df\n\ntrain[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']] = train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].astype(np.float32)\nprint(train.shape)\ntrain['Target'] = train['Target'].fillna(0)\nVWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\nVWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\ntrain['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\ndf = train[['Asset_ID', 'Target']].copy()\ntimes = dict((t,i) for i,t in enumerate(df.index.unique()))\ndf['id'] = df.index.map(times)\ndf['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\nids = df.id.copy()\ndel df\ntrain = get_features(train)\ntrain_features = [i for i in train.columns if i not in ['Target', 'date', 'timestamp', 'Asset_ID', 'groups']]","metadata":{"execution":{"iopub.status.busy":"2021-11-13T22:21:16.445022Z","iopub.execute_input":"2021-11-13T22:21:16.445989Z","iopub.status.idle":"2021-11-13T22:21:16.529369Z","shell.execute_reply.started":"2021-11-13T22:21:16.445856Z","shell.execute_reply":"2021-11-13T22:21:16.528157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.sort_index()\nind = train.index.unique()\ndef reindex(df):\n    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n    return df\ntrain = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\ngc.collect()\ntrain.shape","metadata":{"papermill":{"duration":6.94223,"end_time":"2021-11-08T18:00:28.066883","exception":false,"start_time":"2021-11-08T18:00:21.124653","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-13T22:18:08.218596Z","iopub.execute_input":"2021-11-13T22:18:08.218872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['group_num'] = train.index.map(times)\ntrain = train.dropna(subset=['group_num'])\ntrain['group_num'] = train['group_num'].astype('int')\ntrain['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\ntrain['is_real'] = train.id.isin(ids) * 1\ntrain = train.drop('id', axis=1)","metadata":{"papermill":{"duration":53.910303,"end_time":"2021-11-08T18:01:22.002197","exception":false,"start_time":"2021-11-08T18:00:28.091894","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train.columns.drop(['Asset_ID','group_num','is_real'])\ntrain.loc[train.is_real == 0, features] = 0.","metadata":{"papermill":{"duration":0.724514,"end_time":"2021-11-08T18:01:22.751294","exception":false,"start_time":"2021-11-08T18:01:22.02678","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['asset_order'] = train.Asset_ID.map(assets_order)\ntrain = train.sort_values(by=['group_num', 'asset_order'])\ntrain = reduce_mem_usage(train)\ntrain.head(20)\ngc.collect()","metadata":{"papermill":{"duration":4.497686,"end_time":"2021-11-08T18:01:27.273453","exception":false,"start_time":"2021-11-08T18:01:22.775767","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = train['Target'].to_numpy().reshape(-1, N_ASSETS)\nfeatures = train.columns.drop(['Asset_ID', 'Target', 'group_num', 'is_real', 'date'])\ntrain = train[features]\ntrain = train.values\ntrain = train.reshape(-1, N_ASSETS, train.shape[-1])","metadata":{"papermill":{"duration":1.821193,"end_time":"2021-11-08T18:01:29.120631","exception":false,"start_time":"2021-11-08T18:01:27.299438","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class item_generate(keras.utils.Sequence):\n    def __init__(self, x_set, y_set, batch_size, length):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.length = length\n        self.size = len(x_set)\n    def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size)))\n    def __getitem__(self, idx):\n        batch_x=[]\n        batch_y=[]\n        for i in range(self.batch_size):\n            start_ind = self.batch_size*idx + i\n            end_ind = start_ind + self.length \n            if end_ind <= self.size:\n                batch_x.append(self.x[start_ind : end_ind])\n                batch_y.append(self.y[end_ind -1])\n        return np.array(batch_x), np.array(batch_y)","metadata":{"papermill":{"duration":0.03584,"end_time":"2021-11-08T18:01:29.23194","exception":false,"start_time":"2021-11-08T18:01:29.1961","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-13T22:09:24.875739Z","iopub.status.idle":"2021-11-13T22:09:24.876363Z","shell.execute_reply.started":"2021-11-13T22:09:24.876126Z","shell.execute_reply":"2021-11-13T22:09:24.876154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test = train[:-len(train)//PCT_VALIDATION], train[-len(train)//PCT_VALIDATION:]\ny_train, y_test = targets[:-len(train)//PCT_VALIDATION], targets[-len(train)//PCT_VALIDATION:]","metadata":{"papermill":{"duration":0.032299,"end_time":"2021-11-08T18:01:29.289071","exception":false,"start_time":"2021-11-08T18:01:29.256772","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-13T22:09:24.877307Z","iopub.status.idle":"2021-11-13T22:09:24.878059Z","shell.execute_reply.started":"2021-11-13T22:09:24.877805Z","shell.execute_reply":"2021-11-13T22:09:24.877833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = item_generate(X_train, y_train, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\nval_generator = item_generate(X_test, y_test, length = WINDOW_SIZE, batch_size = BATCH_SIZE)\nprint(f'Sample shape: {train_generator[0][0].shape}')\nprint(f'Target shape: {train_generator[0][1].shape}')","metadata":{"papermill":{"duration":0.048353,"end_time":"2021-11-08T18:01:29.362579","exception":false,"start_time":"2021-11-08T18:01:29.314226","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-13T22:09:24.87925Z","iopub.status.idle":"2021-11-13T22:09:24.88Z","shell.execute_reply.started":"2021-11-13T22:09:24.879753Z","shell.execute_reply":"2021-11-13T22:09:24.87978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras import backend as K\nfrom tensorflow.keras.losses import MSE\nfrom keras.layers import Dense, Activation, Dropout, Layer, LayerNormalization, Conv1D, Input, Concatenate, Permute, Add, GlobalAvgPool1D\n\nclass FixedPositionalEncoding(Layer):\n    def __init__(self, d_model, dropout = 0.1, max_len = 1024, scale_factor = 1.0, batch_size = BATCH_SIZE):\n        super(FixedPositionalEncoding, self).__init__()\n        self.dropout = Dropout(dropout)\n        self.batch_size = batch_size\n        self.pe = np.zeros((max_len, d_model))\n        position = np.expand_dims(np.arange(0, max_len, dtype = np.float32), 1)\n        div_term = np.exp(np.arange(0, d_model, 2)) * (-np.log(10000.0) / d_model)\n        self.pe[:, 0::2] = tf.sin(position * div_term)\n        self.pe[:, 1::2] = tf.cos(position * div_term)\n        self.pe = scale_factor * np.expand_dims(self.pe, 1)\n    def call(self, x):\n        x = x + self.pe\n        return self.dropout(x)\n\nclass LearnablePositionalEncoding(Layer):\n    def __init__(self, d_model, dropout = 0.1, max_len = 1024):\n        super(LearnablePositionalEncoding, self).__init__()\n        self.dropout = Dropout(dropout)\n        self.pe = tf.Variable((max_len, 1, d_model))\n        self.pe = nn.init.uniform_(self.pe, -0.02, 0.02)\n\n    def call(self, inputs, *args, **kwargs):\n        x = inputs + self.pe[:inputs.size(0), :]\n        return self.dropout(x)\n    \nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.depth = d_model // self.num_heads\n        self.wq = tf.keras.layers.Dense(d_model, kernel_initializer='glorot_uniform', use_bias=False)\n        self.wk = tf.keras.layers.Dense(d_model, kernel_initializer='glorot_uniform', use_bias=False)\n        self.wv = tf.keras.layers.Dense(d_model, kernel_initializer='glorot_uniform', use_bias=False)\n        self.ff = tf.keras.layers.Dense(d_model, kernel_initializer='glorot_uniform', use_bias=False)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs, *args, **kwargs):\n        q, k, v, mask = inputs\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n        output = self.ff(concat_attention)\n        return output, attention_weights\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    if mask is not None: scaled_attention_logits += (mask * -1e9)\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n    output = tf.matmul(attention_weights, v)\n    return output, attention_weights\n\nclass PointwiseFeedforward(tf.keras.layers.Layer):\n    def __init__(self, d_model):\n        super(PointwiseFeedforward, self).__init__()\n        self.ff1 = tf.keras.layers.Dense(d_model, kernel_initializer='glorot_uniform')\n        self. ff2 = tf.keras.layers.Dense(d_model, kernel_initializer='glorot_uniform')\n\n    def call(self, x):\n        x = self.ff1(x)\n        x = tf.nn.leaky_relu(x)\n        x = self.ff2(x)\n        return x\n\nclass TransformerLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(TransformerLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = PointwiseFeedforward(d_model)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n    def call(self, x, y, mask):\n        attn_output, _ = self.mha(x, y, y, mask)\n        out1 = self.layernorm1(x + attn_output)\n        ffn_output = self.ffn(out1)\n        out2 = self.layernorm2(out1 + ffn_output)\n        return out2\n\nclass SingleAttention(Layer):\n    def __init__(self, d_k, d_v):\n        super(SingleAttention, self).__init__()\n        self.d_k = d_k\n        self.d_v = d_v\n\n    def build(self, input_shape):\n        self.query = Dense(self.d_k, input_shape = input_shape, kernel_initializer = 'glorot_uniform', bias_initializer = 'glorot_uniform')\n        self.key   = Dense(self.d_k, input_shape = input_shape, kernel_initializer = 'glorot_uniform', bias_initializer = 'glorot_uniform')\n        self.value = Dense(self.d_v, input_shape = input_shape, kernel_initializer = 'glorot_uniform', bias_initializer = 'glorot_uniform')\n\n    def call(self, inputs):\n        q = self.query(inputs[0])\n        k = self.key(inputs[1])\n        v = self.value(inputs[2])\n        attn_weights = tf.matmul(q, k, transpose_b = True)\n        attn_weights = tf.map_fn(lambda x: x / np.sqrt(self.d_k), attn_weights)\n        attn_weights = tf.nn.softmax(attn_weights, axis = -1)\n        attn_out = tf.matmul(attn_weights, v)\n        return attn_out\n\nclass MultiAttention(Layer):\n    def __init__(self, d_k, d_v, n_heads):\n        super(MultiAttention, self).__init__()\n        self.d_k = d_k\n        self.d_v = d_v\n        self.n_heads = n_heads\n        self.attn_heads = list()\n\n    def build(self, input_shape):\n        for n in range(self.n_heads):\n            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))\n        self.linear = Dense(input_shape[0][-1], kernel_initializer = 'glorot_uniform', bias_initializer = 'glorot_uniform')\n\n    def call(self, inputs):\n        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n        concat_attn = Concatenate(axis = 2)(attn)\n        multi_linear = self.linear(concat_attn)\n        return multi_linear\n\nclass TransformerEncoder(Layer):\n    def __init__(self, d_k, n_heads, ff_dim, dropout = 0.1, **kwargs):\n        super(TransformerEncoder, self).__init__()\n        self.d_k = d_k\n        self.ff_dim = ff_dim\n        self.n_heads = n_heads\n        self.attn_heads = list()\n        self.dropout_rate = dropout\n\n    def build(self, input_shape):\n        self.attn_multi = MultiAttention(self.d_k, self.d_k, self.n_heads)\n        self.attn_dropout = Dropout(self.dropout_rate)\n        self.attn_normalize = LayerNormalization(epsilon = 1e-6)\n        self.ff_conv1D_1 = Conv1D(filters = self.ff_dim, kernel_size = 1, activation = 'relu', padding = 'same')\n        self.ff_conv1D_2 = Conv1D(filters = input_shape[-1], kernel_size = 1, padding = 'same')\n        self.ff_dropout = Dropout(self.dropout_rate)\n        self.ff_normalize = LayerNormalization(epsilon = 1e-6)\n        self.addition = Add()\n        self.addition_2 = Add()\n\n    def call(self, inputs):\n        attn_layer = self.attn_multi([inputs, inputs, inputs])\n        attn_layer = self.attn_dropout(attn_layer)\n        attn_layer = self.attn_normalize(self.addition_2([inputs, attn_layer]))\n        ff_layer = self.ff_conv1D_1(attn_layer)\n        ff_layer = self.ff_conv1D_2(ff_layer)\n        ff_layer = self.ff_dropout(ff_layer)\n        ff_layer = self.ff_normalize(self.addition([inputs, ff_layer]))\n        return ff_layer\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({'d_k': self.d_k, 'd_v': self.d_v, 'n_heads': self.n_heads, 'ff_dim': self.ff_dim, 'attn_heads': self.attn_heads, 'dropout_rate': self.dropout_rate})\n        return config\n\ndef batch_generator(batch_size):\n    while True:\n        x_train, y_train = train_features, targets\n        for i in range(0, len(x_train), batch_size):\n            x_batch = x_train[i:i + batch_size]\n            y_batch = y_train[i:i + batch_size]\n            for idx in range(x_batch.shape[0]):\n                mask = noise_mask(x_batch[0], masking_ratio, mean_mask_length, 'separate', 'geometric', None)\n                x_batch[idx][np.where(mask)] = -1.0\n                y_batch[idx][np.where(np.sum(mask, axis = -1))] = -1.0\n            yield (x_batch, y_batch)\n\nclass TSTransformer(Model):\n    def __init__(self, feat_dim, max_len, d_model, n_heads, num_layers, dim_feedforward, dropout = 0.1, pos_encoding = 'fixed', activation = 'gelu', norm = 'BatchNorm', freeze = False):\n        super(TSTransformer, self).__init__()\n        self.max_len     = max_len\n        self.d_model     = d_model\n        self.n_heads     = n_heads\n        self.project_inp = Dense(d_model)\n        self.pos_enc     = FixedPositionalEncoding(d_model, dropout = dropout * (1.0 - freeze), max_len = max_len)\n        self.encoder_layers = [TransformerEncoder(d_model, self.n_heads, dim_feedforward, dropout * (1.0 - freeze), activation = activation) for i in range(num_layers)]\n        self.output_layer = Dense(feat_dim)\n        self.act          = Activation('relu')\n        self.dropout1     = Dropout(dropout)\n        self.feat_dim     = feat_dim\n\n    def call(self, X):\n        inp = K.permute_dimensions(X, (1, 0, 2))\n        inp = self.project_inp(inp) * math.sqrt(self.d_model)\n        inp = self.pos_enc(inp)\n        x = inp\n        for lay in self.encoder_layers: x = lay(x)\n        output = self.act(x)\n        output = K.permute_dimensions(output, (1, 0, 2))\n        output = self.dropout1(output)\n        output = self.output_layer(output)\n        return output\ndef noise_mask(X, masking_ratio, lm = 3, mode = 'separate', distribution = 'geometric', exclude_feats = None):\n    if exclude_feats is not None: exclude_feats = set(exclude_feats)\n    if distribution == 'geometric':\n        if mode == 'separate':\n            mask = np.ones(X.shape, dtype = bool)\n            for m in range(X.shape[1]):\n                if exclude_feats is None or m not in exclude_feats: mask[:, m] = geom_noise_mask_single(X.shape[0], lm, masking_ratio)  \n        else: mask = np.tile(np.expand_dims(geom_noise_mask_single(X.shape[0], lm, masking_ratio), 1), X.shape[1])\n    else:\n        if mode == 'separate': mask = np.random.choice(np.array([True, False]), size = X.shape, replace = True, p = (1 - masking_ratio, masking_ratio))\n        else: mask = np.tile(np.random.choice(np.array([True, False]), size = (X.shape[0], 1), replace = True, p = (1 - masking_ratio, masking_ratio)), X.shape[1])\n    return mask\n\ndef geom_noise_mask_single(L, lm, masking_ratio):\n    keep_mask = np.ones(L, dtype = bool)\n    p_m = 1 / lm\n    p_u = p_m * masking_ratio / (1 - masking_ratio)\n    p = [p_m, p_u]\n    state = int(np.random.rand() > masking_ratio)\n    for i in range(L):\n        keep_mask[i] = state\n        if np.random.rand() < p[state]: state = 1 - state\n    return keep_mask\n\ndef padding_mask(lengths, max_len = None):\n    batch_size = lengths.numel()\n    max_len = max_len or lengths.max_val()\n    return (tf.range(0, max_len, device = lengths.device).type_as(lengths).repeat(batch_size, 1).lt(tf.expand_dims(lengths, 1)))\n\ndef collate_unsuperv(data, max_len = None, mask_compensation = False):\n    batch_size = len(data)\n    features, masks, u_out = zip(*data)\n    lengths = [X.shape[0] for X in features]\n    if max_len is None:\n        max_len = max(lengths)\n    X = tf.zeros(batch_size, max_len, features[0].shape[-1])\n    target_masks = tf.zeros_like(X, dtype = tf.bool)\n    for i in range(batch_size):\n        end = min(lengths[i], max_len)\n        X[i, :end, :] = features[i][:end, :]\n        target_masks[i, :end, :] = masks[i][:end, :]\n    targets = X.clone()\n    X = X * target_masks\n    if mask_compensation: X = compensate_masking(X, target_masks)\n    padding_masks = tf.zeros(batch_size, max_len, dtype = tf.bool)\n    for i in range(batch_size): padding_masks[i, :] = tf.where(u_out[i] == 0, 1, 0)\n    target_masks = ~target_masks\n    return X, targets, target_masks, padding_masks\n\ndef masked_mse(y_true, y_pred):\n    is_mask = K.equal(tf.cast(y_true, tf.float32), tf.cast(-1.0, tf.float32))\n    is_mask = K.cast(is_mask, dtype = K.floatx())\n    is_mask = 1 - is_mask\n    y_true = tf.cast(y_true, tf.float32) * tf.cast(is_mask, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32) * tf.cast(is_mask, tf.float32)\n    return MSE(y_true, y_pred)\n\ndef get_transformer_model(feat_dim, max_len, d_model, n_heads, num_layers, dim_feedforward, dropout = 0.1, pos_encoding = 'fixed', activation = 'gelu', norm = 'BatchNorm', freeze = False):\n    input_layer = Input((breath_steps, train_features.shape[-1]))\n    inp = input_layer\n    inp = Dense(d_model)(inp) * math.sqrt(d_model)\n    x = inp\n    for idx, lay in enumerate(range(num_layers)):\n        x = TransformerEncoder(d_model, n_heads, dim_feedforward, dropout * (1.0 - freeze), activation = activation)(x)\n    output = Activation('relu')(x)\n    output = Dropout(dropout)(output)\n    output = Dense(feat_dim)(output)\n    output = Permute([2, 1])(output)\n    output = GlobalAvgPool1D()(output)\n    return Model(inputs = input_layer, outputs = output)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T22:23:55.475122Z","iopub.execute_input":"2021-11-13T22:23:55.47561Z","iopub.status.idle":"2021-11-13T22:24:00.338096Z","shell.execute_reply.started":"2021-11-13T22:23:55.47557Z","shell.execute_reply":"2021-11-13T22:24:00.337045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Concatenate, Add, GRU, GlobalAvgPool1D, Dense, Dropout, Input, Bidirectional, LSTM, Conv1D, Multiply\n\ndef MaxCorrelation(y_true,y_pred): return -tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\ndef Correlation(y_true,y_pred): return tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\n\ndef masked_mse(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_squared_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\ndef masked_mae(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_absolute_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\ndef masked_cosine(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.cosine_similarity(y_true_masked, y_pred_masked)\n\ndef get_transformer(x):\n    n_heads = 8\n    d_model = 128\n    dropout = 0.1\n    freeze = False\n    num_layers = 3\n    activation = 'gelu'\n    dim_feedforward = 256\n    \n    x = Dense(d_model)(x) * math.sqrt(d_model)\n    x = TransformerEncoder(d_model, n_heads, dim_feedforward, dropout * (1.0 - freeze), activation = activation)(x)\n    \n    return x\n\ndef get_wavenet(x):\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2 ** i for i in range(n)]\n        x = Conv1D(filters = filters, kernel_size = 1, padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters, kernel_size = kernel_size, padding = 'same', activation = 'tanh', dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters, kernel_size = kernel_size, padding = 'same', activation = 'sigmoid', dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters, kernel_size = 1, padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n\n    x = wave_block(x, 16, 3, 8)\n    x = wave_block(x, 32, 3, 5)\n\n    return x\n\ndef get_lstm(x):\n    x = layers.LSTM(units=32, return_sequences=True)(x)\n    return x\n\ndef get_model(n_assets = 14, model_name = 'wavenet'):\n    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n    branch_outputs = []\n    for i in range(n_assets):\n        a = layers.Lambda(lambda x: x[:,:, i])(x_input) \n        a = layers.Masking(mask_value = 0., )(a)\n        if model_name == 'wavenet':\n            a = get_wavenet(a)\n        elif model_name == 'transformer':\n            a = get_transformer(a)\n        elif model_name == 'lstm':\n            a = get_lstm(a)\n        a = layers.GlobalAvgPool1D()(a)\n        branch_outputs.append(a)\n    x = layers.Concatenate()(branch_outputs)\n    x = layers.Dense(units = 128)(x)\n    out = layers.Dense(units = n_assets)(x)\n    model = keras.Model(inputs=x_input, outputs=out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), loss = masked_cosine, metrics=[Correlation])\n    return model\n    \nwavenet = wavenet_model()\ntransfomer = ","metadata":{"papermill":{"duration":23.222675,"end_time":"2021-11-08T18:01:52.662288","exception":false,"start_time":"2021-11-08T18:01:29.439613","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-13T22:09:24.881268Z","iopub.status.idle":"2021-11-13T22:09:24.882001Z","shell.execute_reply.started":"2021-11-13T22:09:24.881744Z","shell.execute_reply":"2021-11-13T22:09:24.881771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(get_model(n_assets=1, 'wavenet'), show_shapes=True)","metadata":{"papermill":{"duration":5.288243,"end_time":"2021-11-08T18:01:57.977238","exception":false,"start_time":"2021-11-08T18:01:52.688995","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-13T22:09:24.883121Z","iopub.status.idle":"2021-11-13T22:09:24.883854Z","shell.execute_reply.started":"2021-11-13T22:09:24.883607Z","shell.execute_reply":"2021-11-13T22:09:24.883635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features)\n\ntf.random.set_seed(0)\nestop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 7, verbose = 0, mode = 'min',restore_best_weights = True)\nscheduler = keras.optimizers.schedules.ExponentialDecay(1e-2, (0.5 * len(X_train) / BATCH_SIZE), 1e-2)\nlr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\nhistory = model.fit(train_generator, validation_data = (val_generator), epochs = EPOCHS, callbacks = [lr, estop])\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\nhistories = pd.DataFrame(history.history)\nepochs = list(range(1,len(histories)+1))\nloss = histories['loss']\nval_loss = histories['val_loss']\nCorrelation = histories['Correlation']\nval_Correlation = histories['val_Correlation']\nax[0].plot(epochs, loss, label = 'Train Loss')\nax[0].plot(epochs, val_loss, label = 'Val Loss')\nax[0].set_title('Losses')\nax[0].set_xlabel('Epoch')\nax[0].legend(loc='upper right')\nax[1].plot(epochs, Correlation, label = 'Train Correlation')\nax[1].plot(epochs, val_Correlation, label = 'Val Correlation')\nax[1].set_title('Correlations')\nax[1].set_xlabel('Epoch')\nax[1].legend(loc='upper right')\nfig.show()\ngc.collect()\n\npredictions = model.predict(val_generator)\n\nfor i in range(N_ASSETS):\n    # drop first 14 values in the y_test, since they are absent in val_generator labels\n    y_true = np.squeeze(y_test[WINDOW_SIZE - 1:, i])\n    y_pred = np.squeeze(predictions[:, i])\n    real_target_ind = np.argwhere(y_true!=0)\n    asset_id = list(assets_order.keys())[i]\n    asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n    print(f\"{asset_name}: {np.corrcoef(y_pred[real_target_ind].flatten(), y_true[real_target_ind].flatten())[0,1]:.4f}\")","metadata":{"papermill":{"duration":2433.778932,"end_time":"2021-11-08T18:42:31.784877","exception":false,"start_time":"2021-11-08T18:01:58.005945","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-13T22:09:24.885054Z","iopub.status.idle":"2021-11-13T22:09:24.885824Z","shell.execute_reply.started":"2021-11-13T22:09:24.885554Z","shell.execute_reply":"2021-11-13T22:09:24.885583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission\n# placeholder for first 15 samples\nsup = pd.read_csv('../input/g-research-crypto-forecasting/supplemental_train.csv')[:WINDOW_SIZE * (N_ASSETS)]\nplaceholder = get_features(sup)\nplaceholder['asset_order'] = placeholder.Asset_ID.map(assets_order)\ntest_sample = np.array(placeholder[features])\ntest_sample = test_sample.reshape(-1, (N_ASSETS), test_sample.shape[-1])\ntest_sample = np.expand_dims(test_sample, axis=0)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-11-13T22:09:24.887Z","iopub.status.idle":"2021-11-13T22:09:24.887768Z","shell.execute_reply.started":"2021-11-13T22:09:24.887519Z","shell.execute_reply":"2021-11-13T22:09:24.887548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for test gap filling\nexample = pd.read_csv('../input/g-research-crypto-forecasting/example_test.csv')[:WINDOW_SIZE - 1]\nexample['asset_order'] = example.Asset_ID.map(assets_order) \nexample = example[['Asset_ID','asset_order']]","metadata":{"papermill":{"duration":4.048919,"end_time":"2021-11-08T18:44:21.954651","exception":false,"start_time":"2021-11-08T18:44:17.905732","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-13T22:09:24.888859Z","iopub.status.idle":"2021-11-13T22:09:24.88963Z","shell.execute_reply.started":"2021-11-13T22:09:24.889388Z","shell.execute_reply":"2021-11-13T22:09:24.889417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from backtesting import Strategy\nfrom backtesting.lib import crossover\nfrom backtesting.test import SMA\nfrom backtesting.lib import plot_heatmaps\n\nCash = 100000\n\ndef BBANDS(data, n_lookback, n_std):\n    \"\"\"Bollinger bands indicator\"\"\"\n    hlc3 = (data.High + data.Low + data.Close) / 3\n    mean, std = hlc3.rolling(n_lookback).mean(), hlc3.rolling(n_lookback).std()\n    upper = mean + n_std*std\n    lower = mean - n_std*std\n    return upper, lower\n\n\n\nclass MyStrategy(Strategy):\n    n1 = 50\n    n2 = 100\n    n_enter = 20\n    n_exit = 10\n    \n    def init(self):\n        self.corr = self.I(SMA, self.data.Close, self.n1)\n        self.sma_enter = self.I(SMA, self.data.Close, self.n_enter)\n        self.sma_exit = self.I(SMA, self.data.Close, self.n_exit)\n        \n    def next(self):\n        \n        if not self.position:\n            pos_corr_count = 0\n            neg_corr_count = 0\n            if self.corr > 0 and pos_corr_count >= 3:\n                self.buy()\n            elif self.corr < 0 and neg_corr_count >= 3:\n                self.sell()\n            if self.corr > 0:\n                pos_corr_count += 1\n                neg_corr_count = 0\n            else:\n                pos_corr_count = 0\n                neg_corr_count += 1\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = get_features(test_df)\n    test_data = test_df.merge(example, how='outer', on='Asset_ID').sort_values('asset_order')\n    test = np.array(test_data[features].fillna(0))\n    test = test.reshape(-1, 1, N_ASSETS, test.shape[-1])\n    test_sample = np.hstack([test_sample, test])[:,-1 * WINDOW_SIZE:]\n    y_pred = model.predict(test_sample).squeeze().reshape(-1, 1).squeeze()\n    test_data['Target'] = y_pred\n    for _, row in test_df.iterrows():\n        try: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = test_data.loc[test_data['row_id'] == row['row_id'], 'Target'].item()\n        except: sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = 0\n    env.predict(sample_prediction_df)","metadata":{"papermill":{"duration":32.665279,"end_time":"2021-11-08T18:44:58.360322","exception":false,"start_time":"2021-11-08T18:44:25.695043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-13T22:02:57.473871Z","iopub.status.idle":"2021-11-13T22:02:57.474548Z","shell.execute_reply.started":"2021-11-13T22:02:57.474309Z","shell.execute_reply":"2021-11-13T22:02:57.474335Z"},"trusted":true},"execution_count":null,"outputs":[]}]}