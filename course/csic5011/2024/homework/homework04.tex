\documentclass[11pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}

\def\N{{\mathbb N}}
\def\NN{{\mathcal N}}
\def\R{{\mathbb R}}
\def\E{{\mathbb E}}
\def\tr{{\mathrm{trace}}}
\def\P{{\mathrm{Prob}}}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0.25 in}
\setlength{\parskip}{0.1 in}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \setcounter{section}{0}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf A Mathematical Introduction to Data Science \hfill #4} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\it Instructor: #2\hfill #3} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}


\begin{document}

\lecture{Homework 4. Random Projections}{Yuan Yao}{Due: in 1 week}{Mar 6, 2024}

The problem below marked by $^*$ is optional with bonus credits. For the experimental problem, include the source codes which are runnable under standard settings. Since there is NO grader assigned for this class, homework will not be graded. But if you would like to submit your exercise, please send your homework to the address (\href{mailto:datascience.hw@gmail.com}{datascience.hw@gmail.com})  with a title ``CSIC5011: Homework \#". I'll read them and give you bonus credits. 

\begin{enumerate}


\item {\em SNPs of World-wide Populations:} This dataset contains a data matrix $X\in \R^{n\times p}$ of about $p=650,000$ columns of SNPs (Single Nucleid Polymorphisms) and $n=1064$ rows of peoples around the world (but there are 21 rows mostly with missing values). Each element is of three choices, $0$ (for `AA'), $1$ (for `AC'), $2$ (for `CC'), and some missing values marked by $9$. 

\url{https://drive.google.com/file/d/1KMLPEG91mnzdK2pUlq2BkjOx2BsaZy9s/view?usp=sharing}

which is big (151MB in zip and 2GB original txt). Moreover, the following file contains the region where each people comes from, as well as two variables {\texttt{ind1}} and{\texttt{ind2}} such that $X({\texttt{ind1}},{\texttt{ind2}})$ removes all missing values. 

\url{https://github.com/yao-lab/yao-lab.github.io/blob/master/data/HGDP_region.mat}

Another cleaned dataset is due to Quanhua MU and Yoonhee Nam:  

\begin{itemize}
\item Genotyped data of the 1043 ($n$) subjects. 0(AA), 1(AC), 2(CC). Missing values are removed, only autosomal SNPs were selected ($p\approx 400K$). Google drive link: \\
\url{https://drive.google.com/file/d/1a9I8_akfCMHBRrPMdnWkjyL9fKcQbJJq/view?usp=sharing}
\item Sample Information of 1043 subjects. Google drive link: \\
\url{https://drive.google.com/file/d/11Q-8B57WDQnrIV92b-h_WLqDGviiYsm2/view?usp=sharing} \\
\end{itemize}

A good reference for this data can be the following paper in Science, 

\url{http://www.sciencemag.org/content/319/5866/1100.abstract}

Explore the genetic variation of those persons with their geographic variations, by MDS/PCA. Since $p$ is big, explore random projections for dimensionality reduction.  

\item {\em Phase Transition in Compressed Sensing:} Let $A\in \R^{n\times d}$ be a Gaussian random matrix, \emph{i.e.} $A_{ij} \sim \NN(0,1)$. In the following experiments, fix $d=20$. For each $n=1,\ldots,d$, and each $k=1,\ldots, d$, repeat the following procedure 50 times:

\begin{enumerate}
\item Construct a sparse vector $x_0\in\R^d$ with $k$ nonzero entries. The locations of the nonzero entries are selected at random and each nonzero equals $\pm 1$ with equal probability;
\item Draw a standard Gaussian random matrix $A\in \R^{n\times d}$, and set $b=Ax_0$;
\item Solve the following linear programming problem to obtain an optimal point $\hat{x}$,
\begin{eqnarray*}
&\min_x&  \|x\|_1:= \sum |x_i| \\
& s.t. & A x = b,
\end{eqnarray*} 
for example, matlab toolbox {\tt{cvx}} can be an easy solver;
\item Declare success if $\|\hat{x} - x_0\|\leq 10^{-3}$;
\end{enumerate}

After repeating 50 times, compute the success probability $p(n,k)$; draw a figure with x-axis for $k$ and y-axis for $n$, to visualize the success probability. For example, matlab command {\tt{imagesc(p)}} can be a choice. 

Can you try to give an analysis of the phenomenon observed? The following paper by Tropp et al. may give you a good starting point to think.

\begin{itemize}
\item Dennis Amelunxen, Martin Lotz, Michael B. McCoy, Joel A. Tropp. Living on the edge: Phase transitions in convex programs with random data. arXiv:1303.6672. URL: \url{https://arxiv.org/abs/1303.6672} 
\end{itemize}


%\item {\em $^*$Singular Value Decomposition:} The goal of this exercise is to refresh your memory about the singular value decomposition and matrix norms. A good reference to the singular value decomposition is Chapter 2 in this book:\\
%{\em Matrix Computations}, Golub and Van Loan, 3rd edition.\\
%Parts of the book are available online here:\\
%\url{http://www.math.pku.edu.cn/teachers/yaoy/reference/golub.pdf}
%
%\begin{enumerate}
%
%\item {\em Existence:} Prove the existence of the singular value decomposition. That is, show that if $A$ is an $m\times n$ real valued matrix, then $A = U\Sigma V^T$, where $U$ is $m\times m$ orthogonal matrix, $V$ is $n \times n$ orthogonal matrix, and $\Sigma = \text{diag}(\sigma_1,\sigma_2,\ldots,\sigma_p)$ (where $p=\min\{m,n\}$) is an $m\times n$ diagonal matrix. It is customary to order the singular values in decreasing order: $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_p \geq 0$. Determine to what extent the SVD is unique. (See Theorem 2.5.2, page 70 in Golub and Van Loan).
%
%\item {\em Best rank-k approximation - operator norm:} Prove that the ``best" rank-$k$ approximation of a matrix in the operator norm sense is given by its SVD. That is, if $A = U\Sigma V^T$ is the SVD of $A$, then $A_k = U\Sigma_k V^T$ (where $\Sigma_k = \text{diag}(\sigma_1,\sigma_2,\ldots,\sigma_k,0,\ldots,0)$ is a diagonal matrix containing the largest $k$ singular values) is a rank-$k$ matrix that satisfies
%    $$\|A-A_k\| = \min_{\operatorname{rank}(B)=k} \|A-B\|.$$ (Recall that the operator norm of $A$ is $\|A\| = \max_{\|x\|=1} \|Ax\|$. See Theorem 2.5.3 (page 72) in Golub and Van Loan).
%
%\item {\em Best rank-k approximation - Frobenius norm:} Show that the SVD also provides the best rank-$k$ approximation for the Frobenius norm, that is, $A_k = U \Sigma_k V^T$ satisfies $$\|A-A_k\|_F = \min_{\operatorname{rank}(B)=k} \|A-B\|_F.$$

%\item {\em Best rank-k approximation - Schatten p-norms:} A matrix norm $\| \cdot \|$ that satisfies
%$$\|QAZ\| = \|A\|,$$
%for all $Q$ and $Z$ orthogonal matrices is called a unitarily invariant norm. The Schatten $p$-norm of a matrix $A$ is given by the $\ell_p$ norm ($p\geq 1$) of its vector of singular values, namely, $$\|A\|_p = \left(\sum_{i} \sigma_i^p\right)^{1/p}.$$ Show that the Schatten $p$-norm is unitarily invariant. Note that the case $p=1$ is sometimes called the nuclear norm of the matrix, the case $p=2$ is the Frobenius norm, and $p=\infty$ is the operator norm.
%
%\item {\em Best rank-k approximation for unitarily invariant norms:} Show that the SVD provides the best rank-$k$ approximation for any unitarily invariant norm. See also 7.4.51 and 7.4.52 in: {\em Matrix Analysis}, Horn and Johnson, Cambridge University Press, 1985.
%
%\item {\em Closest rotation:} Given a square $n\times n$ matrix $A$ whose SVD is $A=U\Sigma V^T$, show that its closest (in the Frobenius norm) orthogonal matrix $R$ (satisfying $RR^T=R^TR=I$) is given by $R=UV^T$. That is, show that 
%    $$\|A - UV^T\|_F = \min_{RR^T=R^TR=I} \|A-R\|_F,$$ where
%    $A=U\Sigma V^T$.   
%    In other words, $R$ is obtained from the SVD of $A$ by dropping the diagonal matrix $\Sigma$. Use this observation to conclude what is the optimal rotation that aligns two sets of points $p_1,p_2,\ldots,p_n$ and $q_1,\ldots,q_n$ in $\mathbb{R}^d$, that is, find $R$ that minimizes $\sum_{i=1}^n \|Rp_i-q_i\|^2$. See also (the papers are posted on course website):\\
%    
%    $\bullet$ [Arun87] Arun, K. S., Huang, T. S., and Blostein, S. D., ``Least-squares fitting of two 3-D point sets", {\em IEEE Transactions on Pattern Analysis and Machine Intelligence,} {\bf 9} (5), pp. 698--700, 1987.\\
%    
%    $\bullet$ [Keller75] Keller, J. B., ``Closest Unitary, Orthogonal and Hermitian Operators to a Given Operator", {\em Mathematics Magazine}, {\bf 48} (4), pp. 192--197, 1975.\\
%    
%    $\bullet$ [FanHoffman55] Fan, K. and Hoffman, A. J., ``Some Metric Inequalities in the Space of Matrices",
%     {\em Proceedings of the American Mathematical Society},
%     {\bf 6} (1), pp. 111--116, 1955.
%    
%\end{enumerate}
%


\end{enumerate}

\end{document}


