{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "plt.style.use('fivethirtyeight')\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "\r\n",
    "import tqdm\r\n",
    "pd.set_option(\"display.max_columns\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('merge_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\r\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\r\n",
    "\r\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\r\n",
    "\r\n",
    "from sklearn.metrics import r2_score\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The recursive performance evaluation scheme\r\n",
    "train_score=[0]*30\r\n",
    "validation_score=[0]*30\r\n",
    "test_score=[0]*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS+H\r\n",
    "from sklearn.linear_model import SGDRegressor\r\n",
    "\r\n",
    "bestRSqr = float(\"-inf\")\r\n",
    "best_data1 = None\r\n",
    "bestOLS_H = None\r\n",
    "best_year = 0\r\n",
    "\r\n",
    "for i in range(0,30):\r\n",
    "    print('Batch {}:\\nTrain data:1960-{}\\nValuation data:{}-{}\\nTest data:{}-{}'.format(i,1978+i,1978+i,1990+i,1990+i,1991+i))\r\n",
    "    data_train=data2[(data2.yyyymm>=196001)&(data2.yyyymm<197801+i*100)]\r\n",
    "    data_validation=data2[(data2.yyyymm>=197801+i*100)&(data2.yyyymm<199001+i*100)]\r\n",
    "    data_test=data2[(data2.yyyymm>=196001+i*100)&(data2.yyyymm<196101+i*100)]\r\n",
    "\r\n",
    "    #Normalize\r\n",
    "    X_train=data_train.drop(\"excess_ret\",axis=1).copy()\r\n",
    "    X_train=scaler.fit_transform(X_train)\r\n",
    "    X_validation=data_validation.drop(\"excess_ret\",axis=1).copy()\r\n",
    "    X_validation=scaler.transform(X_validation)\r\n",
    "    X_test=data_test.drop(\"excess_ret\",axis=1).copy()\r\n",
    "    X_test=scaler.transform(X_test)\r\n",
    "\r\n",
    "    y_train=data_train[[\"excess_ret\"]].copy()\r\n",
    "    y_validation=data_validation[[\"excess_ret\"]].copy()\r\n",
    "    y_test=data_test[[\"excess_ret\"]].copy()\r\n",
    "    y_train=scaler.fit_transform(y_train)\r\n",
    "    y_test=scaler.transform(y_test)\r\n",
    "    y_validation=scaler.transform(y_validation)\r\n",
    "    \r\n",
    "    OLS_H = SGDRegressor(loss='huber',alpha=1e-3,epsilon=0.05,learning_rate='optimal')\r\n",
    "    OLS_H.fit(X_train, y_train)\r\n",
    "    y_train_pred=OLS_H.predict(X_train)\r\n",
    "    train_score[i]=r2_score(y_train,y_train_pred)\r\n",
    "    y_validation_pred=OLS_H.predict(X_validation)\r\n",
    "    validation_score[i]=r2_score(y_validation,y_validation_pred)\r\n",
    "    y_test_pred=OLS_H.predict(X_test)\r\n",
    "    test_score[i]=r2_score(y_test,y_test_pred)\r\n",
    "    currentRSqr = test_score[i]\r\n",
    "\r\n",
    "    if(currentRSqr>bestRSqr):\r\n",
    "        bestRSqr=currentRSqr\r\n",
    "        bestOLS_H = OLS_H\r\n",
    "        best_year = 196001+i*100\r\n",
    "        \r\n",
    "    del data_train,data_validation,data_test,X_train,y_train,X_test,y_test,X_validation,y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT RESULT\r\n",
    "test_score=pd.DataFrame(test_score)\r\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performence of OLS_H\r\n",
    "plt.figure(figsize=(10,5))\r\n",
    "plt.title(\"OLS_H Performance\")\r\n",
    "year=np.arange(1990,2020)\r\n",
    "plt.plot(year,test_score.values)\r\n",
    "plt.xlabel(\"year\")\r\n",
    "plt.ylabel(\"score--R-square\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Importance \r\n",
    "#X_test0 = best_data1.drop(best_data1.columns[0],axis=1,inplace=True).copy()\r\n",
    "\r\n",
    "X_test0 = data2[(data2.yyyymm>=best_year)&(data2.yyyymm<best_year+100)]\r\n",
    "print(X_test0.shape)\r\n",
    "a = X_test0.drop(\"excess_ret\",axis=1).copy()\r\n",
    "print(a.shape)\r\n",
    "id = a.columns\r\n",
    "L = len(a.columns)\r\n",
    "train_score=[0]*L\r\n",
    "validation_score=[0]*L\r\n",
    "test_score1 = [0]*L\r\n",
    "importance_OLS_H=[0]*L\r\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable importance\r\n",
    "\r\n",
    "for i in range(0,L):\r\n",
    "    \r\n",
    "    X_test = X_test0.drop(\"excess_ret\",axis=1).copy()\r\n",
    "    X_test.loc[:,id[i]]=0\r\n",
    "    X_test=scaler.fit_transform(X_test)\r\n",
    "\r\n",
    "    y_test=X_test0[[\"excess_ret\"]].copy()\r\n",
    "    y_test=scaler.fit_transform(y_test)\r\n",
    "    \r\n",
    "\r\n",
    "    y_test_pred=bestOLS_H.predict(X_test)\r\n",
    "    test_score1[i] = r2_score(y_test,y_test_pred)\r\n",
    "    importance_OLS_H[i] = bestRSqr - test_score1[i]\r\n",
    "\r\n",
    "    \r\n",
    "    del X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT\r\n",
    "importance_OLS_H = np.abs(importance_OLS_H)\r\n",
    "importance_OLS_H1 = pd.DataFrame(importance_OLS_H,columns=['Importance'], index=id)\r\n",
    "importance_OLS_H1 = importance_OLS_H1.sort_values(by='Importance',ascending=True)\r\n",
    "importance_OLS_H1 = importance_OLS_H1.tail(20)\r\n",
    "importance_OLS_H1.plot(kind='barh', figsize=(9, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance BY COE\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "a = data2.columns\r\n",
    "id = a.drop(\"excess_ret\").copy()\r\n",
    "\r\n",
    "importance_OLS_H = OLS_H.coef_\r\n",
    "importance_OLS_H = np.abs(importance_OLS_H)\r\n",
    "coefs_OLS_H = pd.DataFrame(importance_OLS_H,columns=['Importance'], index=id)\r\n",
    "coefs_OLS_H = coefs_OLS_H.sort_values(by='Importance',ascending=True)\r\n",
    "coefs_OLS_H= coefs_OLS_H.tail(20)\r\n",
    "coefs_OLS_H.plot(kind='barh', figsize=(9, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The recursive performance evaluation scheme\r\n",
    "train_score=[0]*30\r\n",
    "validation_score=[0]*30\r\n",
    "test_score=[0]*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS3\r\n",
    "from sklearn.linear_model import SGDRegressor\r\n",
    "\r\n",
    "bestRSqr = float(\"-inf\")\r\n",
    "best_data1 = None\r\n",
    "bestOLS_H3 = None\r\n",
    "best_year = 0\r\n",
    "\r\n",
    "for i in range(0,30):\r\n",
    "    print('Batch {}:\\nTrain data:1960-{}\\nValuation data:{}-{}\\nTest data:{}-{}'.format(i,1978+i,1978+i,1990+i,1990+i,1991+i))\r\n",
    "    data_train=data2[(data2.yyyymm>=196001)&(data2.yyyymm<197801+i*100)]\r\n",
    "    data_validation=data2[(data2.yyyymm>=197801+i*100)&(data2.yyyymm<199001+i*100)]\r\n",
    "    data_test=data2[(data2.yyyymm>=196001+i*100)&(data2.yyyymm<196101+i*100)]\r\n",
    "\r\n",
    "    #Normalize\r\n",
    "    X_train=data_train[['mom12m','bm','mvel1']].copy()\r\n",
    "    X_train=scaler.fit_transform(X_train)\r\n",
    "    X_validation=data_validation[['mom12m','bm','mvel1']].copy()\r\n",
    "    X_validation=scaler.transform(X_validation)\r\n",
    "    X_test=data_test[['mom12m','bm','mvel1']].copy()\r\n",
    "    X_test=scaler.transform(X_test)\r\n",
    "\r\n",
    "    y_train=data_train[[\"excess_ret\"]].copy()\r\n",
    "    y_validation=data_validation[[\"excess_ret\"]].copy()\r\n",
    "    y_test=data_test[[\"excess_ret\"]].copy()\r\n",
    "    y_train=scaler.fit_transform(y_train)\r\n",
    "    y_test=scaler.transform(y_test)\r\n",
    "    y_validation=scaler.transform(y_validation)\r\n",
    "    \r\n",
    "    OLS_H = SGDRegressor(loss='huber',alpha=1e-3,epsilon=0.05,learning_rate='optimal')\r\n",
    "    OLS_H.fit(X_train, y_train)\r\n",
    "    y_train_pred=OLS_H.predict(X_train)\r\n",
    "    train_score[i]=r2_score(y_train,y_train_pred)\r\n",
    "    y_validation_pred=OLS_H.predict(X_validation)\r\n",
    "    validation_score[i]=r2_score(y_validation,y_validation_pred)\r\n",
    "    y_test_pred=OLS_H.predict(X_test)\r\n",
    "    test_score[i]=r2_score(y_test,y_test_pred)\r\n",
    "    currentRSqr = test_score[i]\r\n",
    "\r\n",
    "    if(currentRSqr>bestRSqr):\r\n",
    "        bestRSqr=currentRSqr\r\n",
    "        bestOLS_H3 = OLS_H\r\n",
    "        best_year = 196001+i*100\r\n",
    "        \r\n",
    "    del data_train,data_validation,data_test,X_train,y_train,X_test,y_test,X_validation,y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT\r\n",
    "test_score=pd.DataFrame(test_score)\r\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enet best parameter\r\n",
    "import numpy as np\r\n",
    "from sklearn import linear_model\r\n",
    "from tqdm import tqdm\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "from sklearn.linear_model import SGDRegressor\r\n",
    "\r\n",
    "# store r2_score\r\n",
    "rList=[]\r\n",
    "combineList = []\r\n",
    "\r\n",
    "for i in range(30):\r\n",
    "    df_train = data2[(data2.yyyymm>=195703)&(data2.yyyymm<197503)]\r\n",
    "    df_validation = data2[(data2.yyyymm>=197503)&(data2.yyyymm<198703)]\r\n",
    "    df_test = data2[(data2.yyyymm>=198703)&(data2.yyyymm<201603)]\r\n",
    "\r\n",
    "    df_train = df_train.copy()\r\n",
    "    df_validation = df_validation.copy()\r\n",
    "    df_test = df_test.copy()\r\n",
    "\r\n",
    "    trainingstart = 195703\r\n",
    "    trainingend = 197503 + i*100\r\n",
    "    validend = trainingend + 1200\r\n",
    "    testend = validend + 100\r\n",
    "\r\n",
    "    trainingMask = (data2.yyyymm >= trainingstart) & (data2.yyyymm< trainingend)\r\n",
    "    trainingData = data2.loc[trainingMask]\r\n",
    "    \r\n",
    "    validationMask = (data2.yyyymm >= trainingend) & (data2.yyyymm< validend)\r\n",
    "    validationData = data2.loc[validationMask]\r\n",
    "    \r\n",
    "    testMask = (data2.yyyymm >= validend) & (data2.yyyymm < testend)\r\n",
    "    testData = data2.loc[testMask]\r\n",
    "    \r\n",
    "    \r\n",
    "    trainingData_y = trainingData['excess_ret']\r\n",
    "    trainingData_x = trainingData.drop(['excess_ret','permno', 'yyyymm'], axis=1)\r\n",
    "    \r\n",
    "    validationData_y = validationData['excess_ret']\r\n",
    "    validationData_x = validationData.drop(['excess_ret','permno', 'yyyymm'], axis=1)\r\n",
    "    \r\n",
    "    testData_y = testData['excess_ret']\r\n",
    "    testData_x = testData.drop(['excess_ret','permno', 'yyyymm'], axis =1)\r\n",
    "    \r\n",
    "    bestRSqr = float(\"-inf\")\r\n",
    "    bestEnet = None\r\n",
    "    bestCombine = None\r\n",
    "    \r\n",
    "    for alpha in [10e-04,10e-01]:\r\n",
    "        for l1 in [0,0.25,0.5,0.75,1]:\r\n",
    "            ENreg_H = SGDRegressor(loss='huber',penalty='elasticnet',alpha=alpha,  l1_ratio=l1, epsilon=0.05,max_iter=1e6,shuffle=False)\r\n",
    "            ENreg_H.fit(trainingData_x,trainingData_y)\r\n",
    "    \r\n",
    "            pred = ENreg_H.predict(validationData_x)\r\n",
    "            currentRSqr = r2_score(validationData_y.values, pred)\r\n",
    "            \r\n",
    "            if(currentRSqr>bestRSqr):\r\n",
    "                bestRSqr=currentRSqr\r\n",
    "                bestEnet=ENreg_H\r\n",
    "                bestCombine=(alpha,l1)\r\n",
    "        \r\n",
    "    resultPred = bestEnet.predict(testData_x)\r\n",
    "    resultRSqr = r2_score(testData_y.values, resultPred)\r\n",
    "    print('result R square for batch',i,': ', resultRSqr, 'best combine:', bestCombine)\r\n",
    "    rList.append(resultRSqr)\r\n",
    "    combineList.append(bestCombine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The recursive performance evaluation scheme\r\n",
    "train_score1=[0]*30\r\n",
    "validation_score1=[0]*30\r\n",
    "test_score1=[0]*30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enet\r\n",
    "from sklearn.linear_model import SGDRegressor\r\n",
    "\r\n",
    "bestRSqr = float(\"-inf\")\r\n",
    "bestEnet = None\r\n",
    "best_year = 0\r\n",
    "\r\n",
    "for i in range(0,30):\r\n",
    "    print('Batch {}:\\nTrain data:1960-{}\\nValuation data:{}-{}\\nTest data:{}-{}'.format(i,1978+i,1978+i,1990+i,1990+i,1991+i))\r\n",
    "    data_train=data2[(data2.yyyymm>=196001)&(data2.yyyymm<197801+i*100)]\r\n",
    "    data_validation=data2[(data2.yyyymm>=197801+i*100)&(data2.yyyymm<199001+i*100)]\r\n",
    "    data_test=data2[(data2.yyyymm>=196001+i*100)&(data2.yyyymm<196101+i*100)]\r\n",
    "\r\n",
    "    #Normalize\r\n",
    "    X_train=data_train.drop(\"excess_ret\",axis=1).copy()\r\n",
    "    X_train=scaler.fit_transform(X_train)\r\n",
    "    X_validation=data_validation.drop(\"excess_ret\",axis=1).copy()\r\n",
    "    X_validation=scaler.transform(X_validation)\r\n",
    "    X_test=data_test.drop(\"excess_ret\",axis=1).copy()\r\n",
    "    X_test=scaler.transform(X_test)\r\n",
    "\r\n",
    "    y_train=data_train[[\"excess_ret\"]].copy()\r\n",
    "    y_validation=data_validation[[\"excess_ret\"]].copy()\r\n",
    "    y_test=data_test[[\"excess_ret\"]].copy()\r\n",
    "    y_train=scaler.fit_transform(y_train)\r\n",
    "    y_test=scaler.transform(y_test)\r\n",
    "    y_validation=scaler.transform(y_validation)\r\n",
    "    \r\n",
    "    ENreg_H = SGDRegressor(loss='huber',penalty='elasticnet',alpha=1e-3,l1_ratio=0.5, epsilon=0.05,max_iter=1e6,shuffle=False)\r\n",
    "    ENreg_H.fit(X_train,y_train)\r\n",
    "    y_train_pred=ENreg_H.predict(X_train)\r\n",
    "    train_score1[i]=r2_score(y_train,y_train_pred)\r\n",
    "    y_validation_pred=ENreg_H.predict(X_validation)\r\n",
    "    validation_score1[i]=r2_score(y_validation,y_validation_pred)\r\n",
    "    y_test_pred=ENreg_H.predict(X_test)\r\n",
    "    test_score1[i]=r2_score(y_test,y_test_pred)\r\n",
    "    currentRSqr = test_score1[i]\r\n",
    "\r\n",
    "    if(currentRSqr>bestRSqr):\r\n",
    "        bestRSqr=currentRSqr\r\n",
    "        bestEnet=ENreg_H\r\n",
    "        best_year = 196001+i*100\r\n",
    "    \r\n",
    "    del data_train,data_validation,data_test,X_train,y_train,X_test,y_test,X_validation,y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\r\n",
    "test_score=pd.DataFrame(test_score1)\r\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performence of ENET-H\r\n",
    "plt.figure(figsize=(10,5))\r\n",
    "plt.title(\"ENT+H Performance\")\r\n",
    "year=np.arange(1990,2020)\r\n",
    "plt.plot(year,test_score1)\r\n",
    "plt.xlabel(\"year\")\r\n",
    "plt.ylabel(\"score--R-square\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Importance \r\n",
    "#X_test0 = best_data1.drop(best_data1.columns[0],axis=1,inplace=True).copy()\r\n",
    "\r\n",
    "X_test0 = data2[(data2.yyyymm>=best_year)&(data2.yyyymm<best_year+100)]\r\n",
    "print(X_test0.shape)\r\n",
    "a = X_test0.drop(\"excess_ret\",axis=1).copy()\r\n",
    "print(a.shape)\r\n",
    "id = a.columns\r\n",
    "L = len(a.columns)\r\n",
    "train_score=[0]*L\r\n",
    "validation_score=[0]*L\r\n",
    "test_score1 = [0]*L\r\n",
    "importance_ENreg_H=[0]*L\r\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable importance\r\n",
    "\r\n",
    "for i in range(0,L):\r\n",
    "    \r\n",
    "    X_test = X_test0.drop(\"excess_ret\",axis=1).copy()\r\n",
    "    X_test.loc[:,id[i]]=0\r\n",
    "    X_test=scaler.fit_transform(X_test)\r\n",
    "\r\n",
    "    y_test=X_test0[[\"excess_ret\"]].copy()\r\n",
    "    y_test=scaler.fit_transform(y_test)\r\n",
    "    \r\n",
    "\r\n",
    "    y_test_pred=bestEnet.predict(X_test)\r\n",
    "    test_score1[i] = r2_score(y_test,y_test_pred)\r\n",
    "    importance_ENreg_H[i] = bestRSqr - test_score1[i]\r\n",
    "\r\n",
    "    \r\n",
    "    del X_test,y_test,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\r\n",
    "importance_ENreg_H = np.abs(importance_ENreg_H)\r\n",
    "importance_ENreg_H1 = pd.DataFrame(importance_ENreg_H,columns=['Importance'], index=id)\r\n",
    "importance_ENreg_H1 = importance_ENreg_H1.sort_values(by='Importance',ascending=True)\r\n",
    "importance_ENreg_H1 = importance_ENreg_H1.tail(20)\r\n",
    "importance_ENreg_H1.plot(kind='barh', figsize=(9, 7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit (conda)",
   "name": "python370jvsc74a57bd0d351f2582fb8059b3c2081c3e2402978b636173226be95e0dabb346cc1ecff00"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}