{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is the neural network implementation. It is run on the google colab, so the address of the files need to be changed if run on other devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/content/drive/My Drive/GKX_20201231.csv')\n",
    "\n",
    "#select data from 1960 to 2019\n",
    "data = data[data.DATE >= 19600100][data.DATE <=20191230].reset_index(drop = True) \n",
    "\n",
    "data['DATE']=data['DATE']//100 \n",
    "data = data.rename(columns={'DATE': 'yyyymm'})#Let the DATE become the format \"yyyymm\"\n",
    "\n",
    "#merge two datasets and fill mixing value\n",
    "dm=pd.read_excel('/content/drive/My Drive/macro.xlsx')\n",
    "dm = dm.loc[(dm['yyyymm']>=196001)&(dm['yyyymm']<=201912)]\n",
    "\n",
    "dm['dp']=dm['D12'].apply(math.log)-dm['Index'].apply(math.log)\n",
    "dm['ep']=dm['E12'].apply(math.log)-dm['Index'].apply(math.log)\n",
    "dm['tms']=dm['lty']-dm['tbl']\n",
    "dm['dfy']=dm['BAA']-dm['AAA']\n",
    "#Rename b/m by bm and Keep only 8 needed macropredictors\n",
    "ned=['yyyymm', 'dp', 'ep', 'bm', 'ntis', 'tbl', 'tms', 'dfy','svar']\n",
    "dm=dm.rename(columns={'b/m':'bm'})[ned]\n",
    "dm.to_csv('macro.csv',index=False)\n",
    "\n",
    "macro = pd.read_csv('macro.csv')\n",
    "macro\n",
    "\n",
    "data1 = pd.merge(data, macro, how='left', on='yyyymm', suffixes=('', '_macro'))\n",
    "\n",
    "data1['excess_ret']=data1['RET']-data1['tbl'] #Calculate return in excess of risk-free rate\n",
    "\n",
    "nonpd = ['yyyymm','RET','SHROUT','mve0','prc','permno','excess_ret'] #Nonpredictors\n",
    "macropd = ['dp','ep_macro','bm_macro','ntis','tbl','tms','dfy','svar'] #Macroeconomic predictors\n",
    "sic2 = ['sic2'] #Industrial dummies\n",
    "lst = nonpd+macropd+sic2\n",
    "stockpd = [p for p in data1.columns if p not in lst] #Stock-level predictors\n",
    "len(stockpd)\n",
    "\n",
    "period_list = np.unique(data1['yyyymm'])\n",
    "len(period_list)\n",
    "\n",
    "for pd in stockpd:\n",
    "    c_row = 0\n",
    "    for period in period_list:\n",
    "        n_period = data1[data1['yyyymm']==period].shape[0]\n",
    "        #Compute the cross-sectional median of this stock predictor\n",
    "        median = np.nanmedian(data1.loc[c_row:(c_row+n_period-1),pd])\n",
    "        #Fill missing values with median\n",
    "        data1.loc[c_row:(c_row+n_period-1),pd] = data1.loc[c_row:(c_row+n_period-1),pd].replace(np.nan,median)\n",
    "        c_row = c_row + n_period\n",
    "\n",
    "newdt = data1.fillna(0)\n",
    "\n",
    "newdt = newdt.drop(columns=['prc','SHROUT','mve0']) #Drop the predictors that are not used\n",
    "newdt.shape\n",
    "\n",
    "newdt.to_csv('merge_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "df = pd.read_csv(\"/content/drive/My Drive/merge_data.csv\")\n",
    "\n",
    "\n",
    "#clear nan\n",
    "def handling_nan(df):\n",
    "  for column in df.columns:\n",
    "    if df[column].isnull().sum() == 0:\n",
    "      continue\n",
    "    #if null>half，drop this column\n",
    "    elif (df[column].isnull().sum()/len(df[column])) > 0.5:\n",
    "      df.drop(column, axis = 1, inplace = True)\n",
    "    #if null<half，fill with mean value\n",
    "    else:\n",
    "      df[column].fillna(df[column].mean(), inplace = True)\n",
    "\n",
    "\n",
    "# training data & valid data\n",
    "train = df[df[\"yyyymm\"] < 197800]\n",
    "valid = df[(df[\"yyyymm\"] > 197800)&(df[\"yyyymm\"] < 199000)]\n",
    "\n",
    "x_train = train.drop([\"permno\",\"yyyymm\",'excess_ret','RET'],axis = 1)\n",
    "x_valid = valid.drop([\"permno\",\"yyyymm\",'excess_ret','RET',],axis = 1)\n",
    "\n",
    "y_train = np.array(train['excess_ret']).reshape(-1,1)\n",
    "y_valid = np.array(valid['excess_ret']).reshape(-1,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "\n",
    "# test data\n",
    "def testing_data(year_of_test):\n",
    "  end_of_test = (year_of_test+1)*100\n",
    "\n",
    "\n",
    "  #handling nan\n",
    "  handling_nan(df)\n",
    "\n",
    "  \n",
    "  \n",
    "  test = df[(df[\"yyyymm\"] > year_of_test*100)&(df[\"yyyymm\"] < end_of_test)]\n",
    "\n",
    "  #x,y\n",
    "  \n",
    "  x_test = test.drop([\"permno\",\"yyyymm\",'excess_ret','RET'],axis = 1)\n",
    "  \n",
    "  y_test = np.array(test['excess_ret']).reshape(-1,1)\n",
    "\n",
    "\n",
    "  #StandardScale\n",
    "  scaler = StandardScaler()\n",
    "  x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "  return x_test, y_test\n",
    "\n",
    "#define R^2 function\n",
    "def R_square(ypred,ytrue): \n",
    "  dif2=np.sum(np.power(ytrue-ypred,2))\n",
    "  return 1-(dif2/np.sum(np.power(ytrue,2)))\n",
    "\n",
    "#define loss function\n",
    "def R_loss(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.square(y_true-y_pred))/tf.reduce_mean(tf.square(y_true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training proces(choose one from NN1-NN5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquare_oos_valid = []\n",
    "rsquare_oos_test = []\n",
    "\n",
    "#NN1 \n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(32, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "sgd = keras.optimizers.SGD(learning_rate = 0.0002, clipnorm = 0.5)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss = R_loss, optimizer= sgd)\n",
    "  \n",
    "EarlyStop = keras.callbacks.EarlyStopping(\n",
    "      monitor='val_loss',\n",
    "      patience=5,\n",
    "      verbose=1,\n",
    "      min_delta=0.001, \n",
    "      mode='min')\n",
    "  \n",
    "Reduce = keras.callbacks.ReduceLROnPlateau(\n",
    "      monitor='val_loss',\n",
    "      factor=0.2,\n",
    "      patience=3,\n",
    "      verbose=1,\n",
    "      mode='min',\n",
    "      min_delta=0.001,\n",
    "      cooldown=0,\n",
    "      min_lr=0)\n",
    "\n",
    "model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      validation_data = (x_valid, y_valid),\n",
    "      epochs = 50,\n",
    "      callbacks = [Reduce,EarlyStop])\n",
    "  \n",
    "model.save('/content/drive/My Drive/NN_models/NN1.h5')\n",
    "\n",
    "rsquare_oos_valid.append(R_square(model.predict(x_valid),y_valid))\n",
    "print(rsquare_oos_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquare_oos_valid = []\n",
    "rsquare_oos_test = []\n",
    "\n",
    "#nn2\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(32, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "sgd = keras.optimizers.SGD(learning_rate = 0.0002, clipnorm = 0.5)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss = R_loss, optimizer= sgd)\n",
    "  \n",
    "EarlyStop = keras.callbacks.EarlyStopping(\n",
    "      monitor='val_loss',\n",
    "      patience=5,\n",
    "      verbose=1,\n",
    "      min_delta=0.001, \n",
    "      mode='min')\n",
    "  \n",
    "Reduce = keras.callbacks.ReduceLROnPlateau(\n",
    "      monitor='val_loss',\n",
    "      factor=0.2,\n",
    "      patience=3,\n",
    "      verbose=1,\n",
    "      mode='min',\n",
    "      min_delta=0.001,\n",
    "      cooldown=0,\n",
    "      min_lr=0)\n",
    "\n",
    "model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      validation_data = (x_valid, y_valid),\n",
    "      epochs = 50,\n",
    "      callbacks = [Reduce,EarlyStop])\n",
    "  \n",
    "model.save('/content/drive/My Drive/NN_models/NN2.h5')\n",
    "\n",
    "rsquare_oos_valid.append(R_square(model.predict(x_valid),y_valid))\n",
    "print(rsquare_oos_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d4887",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquare_oos_valid = []\n",
    "rsquare_oos_test = []\n",
    "\n",
    "#nn3\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(32, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "sgd = keras.optimizers.SGD(learning_rate = 0.0002, clipnorm = 0.5)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss = R_loss, optimizer= sgd)\n",
    "  \n",
    "EarlyStop = keras.callbacks.EarlyStopping(\n",
    "      monitor='val_loss',\n",
    "      patience=5,\n",
    "      verbose=1,\n",
    "      min_delta=0.001, \n",
    "      mode='min')\n",
    "  \n",
    "Reduce = keras.callbacks.ReduceLROnPlateau(\n",
    "      monitor='val_loss',\n",
    "      factor=0.2,\n",
    "      patience=3,\n",
    "      verbose=1,\n",
    "      mode='min',\n",
    "      min_delta=0.001,\n",
    "      cooldown=0,\n",
    "      min_lr=0)\n",
    "\n",
    "model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      validation_data = (x_valid, y_valid),\n",
    "      epochs = 50,\n",
    "      callbacks = [Reduce,EarlyStop])\n",
    "  \n",
    "model.save('/content/drive/My Drive/NN_models/NN3.h5')\n",
    "\n",
    "rsquare_oos_valid.append(R_square(model.predict(x_valid),y_valid))\n",
    "print(rsquare_oos_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquare_oos_valid = []\n",
    "rsquare_oos_test = []\n",
    "\n",
    "#nn4\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(32, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "sgd = keras.optimizers.SGD(learning_rate = 0.0002, clipnorm = 0.5)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss = R_loss, optimizer= sgd)\n",
    "  \n",
    "EarlyStop = keras.callbacks.EarlyStopping(\n",
    "      monitor='val_loss',\n",
    "      patience=5,\n",
    "      verbose=1,\n",
    "      min_delta=0.001, \n",
    "      mode='min')\n",
    "  \n",
    "Reduce = keras.callbacks.ReduceLROnPlateau(\n",
    "      monitor='val_loss',\n",
    "      factor=0.2,\n",
    "      patience=3,\n",
    "      verbose=1,\n",
    "      mode='min',\n",
    "      min_delta=0.001,\n",
    "      cooldown=0,\n",
    "      min_lr=0)\n",
    "\n",
    "model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      validation_data = (x_valid, y_valid),\n",
    "      epochs = 50,\n",
    "      callbacks = [Reduce,EarlyStop])\n",
    "  \n",
    "model.save('/content/drive/My Drive/NN_models/NN4.h5')\n",
    "\n",
    "rsquare_oos_valid.append(R_square(model.predict(x_valid),y_valid))\n",
    "print(rsquare_oos_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquare_oos_valid = []\n",
    "rsquare_oos_test = []\n",
    "\n",
    "#nn5\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(32, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "sgd = keras.optimizers.SGD(learning_rate = 0.0002, clipnorm = 0.5)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss = R_loss, optimizer= sgd)\n",
    "  \n",
    "EarlyStop = keras.callbacks.EarlyStopping(\n",
    "      monitor='val_loss',\n",
    "      patience=5,\n",
    "      verbose=1,\n",
    "      min_delta=0.001, \n",
    "      mode='min')\n",
    "  \n",
    "Reduce = keras.callbacks.ReduceLROnPlateau(\n",
    "      monitor='val_loss',\n",
    "      factor=0.2,\n",
    "      patience=3,\n",
    "      verbose=1,\n",
    "      mode='min',\n",
    "      min_delta=0.001,\n",
    "      cooldown=0,\n",
    "      min_lr=0)\n",
    "\n",
    "model.fit(\n",
    "      x_train,\n",
    "      y_train,\n",
    "      validation_data = (x_valid, y_valid),\n",
    "      epochs = 50,\n",
    "      callbacks = [Reduce,EarlyStop])\n",
    "  \n",
    "model.save('/content/drive/My Drive/NN_models/NN5.h5')\n",
    "\n",
    "rsquare_oos_valid.append(R_square(model.predict(x_valid),y_valid))\n",
    "print(rsquare_oos_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "for year in range(1990,2020):\n",
    "  print(\"start testing the data in \"+str(year))\n",
    "  x_test, y_test = testing_data(year)\n",
    "  \n",
    "\n",
    "  rsquare_oos_test.append(R_square(model.predict(x_test),y_test))\n",
    "  np.savetxt('R2_'+str(year)+'.csv', \n",
    "           rsquare_oos_test,\n",
    "           delimiter =\", \", \n",
    "           fmt ='% s')\n",
    "  \n",
    "  print(rsquare_oos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot R2\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "  \n",
    "x = []\n",
    "y = []\n",
    "\n",
    "R2 = pd.read_csv('R2_2019.csv')\n",
    "R2 = R2.to_numpy()\n",
    "\n",
    "      \n",
    "for i in range(1,len(R2)+1):\n",
    "      x.append(i+1989)\n",
    "      y.append(R2[i-1].item())\n",
    "  \n",
    "plt.bar(x, y, color = 'b', width = 0.72, label = \"R_squre\")\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('R_squre')\n",
    "plt.title('R_squre from 1990-2019')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(R2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_score_nn1 = []\n",
    "handling_nan(df)\n",
    "max_year = rsquare_oos_test.idxmax()\n",
    "# R^2 for year max_year+1990 is the max\n",
    "test = df[(df[\"yyyymm\"] > (max_year+1990)*100)&(df[\"yyyymm\"] < (max_year+1991)*100)]\n",
    "\n",
    "  \n",
    "x_test = test.drop([\"permno\",\"yyyymm\",'RET','excess_ret'],axis = 1)\n",
    "y_test = np.array(test['excess_ret']).reshape(-1,1)\n",
    "\n",
    "for col in range(1,len(x_test.columns)):\n",
    "    tt =x_test.copy()\n",
    "    #tt.iloc[:,col].fillna(0,inplace=True)\n",
    "    tt.iloc[:,col] = 0\n",
    "    #tt.iloc[:,:col] = 0\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    tt = scaler.fit_transform(tt)\n",
    "\n",
    "    feature_score_nn1.append(np.abs(R_square(model.predict(tt),y_test)-R2[max_year]))\n",
    "\n",
    "print(feature_score_nn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot variable importance\n",
    "id = x_test.columns[1:].copy()\n",
    "\n",
    "importance_nn1 = pd.DataFrame(feature_score_nn1,columns=['Importance'], index=id)\n",
    "importance_nn1 = importance_nn1.sort_values(by='Importance',ascending = True)\n",
    "importance_nn1 = importance_nn1.tail(20)\n",
    "importance_nn1.plot(kind='barh',figsize=(9,7))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
