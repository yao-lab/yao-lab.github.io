{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shO_WdZhIY7z"
      },
      "source": [
        "# MATH5470 Project: (Re-)Imag(in)ing Price Trends\n",
        "\n",
        "This jupyter notebook is composed of 5 parts.\n",
        "\n",
        "1. Data processing\n",
        "2. Baseline model (including model construction, training and testing)\n",
        "3. Sensitivity analysis\n",
        "4. Grad-CAM\n",
        "5. Regression model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-LPA7LEhZI9",
        "outputId": "bff555bc-49c0-41d8-ca60-0ae4979c1255"
      },
      "outputs": [],
      "source": [
        "#Download and unzip the dataset.\n",
        "!wget https://www.dropbox.com/s/njehqednn8mycze/img_data.zip?dl=0\n",
        "!unzip img_data.zip?dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_tRarq_U3Zq"
      },
      "source": [
        "Create and save training, validation, and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y-sCpZ0miXz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import  os \n",
        "import pandas as pd\n",
        "\n",
        "#load the image and label dataset\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "image_list = [i for i in file_list if 'dat' in i]\n",
        "label_list = [i for i in file_list if 'feather' in i]\n",
        "image_list.sort()\n",
        "label_list.sort()\n",
        "\n",
        "image_train_val, image_test = image_list[:7],image_list[7:] \n",
        "label_train_val, label_test = label_list[:7],label_list[7:] \n",
        "\n",
        "\n",
        "image_train_val_arr, image_test_arr = [], []\n",
        "label_train_val_arr, label_test_arr = [], [] \n",
        "for i, j in zip(image_train_val, label_train_val):    \n",
        "    image_train_val_arr.append(np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60)))\n",
        "    label_train_val_arr.append(pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values)\n",
        "\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr.append(np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60)))\n",
        "    label_test_arr.append(pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5GLQneAWJYm"
      },
      "source": [
        "Code for cold start if the data haven't been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ru088zcn7iM"
      },
      "outputs": [],
      "source": [
        "cold_start = False\n",
        "\n",
        "if cold_start:\n",
        "  image_train_val_data = np.concatenate(image_train_val_arr, 0)\n",
        "  image_train_val_data[image_train_val_data==255]=1\n",
        "  label_train_val_data = np.concatenate(label_train_val_arr, 0)\n",
        "\n",
        "  image_train_data, image_val_data = image_train_val_data[:int(0.7*len(image_train_val_data))], image_train_val_data[int(0.7*len(image_train_val_data)):]\n",
        "  label_train_data, label_val_data = label_train_val_data[:int(0.7*len(label_train_val_data))], label_train_val_data[int(0.7*len(label_train_val_data)):]\n",
        "\n",
        "  np.save('/content/drive/MyDrive/MATH5470/dataset/train_x.npy', image_train_data)\n",
        "  np.save('/content/drive/MyDrive/MATH5470/dataset/train_y.npy', label_train_data)\n",
        "  np.save('/content/drive/MyDrive/MATH5470/dataset/val_x.npy', image_val_data)\n",
        "  np.save('/content/drive/MyDrive/MATH5470/dataset/val_y.npy', label_val_data)\n",
        "\n",
        "  image_test_data = np.concatenate(image_test_arr, 0)\n",
        "  image_test_data[image_test_data==255]=1\n",
        "  label_test_data = np.concatenate(label_test_arr, 0)\n",
        "\n",
        "  np.save('/content/drive/MyDrive/MATH5470/dataset/test_x.npy', image_test_data)\n",
        "  np.save('/content/drive/MyDrive/MATH5470/dataset/test_y.npy', label_test_data)\n",
        "\n",
        "else:\n",
        "  image_train_data = np.load(\"/content/drive/MyDrive/MATH5470/dataset/train_x.npy\")\n",
        "  image_val_data = np.load(\"/content/drive/MyDrive/MATH5470/dataset/val_x.npy\")\n",
        "  label_train_data = np.load(\"/content/drive/MyDrive/MATH5470/dataset/train_y.npy\")\n",
        "  label_val_data = np.load(\"/content/drive/MyDrive/MATH5470/dataset/val_y.npy\")\n",
        "  image_test_data = np.load(\"/content/drive/MyDrive/MATH5470/dataset/test_x.npy\")\n",
        "  label_test_data = np.load(\"/content/drive/MyDrive/MATH5470/dataset/test_y.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyqfZxnKThc4",
        "outputId": "89f74882-d388-4ff7-aa75-29d8e44f5e48"
      },
      "outputs": [],
      "source": [
        "print(\"The size of training image is \" + str(image_train_data.shape))\n",
        "print(\"The size of training label is \" + str(label_train_data.shape))\n",
        "print(\"The size of validation image is \" + str(image_val_data.shape))\n",
        "print(\"The size of validation label is \" + str(label_val_data.shape))\n",
        "print(\"The size of testing image is \" + str(image_test_data.shape))\n",
        "print(\"The size of testing label is \" + str(label_test_data.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEv50wOvlqv_"
      },
      "source": [
        "DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUBLAEBw9_GC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_file_path, label_file_path, binary=True):\n",
        "        self.data = np.load(data_file_path)\n",
        "        self.label = np.load(label_file_path)\n",
        "        self.binary = binary\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.label[index]\n",
        "        x = torch.from_numpy(x).float()\n",
        "        x = x.unsqueeze(0)\n",
        "        if self.binary:\n",
        "            y = np.where(y > 0, 1, 0)\n",
        "        y = torch.from_numpy(y).float()\n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, data_file, label_file, binary=True):\n",
        "        self.data = data_file\n",
        "        self.label = label_file\n",
        "        self.binary = binary\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.label[index]\n",
        "        x = torch.from_numpy(x).float()\n",
        "        x = x.unsqueeze(0)\n",
        "        if self.binary:\n",
        "            y = np.where(y > 0, 1, 0)\n",
        "        y = torch.from_numpy(y).float()\n",
        "        return x,y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0K51PVFmC40"
      },
      "source": [
        "## CNN baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2-r6PcY-MsG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class ConvNet(nn.Module):\n",
        "    \"\"\"Encoder for feature embedding\"\"\"\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 64, kernel_size=(5,3), padding=(5, 3), stride=(3,1) ,dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d(2,1))\n",
        "        nn.init.xavier_uniform_(self.layer1[0].weight)\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,128,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer2[0].weight)\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(128,256,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer3[0].weight)\n",
        "        self.fc = nn.Linear(18176, 2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        result = self.fc(out)\n",
        "        return result\n",
        "\n",
        "def conv3():\n",
        "    return ConvNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVGo-3nomIxb"
      },
      "source": [
        "### Pretraining of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_9PM_L8_NCY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from copy import deepcopy\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Averager():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n = 0\n",
        "        self.v = 0\n",
        "\n",
        "    def add(self, x):\n",
        "        self.v = (self.v * self.n + x) / (self.n + 1)\n",
        "        self.n += 1\n",
        "\n",
        "    def item(self):\n",
        "        return self.v\n",
        "\n",
        "\n",
        "def pretrain(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    ## training with ce\n",
        "    loss_avg = Averager() \n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        train_inputs, train_targets = batch[0], batch[1]\n",
        "        train_targets = train_targets.long()\n",
        "        train_inputs = train_inputs.to(device=device)\n",
        "        train_targets = train_targets.to(device=device)\n",
        "        train_logits = model(train_inputs)\n",
        "        loss = nn.CrossEntropyLoss()(train_logits, train_targets)\n",
        "        loss_avg.add(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"Train Loss %.4f\" % (loss_avg.item()))\n",
        "    return loss_avg.item()\n",
        "\n",
        "def evaluate_batch(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = num = 0\n",
        "    for iter, pack in enumerate(data_loader):\n",
        "        data, target = pack[0].to(device), pack[1].to(device)\n",
        "        targets = target.long()\n",
        "        logits = model(data)\n",
        "        _, pred = logits.max(1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        num += data.shape[0]\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    return correct / num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9sVIsnPmklg"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezKqTVj7BtFN",
        "outputId": "f42b6621-737d-40dd-cabe-7a2a56e8d616"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import  os \n",
        "\n",
        "trainset = ImageDataset(\"/content/drive/MyDrive/MATH5470/dataset/train_x.npy\",\"/content/drive/MyDrive/MATH5470/dataset/train_y.npy\")\n",
        "valset = ImageDataset(\"/content/drive/MyDrive/MATH5470/dataset/val_x.npy\",\"/content/drive/MyDrive/MATH5470/dataset/val_y.npy\")\n",
        "batch_size = 64\n",
        "epoch = 15\n",
        "lr = 1e-3\n",
        "wd =  5e-4\n",
        "model_name = 'weight.pth'\n",
        "train_loader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "\n",
        "device = 'cuda'\n",
        "model = conv3().cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGRlmdnba99e",
        "outputId": "258c6e51-448a-40c5-8459-4314c581f88a"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2sLNJN_zaqW"
      },
      "source": [
        "## Alternate CNN models and sensitivity analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vLgxTdDzZyJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class AltConvNet(nn.Module):\n",
        "    \"\"\"Encoder for feature embedding\"\"\"\n",
        "    def __init__(self, dropout_rate=0.5, Xavier=True, pool_size=(2,1),filter_size=(5,3),dilation=(2,1), stride=(3,1)):\n",
        "        super(AltConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 64, kernel_size=(5,3), padding=(5,3), stride=stride ,dilation=dilation),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((pool_size)))\n",
        "        if Xavier:\n",
        "          nn.init.xavier_uniform_(self.layer1[0].weight)\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,128,kernel_size=(5,3),padding=(5,3), stride=stride ,dilation=dilation),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((pool_size)),)\n",
        "        if Xavier:\n",
        "          nn.init.xavier_uniform_(self.layer2[0].weight)\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(128,256,kernel_size=(5,3),padding=(5,3), stride=stride ,dilation=dilation),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((pool_size)),)\n",
        "        if Xavier:\n",
        "          nn.init.xavier_uniform_(self.layer3[0].weight)\n",
        "        self.fc = nn.Linear(18432, 2)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        result = self.fc(out)\n",
        "        return result\n",
        "\n",
        "def Altconv3(dropout_rate=0.5, Xavier=True, pool_size=(2,1),filter_size=(5,3),dilation=(2,1), stride=(3,1)):\n",
        "    return AltConvNet(dropout_rate, Xavier, pool_size,filter_size,dilation, stride)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KipESNGiGoBV",
        "outputId": "28641b94-85fb-4ffd-b1ee-af1200b26c22"
      },
      "outputs": [],
      "source": [
        "model = Altconv3(dropout_rate=0).cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5e2qLP6OPKj",
        "outputId": "62ee0ba7-3734-42d6-cc82-3d68d3620e70"
      },
      "outputs": [],
      "source": [
        "model = Altconv3(dropout_rate=0.25).cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8kPrUjA6Kbb",
        "outputId": "d75b0806-1b92-4d4b-8990-cb1754f3b84b"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "model = Altconv3(dropout_rate=0.75).cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AzZ9eC37lPm",
        "outputId": "618f451e-f8c1-46c5-b6d2-a19b116e24a7"
      },
      "outputs": [],
      "source": [
        "model = Altconv3(Xavier=False).cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "SBfKV_D4_J2P",
        "outputId": "5921d30b-5991-4fb1-ee19-02eaf74ed018"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import  os \n",
        "\n",
        "trainset = ImageDataset(\"/content/drive/MyDrive/MATH5470/dataset/train_x.npy\",\"/content/drive/MyDrive/MATH5470/dataset/train_y.npy\")\n",
        "valset = ImageDataset(\"/content/drive/MyDrive/MATH5470/dataset/val_x.npy\",\"/content/drive/MyDrive/MATH5470/dataset/val_y.npy\")\n",
        "batch_size = 64\n",
        "epoch = 15\n",
        "lr = 1e-3\n",
        "wd =  5e-4\n",
        "model_name = 'weight.pth'\n",
        "train_loader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "device = 'cuda'\n",
        "model = Altconv3(filter_size=(3,3)).cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fM7fevujnni",
        "outputId": "7b08c220-a821-4071-fe3e-3cbd8e3b52d2"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RAqpoVToae9",
        "outputId": "607e44a4-965f-4367-9f6a-1ba5999cc68e"
      },
      "outputs": [],
      "source": [
        "model = Altconv3(filter_size=(7,3)).cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuU20CYmLevM"
      },
      "source": [
        "### Try different number of filter in the first layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFRJh5nKI7lV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class AltFilterConvNet(nn.Module):\n",
        "    \"\"\"Encoder for feature embedding\"\"\"\n",
        "    def __init__(self):\n",
        "        super(AltFilterConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 32, kernel_size=(5,3), padding=(5, 3), stride=(3,1) ,dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(32),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d(2,1))\n",
        "        nn.init.xavier_uniform_(self.layer1[0].weight)\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(32,128,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer2[0].weight)\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(128,256,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer3[0].weight)\n",
        "        self.fc = nn.Linear(18176, 2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        result = self.fc(out)\n",
        "        return result\n",
        "\n",
        "def AltFilterconv3():\n",
        "    return AltFilterConvNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXLxKJC0Jz8F",
        "outputId": "cdeaaf5b-dc84-4854-d328-fd614a4abead"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import  os \n",
        "\n",
        "trainset = ImageDataset(\"/content/drive/MyDrive/MATH5470/dataset/train_x.npy\",\"/content/drive/MyDrive/MATH5470/dataset/train_y.npy\")\n",
        "valset = ImageDataset(\"/content/drive/MyDrive/MATH5470/dataset/val_x.npy\",\"/content/drive/MyDrive/MATH5470/dataset/val_y.npy\")\n",
        "batch_size = 32\n",
        "epoch = 15\n",
        "lr = 1e-3\n",
        "wd =  5e-4\n",
        "model_name = 'weight.pth'\n",
        "train_loader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "device = 'cuda'\n",
        "model = AltFilterconv3().cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh9WbesVLrOs"
      },
      "outputs": [],
      "source": [
        "class AltFilterConvNet(nn.Module):\n",
        "    \"\"\"Encoder for feature embedding\"\"\"\n",
        "    def __init__(self):\n",
        "        super(AltFilterConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 128, kernel_size=(5,3), padding=(5, 3), stride=(3,1) ,dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d(2,1))\n",
        "        nn.init.xavier_uniform_(self.layer1[0].weight)\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(128,128,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer2[0].weight)\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(128,256,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer3[0].weight)\n",
        "        self.fc = nn.Linear(18176, 2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        result = self.fc(out)\n",
        "        return result\n",
        "\n",
        "def AltFilterconv3():\n",
        "    return AltFilterConvNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRCbnZJjL1ko",
        "outputId": "cba3f3d2-df87-4fcf-aa76-95f85d2ff746"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epoch = 15\n",
        "lr = 1e-3\n",
        "wd =  5e-4\n",
        "model_name = 'weight.pth'\n",
        "train_loader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "device = 'cuda'\n",
        "model = AltFilterconv3().cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIOtM-j8L7-K"
      },
      "source": [
        "### Try different layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0_8mNRqMAEM"
      },
      "outputs": [],
      "source": [
        "class AltFilterConvNet(nn.Module):\n",
        "    \"\"\"Encoder for feature embedding\"\"\"\n",
        "    def __init__(self):\n",
        "        super(AltFilterConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 64, kernel_size=(5,3), padding=(5, 3), stride=(3,1) ,dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d(2,1))\n",
        "        nn.init.xavier_uniform_(self.layer1[0].weight)\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,128,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer2[0].weight)\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(128,256,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer3[0].weight)\n",
        "        self.fc = nn.Linear(34304, 2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        result = self.fc(out)\n",
        "        return result\n",
        "\n",
        "def AltFilterconv3():\n",
        "    return AltFilterConvNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOuX4sXeMiSr",
        "outputId": "92b7cd5b-1df0-4791-a5cf-4f5ad8786c8d"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epoch = 15\n",
        "lr = 1e-3\n",
        "wd =  5e-4\n",
        "model_name = 'weight.pth'\n",
        "train_loader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "device = 'cuda'\n",
        "model = AltFilterconv3().cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRNW5jdTOS3e"
      },
      "source": [
        "### Try batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7tLiTuBOSRa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class AltConvNet(nn.Module):\n",
        "    \"\"\"Encoder for feature embedding\"\"\"\n",
        "    def __init__(self):\n",
        "        super(AltConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 64, kernel_size=(5,3), padding=(5, 3), stride=(3,1) ,dilation=(2,1)),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d(2,1))\n",
        "        nn.init.xavier_uniform_(self.layer1[0].weight)\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,128,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer2[0].weight)\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(128,256,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer3[0].weight)\n",
        "        self.fc = nn.Linear(18176, 2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        result = self.fc(out)\n",
        "        return result\n",
        "\n",
        "def Altconv3():\n",
        "    return AltConvNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WW3asd9Ot7y",
        "outputId": "fa167891-07d2-4dff-f12a-542e3c6a1cd7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import  os \n",
        "\n",
        "trainset = ImageDataset(\"/content/drive/MyDrive/MATH5470/dataset/train_x.npy\",\"/content/drive/MyDrive/MATH5470/dataset/train_y.npy\")\n",
        "valset = ImageDataset(\"/content/drive/MyDrive/MATH5470/dataset/val_x.npy\",\"/content/drive/MyDrive/MATH5470/dataset/val_y.npy\")\n",
        "batch_size = 32\n",
        "epoch = 15\n",
        "lr = 1e-3\n",
        "wd =  5e-4\n",
        "model_name = 'weight.pth'\n",
        "train_loader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "device = 'cuda'\n",
        "model = Altconv3().cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk4JFRWCOy5y"
      },
      "source": [
        "### Try different activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkV7q7bvO4CC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class AltConvNet(nn.Module):\n",
        "    \"\"\"Encoder for feature embedding\"\"\"\n",
        "    def __init__(self):\n",
        "        super(AltConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 64, kernel_size=(5,3), padding=(5, 3), stride=(3,1) ,dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2,1))\n",
        "        nn.init.xavier_uniform_(self.layer1[0].weight)\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,128,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer2[0].weight)\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(128,256,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        nn.init.xavier_uniform_(self.layer3[0].weight)\n",
        "        self.fc = nn.Linear(18176, 2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        result = self.fc(out)\n",
        "        return result\n",
        "\n",
        "def Altconv3():\n",
        "    return AltConvNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOPMFAXIPqLJ",
        "outputId": "a5564b78-e8bc-4ee6-9492-abd8f4e9606d"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "model = Altconv3().cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3r8YrbhQFeq"
      },
      "source": [
        "### Try different max-pool size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE5iyos9QjCR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class AltConvNet(nn.Module):\n",
        "    \"\"\"Encoder for feature embedding\"\"\"\n",
        "    def __init__(self):\n",
        "        super(AltConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 64, kernel_size=(5,3), padding=(5, 3), stride=(3,1) ,dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d(2,2))\n",
        "        nn.init.xavier_uniform_(self.layer1[0].weight)\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,128,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,2)),)\n",
        "        nn.init.xavier_uniform_(self.layer2[0].weight)\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(128,256,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,2)),)\n",
        "        nn.init.xavier_uniform_(self.layer3[0].weight)\n",
        "        self.fc = nn.Linear(2816, 2)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        result = self.fc(out)\n",
        "        return result\n",
        "\n",
        "def Altconv3():\n",
        "    return AltConvNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWIg-z2wQzcK",
        "outputId": "1d91c279-6143-4dcf-b2b6-25280090334d"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "model = Altconv3().cuda()\n",
        "optimizer=torch.optim.AdamW(model.parameters(), lr, weight_decay=wd)\n",
        "evaluate_batch(model, val_loader, device)\n",
        "best_acc = 0\n",
        "count = 0\n",
        "\n",
        "for i in range(1, epoch+1):\n",
        "    print('Epoch : ', i)\n",
        "    pretrain(model, train_loader, optimizer, device)\n",
        "    val_acc = evaluate_batch(model, val_loader, device)\n",
        "    print('Val Acc : ', val_acc)\n",
        "    if  best_acc < val_acc:\n",
        "        count = 0\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    else:\n",
        "        count += 1\n",
        "    if count >= 2:\n",
        "        break\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "file_list = os.listdir(\"/content/monthly_20d\")\n",
        "img_list = [i for i in file_list if 'dat' in i]\n",
        "lab_list = [i for i in file_list if 'feather' in i]\n",
        "img_list.sort()\n",
        "lab_list.sort()\n",
        "img_train_val, img_test = img_list[:7],img_list[7:] \n",
        "lab_train_val, lab_test = lab_list[:7],lab_list[7:] \n",
        "\n",
        "model.eval()\n",
        "N1, N2 =0, 0\n",
        "for i, j in zip(image_test, label_test):\n",
        "    image_test_arr=np.memmap(\"/content/monthly_20d/\"+i, dtype=np.uint8, mode='r').reshape(\n",
        "                                (-1, 64, 60))\n",
        "    label_test_arr=pd.read_feather(\"/content/monthly_20d/\"+j)['Ret_20d'].values\n",
        "    testset = ImgDataset(image_test_arr, label_test_arr)\n",
        "    test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "    test_acc = evaluate_batch(model, test_loader, device)\n",
        "    N2 += len(testset)\n",
        "    N1 += (test_acc *len(testset))\n",
        "print('Test Acc : ', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grad-CAM and data visulization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from utils import *\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grad-CAM \n",
        "please first install the supporting pacakge:\n",
        "\n",
        "pip install grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install grad-cam\n",
        "from pytorch_grad_cam import GradCAM # or ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "epoch = 15\n",
        "lr = 1e-3\n",
        "wd =  5e-4\n",
        "\n",
        "train_x  = np.load('dataset/train_x.npy')\n",
        "train_y = np.load('dataset/train_y.npy')\n",
        "val_x , val_y = np.load('dataset/val_x.npy'), np.load('dataset/val_y.npy')\n",
        "trainset = dataset.NpyDataset(train_x , train_y )\n",
        "valset = dataset.NpyDataset(val_x , val_y)\n",
        "model_name = 'weight.pth'\n",
        "train_loader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(dataset=valset, batch_size=batch_size, shuffle=False, num_workers=4,\n",
        "                pin_memory=True, drop_last=True)\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "#load the trained model. Here we assume the weight has been saved\n",
        "model_name = \"weight.pth\"\n",
        "model = conv3()\n",
        "model.load_state_dict(torch.load(model_name,map_location=torch.device('cpu')))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find 5 images that are classified as 1 or 0\n",
        "\n",
        "\n",
        "\n",
        "classification_object = 0 # or let classification_object = 1 to get another bunch of images\n",
        "image_list = []\n",
        "label_list = []\n",
        "counter = 0\n",
        "# better shuffle the data loader\n",
        "for iter, pack in enumerate(val_loader):\n",
        "    data, target = pack[0].to(device), pack[1].to(device)\n",
        "    #targets = target.long()\n",
        "    logits = model(data)\n",
        "    _, pred = logits.max(1)\n",
        "    class_list = data[pred==classification_object,]\n",
        "    if class_list.size(0) > 0:\n",
        "        obj = class_list[random.randrange(class_list.size(0)),]\n",
        "        counter = counter+1\n",
        "    image_list.append(obj)\n",
        "    if counter >= 5:\n",
        "        break\n",
        "\n",
        "image_list = torch.stack(image_list)\n",
        "\n",
        "\n",
        "input_tensors = image_list\n",
        "    \n",
        "\n",
        "# Create an input tensor image for your model..\n",
        "# Note: input_tensor can be a batch tensor with several images!\n",
        "\n",
        "# Construct the CAM object once, and then re-use it on many images:\n",
        "aug_smooth = True\n",
        "eigen_smooth = True\n",
        "\n",
        "\n",
        "target_layer_1 = [model.layer1[-1]]\n",
        "cam_1 = GradCAM(model=model, target_layers=target_layer_1, use_cuda=False)\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "grayscale_cam_1 = cam_1(input_tensor=input_tensors, targets=None ,aug_smooth=aug_smooth,eigen_smooth=eigen_smooth)\n",
        "\n",
        "#visualization = show_cam_on_image(input_tensors, grayscale_cam, use_rgb=True)\n",
        "\n",
        "target_layer_2 = [model.layer2[-1]]\n",
        "cam_2 = GradCAM(model=model, target_layers=target_layer_2, use_cuda=False)\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "grayscale_cam_2 = cam_2(input_tensor=input_tensors, targets=None ,aug_smooth=aug_smooth,eigen_smooth=eigen_smooth)\n",
        "\n",
        "\n",
        "target_layer_3 = [model.layer3[-1]]\n",
        "cam_3 = GradCAM(model=model, target_layers=target_layer_3, use_cuda=False)\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "grayscale_cam_3 = cam_3(input_tensor=input_tensors, targets=None ,aug_smooth=aug_smooth,eigen_smooth=eigen_smooth)\n",
        "\n",
        "camera = torch.cat((image_list[:,0,], torch.from_numpy(grayscale_cam_1),torch.from_numpy(grayscale_cam_2),torch.from_numpy(grayscale_cam_3)),0)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show multiple images\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "columns = 5\n",
        "rows = 4\n",
        "for i in range(0, columns*rows ):\n",
        "    fig.add_subplot(rows, columns, i+1)\n",
        "    plt.axis('off')\n",
        "    if i<= 4:\n",
        "        plt.imshow(camera[i,],cmap=\"gray\")\n",
        "    else:\n",
        "        plt.imshow(camera[i,],cmap=\"Blues_r\")\n",
        "plt.savefig('4x5.png',format='png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define constants\n",
        "\n",
        "MODEL_NAME = 'model.pth'\n",
        "BATCH_SIZE = 128\n",
        "WEIGHT_DECAY = 5E-4\n",
        "EPOCH = 15\n",
        "TRAIN_DEVICE = 'cuda'\n",
        "IMAGE_NAME = 'both.png'\n",
        "SAVE_NAME = 'grad_cam.png'\n",
        "VISUALIZATION_DEVICE = 'cpu'\n",
        "\n",
        "RAW_DATA_PATH = 'monthly_20d'\n",
        "#COOK_DATA_PATH = 'cookedData'\n",
        "COOK_DATA_PATH = 'gdrive/MyDrive/Math5470Project'\n",
        "VISUALIZATION_PATH = 'pic'\n",
        "BINARY_CLASSIFICATION = False\n",
        "DEBUG = False\n",
        "LR_INITIAL = 1E-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the data\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "class TrendDataset(Dataset):\n",
        "    def __init__(self, data_file, label_file):\n",
        "        self.data = data_file\n",
        "        self.label = label_file\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data[index]\n",
        "        label = self.label[index]\n",
        "        \n",
        "        if BINARY_CLASSIFICATION:\n",
        "            label = np.where(label > 0, 1, 0)\n",
        "            label = torch.from_numpy(label).float()\n",
        "        else :\n",
        "            data = np.where(data == 255, 1, data)\n",
        "\n",
        "        data = torch.from_numpy(data.copy()).float()\n",
        "        data = data.unsqueeze(0)\n",
        "\n",
        "        return data, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## regression CNN model\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class ConvNet(nn.Module):\n",
        "    \"\"\"Encoder for feature embedding\"\"\"\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(1, 64, kernel_size=(5,3), padding=(5, 3), stride=(3,1) ,dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d(2,1))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(64,128,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(128),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        self.layer3 = nn.Sequential(\n",
        "                        nn.Conv2d(128,256,kernel_size=(5,3),padding=(5,3), stride=(3,1), dilation=(2,1)),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.LeakyReLU(0.1),\n",
        "                        nn.MaxPool2d((2,1)),)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        if BINARY_CLASSIFICATION :\n",
        "            self.fc = nn.Linear(18176, 2)\n",
        "        else :\n",
        "            self.fc = nn.Linear(18176, 1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        result = self.fc(out)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## new pretrain function for regression\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "\n",
        "#class Averager():\n",
        "#\n",
        "#    def __init__(self):\n",
        "#        self.n = 0\n",
        "#        self.v = 0\n",
        "#\n",
        "#    def add(self, x):\n",
        "#        self.v = (self.v * self.n + x) / (self.n + 1)\n",
        "#        self.n += 1\n",
        "#\n",
        "#    def item(self):\n",
        "#        return self.v\n",
        "\n",
        "def pretrain(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    #loss_avg = Averager()\n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        train_inputs, train_targets = batch[0], batch[1]\n",
        "        if BINARY_CLASSIFICATION:\n",
        "          train_targets = train_targets.long()\n",
        "        else:\n",
        "          train_targets = train_targets.float()\n",
        "        train_inputs = train_inputs.to(device=device)\n",
        "        train_targets = train_targets.to(device=device)\n",
        "        train_logits = model(train_inputs)\n",
        "\n",
        "        if BINARY_CLASSIFICATION:\n",
        "          loss = nn.CrossEntropyLoss()(train_logits, train_targets)\n",
        "        else :\n",
        "          train_targets = torch.nan_to_num(train_targets)\n",
        "          loss = nn.MSELoss()(train_logits.reshape(128,), train_targets)\n",
        "        total_loss += loss.item()\n",
        "        #loss_avg.add(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(\"Train Loss %.4f\" % avg_loss)\n",
        "    return avg_loss\n",
        "\n",
        "def evaluate_batch(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = num = count = 0\n",
        "    total_loss = total_r_square = 0\n",
        "    for iter, pack in enumerate(data_loader):\n",
        "        data, target = pack[0].to(device), pack[1].to(device)\n",
        "        logits = model(data)\n",
        "        num += data.shape[0]\n",
        "        count += 1\n",
        "        if BINARY_CLASSIFICATION:\n",
        "            _, pred = logits.max(1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "        else:\n",
        "            target = torch.nan_to_num(target)\n",
        "            loss = nn.MSELoss()(logits.reshape(128,), target)\n",
        "            total_loss += loss.item()\n",
        "            r_square = 1 - torch.sum(torch.square(torch.sub(target,logits))) / torch.sum(torch.square(target - target.mean()))\n",
        "            total_r_square += r_square.item()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    model.train()\n",
        "    \n",
        "    if BINARY_CLASSIFICATION:\n",
        "        return correct / num\n",
        "    else:\n",
        "        print('avg r square:', total_r_square / count)\n",
        "        return total_loss / count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "#from model import ConvNet\n",
        "\n",
        "\n",
        "def trainModel():\n",
        "    train_x, train_y = np.load(COOK_DATA_PATH+'/train_x.npy'), np.load(COOK_DATA_PATH+'/train_y.npy')\n",
        "    validate_x, validate_y = np.load(COOK_DATA_PATH+'/val_x.npy'), np.load(COOK_DATA_PATH+'/val_y.npy')\n",
        "    train_dataset = TrendDataset(train_x, train_y)\n",
        "    validate_dataset = TrendDataset(validate_x, validate_y)\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4,\n",
        "                              pin_memory=True, drop_last=True)\n",
        "    val_loader = DataLoader(dataset=validate_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4,\n",
        "                            pin_memory=True, drop_last=True)\n",
        "\n",
        "    model = ConvNet().cuda()\n",
        "    optimizer=torch.optim.AdamW(model.parameters(), LR_INITIAL, weight_decay=WEIGHT_DECAY)\n",
        "    evaluate_batch(model, val_loader, TRAIN_DEVICE)\n",
        "    best_acc = 0\n",
        "    count = 0\n",
        "\n",
        "    file_list = os.listdir(os.path.join(os.getcwd(), RAW_DATA_PATH))\n",
        "    img_list = [i for i in file_list if 'dat' in i]\n",
        "    lab_list = [i for i in file_list if 'feather' in i]\n",
        "    img_list.sort()\n",
        "    lab_list.sort()\n",
        "\n",
        "    IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
        "    IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}\n",
        "\n",
        "    printed = False\n",
        "    for i in range(1, EPOCH + 1):\n",
        "        print('Epoch : ', i)\n",
        "        pretrain(model, train_loader, optimizer, TRAIN_DEVICE)\n",
        "        if printed == False:\n",
        "            print(val_loader)\n",
        "            printed = True\n",
        "        validate_acc = evaluate_batch(model, val_loader, TRAIN_DEVICE)\n",
        "        print('Validation Acc : ', validate_acc)\n",
        "        print('best_acc: ', best_acc)\n",
        "        if BINARY_CLASSIFICATION:\n",
        "            if  best_acc < validate_acc:\n",
        "                count = 0\n",
        "                best_acc = validate_acc\n",
        "                torch.save(model.state_dict(), MODEL_NAME)\n",
        "            else:\n",
        "                count += 1\n",
        "        else:\n",
        "            if  best_acc > validate_acc or best_acc == 0:\n",
        "                count = 0\n",
        "                best_acc = validate_acc\n",
        "                torch.save(model.state_dict(), MODEL_NAME)\n",
        "            else:\n",
        "                count += 1\n",
        "                \n",
        "        if count >= 2:\n",
        "            break\n",
        "\n",
        "    printed = False\n",
        "    model.load_state_dict(torch.load(MODEL_NAME))\n",
        "    model.eval()\n",
        "    for i, j in zip(img_test, lab_test):\n",
        "        img_test_arr = np.memmap(os.path.join(os.getcwd(), RAW_DATA_PATH,i), dtype=np.uint8, mode='r').reshape(\n",
        "                                    (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
        "        lab_test_arr = pd.read_feather(os.path.join(os.getcwd(), RAW_DATA_PATH,j))['Ret_20d'].values\n",
        "        testset = TrendDataset(img_test_arr, lab_test_arr)\n",
        "        test_loader = DataLoader(dataset=testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4,\n",
        "                                 pin_memory=True, drop_last=True)\n",
        "        if printed == False:\n",
        "            print(test_loader)\n",
        "            printed = True\n",
        "        test_acc = evaluate_batch(model, test_loader, TRAIN_DEVICE)\n",
        "    print('Test Acc : ', test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainModel() # 3 layer, LE activation, no BN, dropout = 0.5, or try different constant"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MATH5470Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
