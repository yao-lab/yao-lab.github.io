{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80147ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ae6e8",
   "metadata": {},
   "source": [
    "# Construct CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85940d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_class=2,       \n",
    "                 initial_filter = 64,\n",
    "                 filter_sizes= (5,3),\n",
    "                 maxpool_sizes= (2,1),\n",
    "                 dilation=(2,1),\n",
    "                 stride=(2,1),\n",
    "                 p=0.5,\n",
    "                 BN=True,\n",
    "                 Activation='LRELU'\n",
    "                ):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        padding = (\n",
    "            (filter_sizes[0]-1)//2,\n",
    "            (filter_sizes[1]-1)//2\n",
    "       )\n",
    "        \n",
    "        #BOOLEAN:whether to use batch norm\n",
    "        self.BN = BN\n",
    "        #STRING: type of activation\n",
    "        self.Activation = Activation\n",
    "        # int : 2 for binary\n",
    "        self.num_class = num_class\n",
    "        # float: prob for dropout\n",
    "        self.p = p\n",
    "        \n",
    "        if self.Activation=='LRELU':\n",
    "            self.lrelu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        else:\n",
    "            self.lrelu = nn.ReLU(inplace=True)\n",
    "            \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                      out_channels=initial_filter, \n",
    "                      kernel_size=filter_sizes,\n",
    "                      padding=padding,\n",
    "                      stride=stride, \n",
    "                      dilation=dilation)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(initial_filter)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=maxpool_sizes, \n",
    "                                       stride=maxpool_sizes)\n",
    "            \n",
    "            \n",
    "        self.conv2= nn.Conv2d(\n",
    "                      in_channels=initial_filter, \n",
    "                      out_channels=initial_filter*2, \n",
    "                      kernel_size=filter_sizes,\n",
    "                      padding=padding\n",
    "                      )\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(initial_filter*2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=maxpool_sizes, \n",
    "                                  stride=maxpool_sizes)\n",
    "            \n",
    "        self.conv3 = nn.Conv2d(in_channels=initial_filter*2, \n",
    "                      out_channels=initial_filter*4, \n",
    "                      kernel_size=filter_sizes,\n",
    "                      padding=padding\n",
    "                      )\n",
    "                        \n",
    "        self.bn3=nn.BatchNorm2d(initial_filter*4)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=maxpool_sizes, \n",
    "                                  stride=maxpool_sizes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(46080, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=self.p)\n",
    "        \n",
    "        #XAVIER initialization\n",
    "        nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print('Block1')\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        if self.BN:\n",
    "            x = self.bn1(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.pool1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        #print('Block2')\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        if self.BN:\n",
    "            x = self.bn2(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.pool2(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #print('Block3')\n",
    "        x = self.conv3(x)\n",
    "        #print(x.shape)\n",
    "        if self.BN:\n",
    "            x = self.bn3(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.pool3(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print('Flatten-FC')\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        #drop out applied to Linear layer. \n",
    "        #print('after drop out')\n",
    "        #print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        #x=self.dropout(self.fc(x))        \n",
    "        \n",
    "        #softmax is not needed for our loss function\n",
    "        #print('after linear layer')\n",
    "        #print(x.shape)\n",
    "        #x = self.classifier(x)  #take a vector z [2,1] -> probability [2,1]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b5ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29209c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#changing the model_name will save the output to another path\n",
    "model_name ='regression_adjusted_return_normalized'\n",
    "target_label ='adj_return_normalized' \n",
    "#'regression_return_normalized','regression_sharpe_normalized','regression_adjusted_return_normalized'\n",
    "#'return_pred_normalized','sharpe_pred_normalized', 'adj_return_normalized'\n",
    "\n",
    "model = CNN()\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "if use_gpu:\n",
    "    print('GPU')\n",
    "    device = torch.device(\"cuda:2\" if use_gpu else \"cpu\")\n",
    "    model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042461ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show basic information of model\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(1, 64, 60)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32fd90",
   "metadata": {},
   "source": [
    "# Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c591b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 1e-5 \n",
    "BATCH_SIZE = {'train':128,\n",
    "              'test':128}\n",
    "momentum = 0.9\n",
    "num_epochs = 50\n",
    "#Patience of early stopping\n",
    "patience = 2\n",
    "     \n",
    "#pin_memory ensures movement of data from cpu to gpu is efficient and fast. \n",
    "#In case one uses inbuilt datasets like MNIST or CIFAR10 then this parameter is not required as in that case data is loaded directly into GPU. \n",
    "#num_workers attribute tells the data loader instance how many sub-processes to use for data loading\n",
    "pin_memory = True\n",
    "num_workers = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2ae75",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b451bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_Raw = pd.read_hdf('./Label_Raw_Addtional_Predictors.h5',key='data')\n",
    "length_Train=pd.read_hdf('./length_Train.h5',key='data')\n",
    "\n",
    "#Now all images are saved in 1993 file\n",
    "year = 1993\n",
    "Images = np.memmap(\n",
    "            op.join(\"./img_data/monthly_20d\", f\"20d_month_has_vb_[20]_ma_{year}_images.dat\"), \n",
    "            dtype=np.uint8, mode='r+').reshape(\n",
    "                                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "        \n",
    "Label_Raw = Label_Raw.reset_index().drop('index',axis=1) \n",
    "Label_Raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a4b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04de891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert into labels:\n",
    "# 1: positive return\n",
    "# 0: negative returns\n",
    "raw_labels = Label_Raw[target_label].mask(Label_Raw[target_label]==0).dropna()\n",
    "\n",
    "#adjust format for dataloader below\n",
    "annotations = raw_labels.reset_index().rename(columns= {'index':'img_name',\n",
    "                                           target_label:'label'})\n",
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f956bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed60dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Pytorch's DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class CNNDataset(Dataset):\n",
    "    def __init__(self, Images_map ,annotations, transform=None):\n",
    "        \n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "        self.Images = Images_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_id = self.annotations.iloc[index, 0]\n",
    "        \n",
    "        img = torch.from_numpy(self.Images[[img_id]]).type(torch.float)        \n",
    "        \n",
    "        y_label = torch.tensor((self.annotations.iloc[index, 1])).type(torch.float)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_size = int(len(annotations) * 0.7)\n",
    "val_size = len(annotations) - train_size\n",
    "#create dataset object\n",
    "dataset = CNNDataset(Images,annotations)\n",
    "\n",
    "#split train vs validation\n",
    "train_set, validation_set = torch.utils.data.random_split(dataset,[train_size,val_size])\n",
    "\n",
    "#load data\n",
    "trainloader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE['train'],shuffle=True,\n",
    "                          num_workers=num_workers,pin_memory=pin_memory)\n",
    "valloader = DataLoader(dataset=validation_set, batch_size=BATCH_SIZE['test'],shuffle=False,\n",
    "                               num_workers=num_workers, pin_memory=pin_memory) \n",
    "\n",
    "loaders = {'train': trainloader, 'test': valloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2191677",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.pyplot import show\n",
    "for img in images[:10]:\n",
    "    imshow(img[0])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e21a3a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a847cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import scipy.stats\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)  #Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b52723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train 5 models with different random seeds\n",
    "for seed in range(0,5):\n",
    "    # The log for recording train (test) loss and errors.\n",
    "    log = {\n",
    "        'num_params': [],\n",
    "        'train_loss': [],\n",
    "        'train_error': [],\n",
    "        'test_loss': [],\n",
    "        'test_error': []\n",
    "    }\n",
    "    log_saver = log \n",
    "    num_epochs = 50\n",
    "\n",
    "\n",
    "    print('seed '+str(seed))\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    name = model_name+'_'+str(seed)\n",
    "    \n",
    "    since = time.time()\n",
    "    steps = 0\n",
    "    last_loss = 100\n",
    "    triggertimes = 0\n",
    "\n",
    "\n",
    "    model = CNN()\n",
    "    if use_gpu:\n",
    "        print('GPU')\n",
    "        device = torch.device(\"cuda:2\" if use_gpu else \"cpu\")\n",
    "        model = model.cuda()\n",
    "\n",
    "    use_tpu=False\n",
    "    if use_tpu:\n",
    "      print('TPU')\n",
    "      # Places network on the default TPU core\n",
    "      model = model.to(device)\n",
    "\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  #Adam\n",
    "    number_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    log['num_params'].append(number_params)\n",
    "\n",
    "    print(f'total parameters: {number_params}')    \n",
    "\n",
    "    #iterating over epochs    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'test']:\n",
    "                        \n",
    "            loss_meter = AverageMeter()      \n",
    "            mean_meter = AverageMeter()\n",
    "            mean_sq_meter = AverageMeter()\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "            \n",
    "                        \n",
    "            for i, data in enumerate(loaders[re.findall('[a-zA-Z]+',phase)[0]]):\n",
    "                inputs, labels = data\n",
    "                #normalize greyscale to [0,1]                \n",
    "                normalize = True\n",
    "                if normalize:\n",
    "                    inputs /=255\n",
    "                \n",
    "                if use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs) \n",
    "                \n",
    "                if steps <10000 and steps % 100 ==0 :\n",
    "                    display(\n",
    "                        pd.Series( outputs.view(-1).detach().cpu().numpy()).describe()\n",
    "                        )\n",
    "\n",
    "                                    \n",
    "                loss = criterion(outputs.view(-1), \n",
    "                    labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    steps += 1\n",
    "\n",
    "                N = outputs.size(0)\n",
    "\n",
    "                loss_meter.update(loss.data.item(), N)\n",
    "                \n",
    "                #metrics to compute R-squared\n",
    "                mean_meter.update(labels.mean().item(),N)                \n",
    "                mean_sq_meter.update((labels**2).mean().item(),N)\n",
    "                \n",
    "                \n",
    "                        \n",
    "            epoch_loss = loss_meter.avg\n",
    "            mean_tss = mean_sq_meter.avg - mean_meter.avg\n",
    "            epoch_error = epoch_loss / mean_tss\n",
    "                        \n",
    "            if phase == 'train':\n",
    "                log_saver['train_loss'].append(epoch_loss)\n",
    "                log_saver['train_error'].append(epoch_error)\n",
    "\n",
    "            elif phase == 'test':\n",
    "\n",
    "                log_saver['test_loss'].append(epoch_loss)\n",
    "                log_saver['test_error'].append(epoch_error)\n",
    "\n",
    "            print(\n",
    "                    f'{phase} loss: {epoch_loss:.4f}; error: {epoch_error:.4f}'\n",
    "            )\n",
    "            print('sample count total:')\n",
    "            print(i)\n",
    "            print(loss_meter.count)\n",
    "            \n",
    "                    \n",
    "        if epoch % 100 == 0 or epoch == num_epochs - 1:\n",
    "            print('Saving..')\n",
    "            state = {'net': model, 'epoch': epoch, 'log': log_saver}\n",
    "\n",
    "            if not os.path.isdir('./checkpoint_CNN/'+model_name):\n",
    "                os.mkdir('./checkpoint_CNN/'+model_name)\n",
    "            torch.save(state,\n",
    "                       './checkpoint_CNN/'+model_name+'/'+name+'.t7')\n",
    "            \n",
    "        #Early Stopping            \n",
    "        print('Cycle Completed for: '+phase+', total samples '+str(loss_meter.count))\n",
    "        current_loss = epoch_loss\n",
    "        if current_loss >= last_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger Times:', trigger_times)\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                break\n",
    "                #return model, log_saver\n",
    "        else:\n",
    "            print('trigger times: 0')\n",
    "            trigger_times = 0\n",
    "        last_loss = current_loss\n",
    "                    \n",
    "            \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s'\n",
    "    )\n",
    "    \n",
    "    #save final model:\n",
    "    state = {'net': model, 'epoch': epoch, 'log': log_saver}\n",
    "    torch.save(state,\n",
    "               './checkpoint_CNN/'+model_name+'/'+name+'_final.t7')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e490ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results (for the last model in the loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ae78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot(log, result_dir):\n",
    "    fontdict = {'size': 30}\n",
    "\n",
    "    def get_fig(i, title):\n",
    "        fig = plt.figure(i, figsize=(20, 10))\n",
    "        ax = fig.add_subplot(111)\n",
    "        #plt.title(title, fontsize=30, y=1.04)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        return fig, ax\n",
    "\n",
    "    fig1, ax1 = get_fig(1, 'Loss on Cifar10')\n",
    "    fig2, ax2 = get_fig(2, 'Error on Cifar10')\n",
    "\n",
    "    ax1.plot(log['train_loss'],'b', linewidth=3, label='training')\n",
    "    ax1.plot(log['test_loss'],'r', linewidth=3, label='test')\n",
    "    ax1.set_ylabel('loss',fontdict=fontdict)\n",
    "    ax2.plot(log['train_error'],'b', linewidth=3, label='training')\n",
    "    ax2.plot(log['test_error'],'r', linewidth=3, label='test')\n",
    "    ax2.set_ylabel('error',fontdict=fontdict)\n",
    "\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_xlabel('Number of epochs', fontdict=fontdict)\n",
    "        ax.legend(loc='upper right', fontsize=20)\n",
    "        \n",
    "    if not os.path.exists(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    fig1.savefig(result_dir + 'loss.png')\n",
    "    fig2.savefig(result_dir + 'error.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b4bc12",
   "metadata": {},
   "source": [
    "plot(log, './alex_results/')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1234a79",
   "metadata": {},
   "source": [
    "# Evaluating the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11f405",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af90f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_Test=pd.read_hdf('./Label_Test_Addtional_Predictors.h5',key='data')\n",
    "\n",
    "length_Test=pd.read_hdf('./length_Test.h5',key='data')   \n",
    "\n",
    "year = 2000\n",
    "Images_Test = np.memmap(\n",
    "                    op.join(\"./img_data/monthly_20d\", f\"20d_month_has_vb_[20]_ma_{year}_images.dat\"), \n",
    "                    dtype=np.uint8, mode='r+',order='C',\n",
    "                    shape=(len(Label_Test), IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "Label_Test = Label_Test.reset_index().drop('index',axis=1) \n",
    "Label_Test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Images_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ddcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_labels = (\n",
    "    Label_Test[target_label].mask(Label_Test[target_label]==0).dropna()\n",
    "    )\n",
    "\n",
    "\n",
    "annotations_test = raw_labels.reset_index().rename(columns= {'index':'img_name',\n",
    "                                           target_label:'label'})\n",
    "annotations_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataset_test = CNNDataset(Images_Test,annotations_test)\n",
    "\n",
    "\n",
    "testloader = DataLoader(dataset=dataset_test, \n",
    "                        batch_size=BATCH_SIZE['test'],\n",
    "                        shuffle=False,\n",
    "                        num_workers=num_workers, pin_memory=pin_memory) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc483e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.pyplot import show\n",
    "for img in images[:10]:\n",
    "    imshow(img[0])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd02d29e",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "epoch_loss = pd.Series()\n",
    "epoch_accuracy = pd.Series()\n",
    "PREDICTIONS = {}\n",
    "# The log for recording train (test) loss and errors.\n",
    "\n",
    "\n",
    "for seed in range(0,5):\n",
    "\n",
    "    print('seed '+str(seed))\n",
    "    torch.manual_seed(seed)\n",
    "    name = model_name+'_'+str(seed)\n",
    "    \n",
    "    \n",
    "    state = torch.load(\n",
    "        './checkpoint_CNN/'+model_name+'/'+name+'_final.t7')\n",
    "                    \n",
    "\n",
    "    model = state['net']\n",
    "\n",
    "    number_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'total parameters: {number_params}')\n",
    "\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "\n",
    "    since = time.time()\n",
    "    steps = 0\n",
    "\n",
    "    loss_meter = AverageMeter()      \n",
    "    mean_meter = AverageMeter()\n",
    "    mean_sq_meter = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    Predictions = []\n",
    "\n",
    "    for i, data in enumerate(testloader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        normalize = True\n",
    "        if normalize:\n",
    "            inputs /=255\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = model(inputs) \n",
    "\n",
    "\n",
    "        if use_gpu:\n",
    "          prob = list(outputs.cpu().view(-1).detach().numpy())          \n",
    "        else:\n",
    "          prob = list(outputs.view(-1).detach().numpy())\n",
    "        Predictions += prob\n",
    "\n",
    "        loss = criterion(outputs.view(-1), \n",
    "            labels)\n",
    "\n",
    "        N = outputs.size(0)\n",
    "\n",
    "        loss_meter.update(loss.data.item(), N)\n",
    "        \n",
    "        #metric used to compute r-squared\n",
    "        mean_meter.update(labels.mean().item(),N)\n",
    "\n",
    "        mean_sq_meter.update((labels**2).mean().item(),N)\n",
    "        del inputs, labels, outputs\n",
    "\n",
    "    epoch_loss.loc[seed] = loss_meter.avg\n",
    "    mean_tss = mean_sq_meter.avg - mean_meter.avg\n",
    "    epoch_accuracy.loc[seed] = epoch_loss.loc[seed] / mean_tss \n",
    "    PREDICTIONS[seed] = pd.Series(Predictions)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7253904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#averaging all results\n",
    "Predictions = pd.concat(PREDICTIONS,axis=1)\n",
    "Predictions['mean'] = Predictions.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "assert len(Predictions) == len(annotations_test), \"Unequal dimensions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions.index=annotations_test.index\n",
    "\n",
    "annotations_test['prediction'] = Predictions['mean']\n",
    "Label_Test['prediction']=annotations_test.set_index('img_name')['prediction']\n",
    "Label_Test['prediction'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ceee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store results\n",
    "\n",
    "if not os.path.isdir('./output_summary/'):\n",
    "    os.mkdir('./output_summary/')\n",
    "\n",
    "try:\n",
    "    Summary=pd.read_hdf('./output_summary/Test Results.h5',key='df')\n",
    "except:\n",
    "    Summary = pd.DataFrame(columns=['test loss','test accuracy'])\n",
    "\n",
    "    \n",
    "test_results = pd.Series({\n",
    "            'test loss':epoch_loss.mean(),\n",
    "            'test accuracy':epoch_accuracy.mean()})\n",
    "Summary.loc[model_name] = test_results\n",
    "Summary.to_hdf('./output_summary/Test Results.h5',key='df')\n",
    "\n",
    "\n",
    "test_results.to_hdf('./output_summary/Test Results.h5',key=model_name)\n",
    "epoch_loss.to_hdf('./output_summary/epoch_loss.h5',key=model_name)\n",
    "epoch_accuracy.to_hdf('./output_summary/epoch_accuracy.h5',key=model_name)\n",
    "Predictions.to_hdf('./output_summary/Predictions by seed.h5',key=model_name)\n",
    "Label_Test.to_hdf('./output_summary/Predictions.h5',key=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ab30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Label_Test=pd.read_hdf('./output_summary/Predictions.h5',key=model_name)\n",
    "Label_Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799af70",
   "metadata": {},
   "source": [
    "# Portfolio Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_field = 'prediction'\n",
    "return_field = 'Ret_month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf21032",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Label_Test.set_index(['Date','StockID'])[pred_field].unstack()\n",
    "return_next_per = Label_Test.set_index(['Date','StockID'])[return_field].unstack()\n",
    "marketcap = Label_Test.set_index(['Date','StockID'])['MarketCap'].unstack()\n",
    "vol = Label_Test.set_index(['Date','StockID'])['EWMA_vol'].unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe9365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to monitor the performance of the model\n",
    "# stocks are split into 10 deciles based on the factor value generated by the model\n",
    "y_rank = y_pred.rank(axis=1)\n",
    "y_count = y_rank.max(axis=1)\n",
    "#split stocks on each date into deciles\n",
    "y_normalize = y_rank.div(y_count,axis=0).sub(0.1/y_count,axis=0) * 10\n",
    "decile_portfolio = np.floor(y_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df348553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the weighting scheme within each decile can be:\n",
    "# equally-weighted\n",
    "# value-weighted\n",
    "\n",
    "Portfolio_Returns = {}\n",
    "EW = {}\n",
    "VW = {}\n",
    "\n",
    "#portfolio weights\n",
    "EW_w = {} #equally weighted portfolio\n",
    "VW_w = {} # value (market capitalization) weighted portfolios\n",
    "\n",
    "Count = {}\n",
    "for i in range(0,10):\n",
    "        port = (decile_portfolio==i)\n",
    "        \n",
    "        #equal weight\n",
    "        EW[i+1]=return_next_per.where(port).mean(axis=1)\n",
    "        \n",
    "        if i in [0,9]:\n",
    "            EW_w[i+1]=((port * 1).div(port.sum(axis=1),axis=0)).fillna(0)\n",
    "        \n",
    "        #total number of stocks \n",
    "        Count[i] = port.sum(axis=1)\n",
    "        \n",
    "        #market cap weight\n",
    "        marketcap_port = marketcap.where(port)\n",
    "        marketcap_port = marketcap_port.div(marketcap_port.sum(axis=1),axis=0)\n",
    "        \n",
    "        VW[i+1] = (marketcap_port * return_next_per).sum(axis=1)\n",
    "        if i in [0,9]:\n",
    "            VW_w[i+1]=marketcap_port.fillna(0)\n",
    "            \n",
    "#portfolio weights\n",
    "EW_w = EW_w[10] - EW_w[1]\n",
    "VW_w = VW_w[10] - VW_w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vol Scaled Portfolio\n",
    "# this is an experiment: no need to report it \n",
    "# since different stocks have different volatilities,\n",
    "# we would like to hold less of a stock if it is more volatile\n",
    "\n",
    "\n",
    "#demean s.t scores [-0.5,+0.5]\n",
    "y_vol_scale = (y_normalize/10 - 0.5)\n",
    "#adjust the volatility\n",
    "y_vol_scale = y_vol_scale/vol\n",
    "#adjust to have same leverage\n",
    "mult = (EW_w.abs() ).sum(axis=1) / (y_vol_scale.abs() ).sum(axis=1)\n",
    "W_vol_scale  = y_vol_scale.mul(mult,axis=0)\n",
    "Port_vol_scale = (W_vol_scale * return_next_per).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Portfolio_Returns['EW'] = EW\n",
    "Portfolio_Returns['VW'] = VW\n",
    "Portfolio_Returns['Vol Controlled'] = Port_vol_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    Portfolio_Returns['EW'],axis=1).to_hdf('./output_summary/Portfolio_Returns_EW.h5',key=model_name)\n",
    "pd.concat(\n",
    "    Portfolio_Returns['VW'],axis=1).to_hdf('./output_summary/Portfolio_Returns_VW.h5',key=model_name)\n",
    "Portfolio_Returns['Vol Controlled'].to_hdf('./output_summary/Portfolio_Returns_Vol Controlled.h5',key=model_name)\n",
    "\n",
    "EW_w.to_hdf('./output_summary/Equal_Weight_Portfolio.h5',key=model_name)\n",
    "\n",
    "VW_w.to_hdf('./output_summary/Value_Weight_Portfolio.h5',key=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d28a83",
   "metadata": {},
   "source": [
    "# Compute Performance of This Single Strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "def alpha_t(HL):\n",
    "    t_stat = HL.mean()/(\n",
    "        HL.std()/np.sqrt(len(HL)))\n",
    "    return 1 - t.cdf(t_stat, len(HL)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p-values of the sharpe ratio\n",
    "rejection = pd.Series()\n",
    "\n",
    "#equal weighted portfolio\n",
    "ret = pd.concat(Portfolio_Returns['EW'],axis=1)\n",
    "HL = ret[10] - ret[1]\n",
    "Dec = pd.concat({\n",
    "    'Ret':ret.mean()*12,\n",
    "    'SR':ret.mean()/ret.std()*(12**0.5)\n",
    "},axis=1)\n",
    "\n",
    "Dec.loc['H-L'] = pd.Series({\n",
    "    'Ret':(HL).mean()*12,\n",
    "    'SR': (HL).mean()/ (HL).std()*(12**0.5)\n",
    "})\n",
    "\n",
    "rejection.loc['Equal-Weight'] = alpha_t(HL)\n",
    "#Turnover\n",
    "portfolio_beginning = EW_w.shift() * (1+return_next_per.shift())\n",
    "scaling_factor = (EW_w.shift() * return_next_per.shift()).sum(axis=1)+1\n",
    "portfolio_beginning = portfolio_beginning.div(scaling_factor,axis=0)\n",
    "TO = EW_w -  portfolio_beginning\n",
    "TO = TO.abs().sum(axis=1).mean()/2\n",
    "\n",
    "\n",
    "#value weighted portfolio\n",
    "ret = pd.concat(Portfolio_Returns['VW'],axis=1)\n",
    "HL = ret[10] - ret[1]\n",
    "Dec2 = pd.concat({\n",
    "    'Ret':ret.mean()*12,\n",
    "    'SR':ret.mean()/ret.std()*(12**0.5)\n",
    "},axis=1)\n",
    "\n",
    "Dec2.loc['H-L'] = pd.Series({\n",
    "    'Ret':(HL).mean()*12,\n",
    "    'SR': (HL).mean()/ (HL).std()*(12**0.5)\n",
    "})\n",
    "rejection.loc['Value-Weight'] = alpha_t(HL)\n",
    "\n",
    "#Turnover\n",
    "portfolio_beginning = VW_w.shift() * (1+return_next_per.shift())\n",
    "scaling_factor = (VW_w.shift() * return_next_per.shift()).sum(axis=1)+1\n",
    "portfolio_beginning = portfolio_beginning.div(scaling_factor,axis=0)\n",
    "TO2 = VW_w -  portfolio_beginning\n",
    "TO2 = TO2.abs().sum(axis=1).mean()/2\n",
    "\n",
    "pd.concat({'Equal-Weight':Dec,\n",
    "          'Value-weight':Dec2},\n",
    "          axis=1).to_hdf('./output_summary/Portfolio Stats.h5',key=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9531483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#significance\n",
    "rejection.to_hdf('./output_summary/rejection.h5',key=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turnover \n",
    "portfolio_beginning = W_vol_scale.shift() * (1+return_next_per.shift())\n",
    "scaling_factor = (W_vol_scale.shift() * return_next_per.shift()).sum(axis=1)+1\n",
    "portfolio_beginning = portfolio_beginning.div(scaling_factor,axis=0)\n",
    "\n",
    "TO3 = W_vol_scale -  portfolio_beginning\n",
    "TO3 = TO3.abs().sum(axis=1).mean()/2\n",
    "\n",
    "pd.Series({'Equal-Weight':TO,\n",
    "          'Value-weight':TO2,\n",
    "          'Vol-Scale':TO3,\n",
    "          }).to_hdf('./output_summary/Turnover.h5',key=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68027a",
   "metadata": {},
   "source": [
    "# Factor Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f193762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "mom = pd.read_csv('./F-F_Momentum_Factor.csv',index_col=[0])\n",
    "mom.index= [\n",
    "    datetime.datetime(int(np.floor(ind/100)),int(ind %100),28) for ind in mom.index]\n",
    "\n",
    "reversal = pd.read_csv('./F-F_ST_Reversal_Factor.csv',index_col=[0])\n",
    "reversal.index= [\n",
    "    datetime.datetime(int(np.floor(ind/100)),int(ind %100),28) for ind in reversal.index]\n",
    "\n",
    "\n",
    "\n",
    "ff5 = pd.read_csv('./F-F_Research_Data_5_Factors_2x3.csv',index_col=[0])\n",
    "ff5.index= [\n",
    "    datetime.datetime(int(np.floor(ind/100)),int(ind %100),28) for ind in ff5.index]\n",
    "\n",
    "\n",
    "ff5['Momentum'] = mom\n",
    "ff5['Reversal'] = reversal\n",
    "\n",
    "ff5 /=100\n",
    "ff5= ff5.resample('BM').last().drop('RF',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd578e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grad Cam shows the image most activated when price patterns are volatile?\n",
    "Factor = {}\n",
    "\n",
    "#try lowVol 1/vol\n",
    "y_rank = (1/vol).rank(axis=1)\n",
    "y_count = y_rank.max(axis=1)\n",
    "y_normalize = y_rank.div(y_count,axis=0).sub(0.1/y_count,axis=0) * 10\n",
    "decile_portfolio = np.floor(y_normalize)\n",
    "\n",
    "EW_factor = {}\n",
    "VW_factor = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "        port = (decile_portfolio==i)\n",
    "        \n",
    "        #equal weight\n",
    "        EW_factor[i+1]=return_next_per.where(port).mean(axis=1)\n",
    "        \n",
    "        #market cap weight\n",
    "        marketcap_port = marketcap.where(port)\n",
    "        marketcap_port = marketcap_port.div(marketcap_port.sum(axis=1),axis=0)\n",
    "        \n",
    "        VW_factor[i+1] = (marketcap_port * return_next_per).sum(axis=1)\n",
    "            \n",
    "#portfolio weights\n",
    "Factor['LowVol'] = pd.concat({\n",
    "    'EW':EW_factor[10] - EW_factor[1],\n",
    "    'VW':VW_factor[10] - VW_factor[1],\n",
    "},axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute factor loadings of CNN model under both value weighted (VW) and equal weighted (EW) setting\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "Summary = {}\n",
    "Factor['LowVol'].index = pd.to_datetime(Factor['LowVol'].index)\n",
    "\n",
    "for w in ['VW','EW']:\n",
    "    \n",
    "    ret = pd.concat(Portfolio_Returns[w],axis=1)\n",
    "    HL = ret[10] - ret[1]\n",
    "\n",
    "    HL.index = pd.to_datetime(HL.index)\n",
    "\n",
    "    ff5['Y'] = HL\n",
    "    ff5.dropna(inplace=True)    \n",
    "    summary_return = {}\n",
    "    \n",
    "    #cnn factor return ~ Famma French 5 factor \n",
    "    \n",
    "    reg = OLS(endog = ff5['Y'],\n",
    "              exog = sm.add_constant(ff5[['CMA','HML','Mkt-RF','RMW','SMB']])\n",
    "             ).fit()\n",
    "\n",
    "    summary = reg.params.append(\n",
    "        reg.tvalues.rename(index={ind: ind+' t-stat' for ind in reg.tvalues.index})\n",
    "    ).sort_index()\n",
    "    \n",
    "    summary_return['Famma-French 5 Factor']=summary\n",
    "    \n",
    "    \n",
    "    #cnn factor return ~ Momentum + Reversal \n",
    "    reg = OLS(endog = ff5['Y'],\n",
    "              exog = sm.add_constant(ff5[['Momentum','Reversal']])\n",
    "             ).fit()\n",
    "\n",
    "    summary = reg.params.append(\n",
    "        reg.tvalues.rename(index={ind: ind+' t-stat' for ind in reg.tvalues.index})\n",
    "    ).sort_index()\n",
    "    \n",
    "    summary_return['Momentum/Reversal']=summary\n",
    "    \n",
    "    \n",
    "    dataset_factor = ff5.join(\n",
    "                        pd.concat({\n",
    "                                'LowVol':Factor['LowVol'][w],\n",
    "                                },axis=1)        \n",
    "    ).dropna()\n",
    "            \n",
    "    \n",
    "    #cnn factor return ~ LowVol \n",
    "    reg = OLS(endog = dataset_factor['Y'],\n",
    "              exog = sm.add_constant(dataset_factor['LowVol'])\n",
    "             ).fit()\n",
    "\n",
    "    summary = reg.params.append(\n",
    "        reg.tvalues.rename(index={ind: ind+' t-stat' for ind in reg.tvalues.index})\n",
    "    ).sort_index()\n",
    "    \n",
    "    summary_return['LowVol']=summary\n",
    "        \n",
    "    \n",
    "    #cnn factor return ~ All Factors  \n",
    "    reg = OLS(endog = dataset_factor['Y'],\n",
    "              exog = sm.add_constant(dataset_factor.drop('Y',axis=1))\n",
    "             ).fit()\n",
    "\n",
    "    summary = reg.params.append(\n",
    "        reg.tvalues.rename(index={ind: ind+' t-stat' for ind in reg.tvalues.index})\n",
    "    ).sort_index()\n",
    "    \n",
    "    summary_return['All']=summary\n",
    "    \n",
    "    \n",
    "    Summary[w] = pd.concat(summary_return,axis=1)\n",
    "\n",
    "\n",
    "Summary = pd.concat(Summary,axis=1)\n",
    "\n",
    "Summary.loc[['const','const t-stat']].append(\n",
    "    Summary.drop(['const','const t-stat'])\n",
    ").to_hdf('./output_summary/Factor_Loadings.h5',key=model_name)\n",
    "\n",
    "Summary.loc[['const','const t-stat']].append(\n",
    "    Summary.drop(['const','const t-stat'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5d1a6",
   "metadata": {},
   "source": [
    "# Compare Performance of different strategies \n",
    "# [this part is used for ROBUSTNESS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b920a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Saved Results on Errors & Accuracy\n",
    "loss_validation = []\n",
    "accuracy_validation = []\n",
    "for seed in range(0,5):\n",
    "    print('seed '+str(seed))\n",
    "    name = model_name+'_'+str(seed)    \n",
    "    state = torch.load(\n",
    "        './checkpoint_CNN/'+model_name+'/'+name+'_final.t7')\n",
    "\n",
    "    loss_validation.append(state['log']['test_loss'][-1])\n",
    "    accuracy_validation.append(state['log']['test_error'][-1])\n",
    "loss_validation = np.mean(loss_validation)\n",
    "accuracy_validation = np.mean(accuracy_validation)\n",
    "\n",
    "test_results = pd.read_hdf('./output_summary/Test Results.h5',\n",
    "                           key=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "stats['Loss'] = pd.Series({'V':loss_validation,\n",
    "                           'T':test_results.loc['test loss']})\n",
    "stats['R_squared'] = pd.Series({'V':1- accuracy_validation,\n",
    "                               'T':1- test_results.loc['test accuracy']})\n",
    "\n",
    "stats['Correlation'] = pd.Series({\n",
    "    'pearson':pd.concat([\n",
    "        Label_Test[pred_field], \n",
    "        Label_Test[return_field]],axis=1).corr(method='pearson').iloc[0,1],\n",
    "    'spearman':pd.concat([\n",
    "        Label_Test[pred_field], \n",
    "        Label_Test[return_field]],axis=1).corr(method='spearman').iloc[0,1]})\n",
    "\n",
    "\n",
    "\n",
    "portfolio_return = {}\n",
    "ret = pd.concat(Portfolio_Returns['EW'],axis=1)\n",
    "portfolio_return['EW'] = ret[10] - ret[1]\n",
    "\n",
    "ret = pd.concat(Portfolio_Returns['VW'],axis=1)\n",
    "portfolio_return['VW'] = ret[10] - ret[1]\n",
    "\n",
    "portfolio_return['Vol Controlled']  = Portfolio_Returns['Vol Controlled']\n",
    "\n",
    "\n",
    "stats['SR'] = pd.Series({col: \n",
    "                         portfolio_return[col].mean() / portfolio_return[col].std() *(12**0.5) \n",
    "                         for col in portfolio_return.keys()})\n",
    "\n",
    "stats['Vol'] = pd.Series({col:portfolio_return[col].std() *(12**0.5) \n",
    "                         for col in portfolio_return.keys()})\n",
    "\n",
    "\n",
    "stats['Max Draw-Down / Vol'] = pd.Series({col:\n",
    "                        (portfolio_return[col].cumsum().cummax() - portfolio_return[col].cumsum()).max()\n",
    "                        for col in portfolio_return.keys()}) / stats['Vol']\n",
    "pd.concat(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.concat(stats).to_hdf('./output_summary/Robustness.h5',key=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
