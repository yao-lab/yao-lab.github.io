{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform imports and set-up\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.interpolate import interp1d\n",
    "from patsy.contrasts import Treatment\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "# import torchvision\n",
    "# from torchvision.datasets import mnist\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot') # emulate pretty r-style plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path, nrows=None):\n",
    "    # Sample 100 rows of data to determine dtypes.\n",
    "    df_test = pd.read_csv(file_path, nrows=100)\n",
    "    float_cols = [c for c in df_test if df_test[c].dtype == \"float64\"]\n",
    "    float16_cols = {c: np.float16 for c in float_cols}\n",
    "    df = pd.read_csv(file_path, engine='c', dtype=float16_cols, nrows=nrows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_file('data_all_clean_rank_2.csv').iloc[:,1:]\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "# data = read_file('data_manipulated.csv').iloc[:,1:]\n",
    "# macropredictors = pd.read_csv(\"macropredictors.csv\")\n",
    "# data = read_file('./datashare/GKX_20201231.csv')\n",
    "# macropredictors_raw = pd.read_excel('PredictorData2021.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([\"permno\", \"DATE\", \"RET\", \"SHROUT\", \"sic2\", \"mve0\", \"prc\", \"d_p\", \"e_p\", \"b_m\", \"ntis\", \"tbl\", \"tms\", \"dfy\", \"svar\"])\n",
    "character_names = np.setdiff1d(data.columns, temp)\n",
    "# dp ep bm ntis tbl tms dfy svar\n",
    "# macro-economic predictors\n",
    "macropredictors_names = np.array([\"d_p\", \"e_p\", \"b_m\", \"ntis\", \"tbl\", \"tms\", \"dfy\", \"svar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic2, sic2_counts = np.unique(data[\"sic2\"], return_counts=True)\n",
    "def map_sic2(x):\n",
    "    return np.where(sic2 == x)[0][0]\n",
    "data[\"sic2\"] = list(map(map_sic2, data[\"sic2\"]))\n",
    "sic2_onehot = pd.get_dummies(data[\"sic2\"], prefix='sic2')\n",
    "sic2_names = sic2_onehot.columns\n",
    "# sic2_onehot = pd.get_dummies(data[\"sic2\"])\n",
    "data = pd.concat([data, sic2_onehot], axis=1)\n",
    "# sic2_names = sic2_onehot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro_character_names = []\n",
    "# for item1 in macropredictors_names:\n",
    "#     for item2 in character_names:\n",
    "#         name_temp = item1 + '_' + item2\n",
    "#         macro_character_names.append(name_temp)\n",
    "#         temp_df = pd.DataFrame(data = {name_temp: data[item1].values * data[item2].values})\n",
    "#         data = pd.concat([data, temp_df], axis=1)\n",
    "# macro_character_names = np.array(macro_character_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_names = np.append(character_names, macro_character_names)\n",
    "all_names = np.append(character_names, macropredictors_names)\n",
    "all_names = np.append(all_names, sic2_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94,), (8,), (75,), (177,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_names.shape, macropredictors_names.shape, sic2_names.shape, all_names.shape # macro_character_names.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_r2(y_predict, y_test):\n",
    "    y_predict = y_predict.reshape(-1)\n",
    "    # y_test = y_test.reshape(-1)\n",
    "    se = np.sum((y_predict - y_test)**2)\n",
    "    denominator = np.sum(y_test**2)\n",
    "    roos = 1 - (se/denominator)\n",
    "    return roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variable_importance(model, x_train, y_train, character_names, macropredictors_names, file_name):\n",
    "    y_predict = model.predict(x_train)\n",
    "    roos_all = compute_r2(y_predict, y_train)\n",
    "    vi_array = np.array([])\n",
    "    names_all = np.append(character_names, macropredictors_names)\n",
    "    for item in names_all:\n",
    "        x_temp = x_train.copy(deep=True)\n",
    "        x_temp[item] = 0\n",
    "        y_predict = model.predict(x_temp)\n",
    "        roos_temp = compute_r2(y_predict, y_train)\n",
    "        vi_array = np.append(vi_array, roos_all-roos_temp)\n",
    "    print(vi_array)\n",
    "    with open(file_name,'a') as fd:\n",
    "        writer = csv.writer(fd)\n",
    "        writer.writerow(vi_array)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -0.2214302863525699\n",
      "Accuracy: -0.3251253715273965\n",
      "Accuracy: -0.2327493977497801\n",
      "Accuracy: -0.1947883676798181\n",
      "Accuracy: -0.030936779530139624\n",
      "Accuracy: -0.154257110377086\n",
      "Accuracy: -0.08045195998205545\n",
      "Accuracy: -0.34344683103397733\n",
      "Accuracy: -0.003773069858996392\n",
      "Accuracy: 0.0016306467928800528\n",
      "Accuracy: -0.016530001335859357\n",
      "Accuracy: -0.008046433051978719\n",
      "Accuracy: -0.033042100139295316\n",
      "Accuracy: -0.04214776605533266\n",
      "Accuracy: 0.01495769267539826\n",
      "Accuracy: -0.0411485995051728\n",
      "Accuracy: 0.05100336687453544\n",
      "Accuracy: -0.038360091340191804\n",
      "Accuracy: -0.01023547622923382\n",
      "Accuracy: -0.025469480238515807\n",
      "Accuracy: -0.04075293533023938\n",
      "Accuracy: -0.20479412503073435\n",
      "Accuracy: -0.0006296894660746677\n",
      "Accuracy: -0.05438968396447441\n",
      "Accuracy: -0.3106250029904538\n",
      "Accuracy: -0.2003748700364849\n",
      "Accuracy: -0.15634831579307917\n",
      "Accuracy: -0.17198906793751867\n",
      "Accuracy: -0.07971515046782707\n",
      "Accuracy: -0.01699468614446409\n"
     ]
    }
   ],
   "source": [
    "train_start = datetime.strptime(\"1957-01-01\", \"%Y-%m-%d\")\n",
    "train_end = datetime.strptime(\"1974-12-31\", \"%Y-%m-%d\")\n",
    "val_start = datetime.strptime(\"1975-01-01\", \"%Y-%m-%d\")\n",
    "val_end = datetime.strptime(\"1986-12-31\", \"%Y-%m-%d\")\n",
    "test_start = datetime.strptime(\"1987-01-01\", \"%Y-%m-%d\")\n",
    "test_end = datetime.strptime(\"1987-12-31\", \"%Y-%m-%d\")\n",
    "filename = \"OLS_result.csv\"\n",
    "filename_vi = 'OLS_result_vi.csv'\n",
    "for i in range(30):\n",
    "    train_index = (data['DATE'] >= train_start) & (data['DATE'] <= train_end)\n",
    "    val_index = (data['DATE'] >= val_start) & (data['DATE'] <= val_end)\n",
    "    test_index = (data['DATE'] >= test_start) & (data['DATE'] <= test_end)\n",
    "    # print(data[train_index].head())\n",
    "    # print(data[train_index].tail())\n",
    "    \n",
    "    # train model\n",
    "    x_train = data[train_index][all_names]\n",
    "    y_train = data[train_index]['RET']\n",
    "    x_val = data[val_index][all_names]\n",
    "    y_val = data[val_index]['RET']\n",
    "\n",
    "    ols = make_pipeline(StandardScaler(), SGDRegressor(loss='huber', alpha=0))\n",
    "    ols.fit(x_train, y_train)\n",
    "\n",
    "    # test model\n",
    "    x_test = data[test_index][all_names]\n",
    "    y_test = data[test_index]['RET']\n",
    "    y_predict = ols.predict(x_test)\n",
    "    y_predict = y_predict.reshape(-1)\n",
    "    df = pd.DataFrame({'predict':y_predict, 'real':y_test})\n",
    "    df.to_csv(filename, mode='a', index=False, header=False)\n",
    "    \n",
    "    rms = np.sum((y_predict - y_test)**2)\n",
    "    denominator = np.sum(y_test**2)\n",
    "    roos = 1 - rms/denominator\n",
    "    print('Accuracy:', roos)\n",
    "\n",
    "    # variable importance computing \n",
    "    # compute_variable_importance(ols, x_train, y_train, character_names, macropredictors_names, filename_vi)\n",
    "\n",
    "    train_end = train_end + relativedelta(years=1)\n",
    "    val_start = val_start + relativedelta(years=1)\n",
    "    val_end = val_end + relativedelta(years=1)\n",
    "    test_start = test_start + relativedelta(years=1)\n",
    "    test_end = test_end + relativedelta(years=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f10aecbdcd0a7ca392daed1b0a2bb4e7bd6eec83b203e50374b332076e351b34"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
