{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform imports and set-up\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.interpolate import interp1d\n",
    "from patsy.contrasts import Treatment\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "# import torchvision\n",
    "# from torchvision.datasets import mnist\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot') # emulate pretty r-style plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path, nrows=None):\n",
    "    # Sample 100 rows of data to determine dtypes.\n",
    "    df_test = pd.read_csv(file_path, nrows=100)\n",
    "    float_cols = [c for c in df_test if df_test[c].dtype == \"float64\"]\n",
    "    float32_cols = {c: np.float32 for c in float_cols}\n",
    "    df = pd.read_csv(file_path, engine='c', dtype=float32_cols)#, nrows=nrows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_file('data_all_clean_rank_2.csv', 1817034).iloc[:,1:]\n",
    "data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "# data = read_file('data_manipulated.csv').iloc[:,1:]\n",
    "# macropredictors = pd.read_csv(\"macropredictors.csv\")\n",
    "# data = read_file('./datashare/GKX_20201231.csv')\n",
    "# macropredictors_raw = pd.read_excel('PredictorData2021.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([\"permno\", \"DATE\", \"RET\", \"SHROUT\", \"sic2\", \"mve0\", \"prc\", \"d_p\", \"e_p\", \"b_m\", \"ntis\", \"tbl\", \"tms\", \"dfy\", \"svar\"])\n",
    "character_names = np.setdiff1d(data.columns, temp)\n",
    "# dp ep bm ntis tbl tms dfy svar\n",
    "# macro-economic predictors\n",
    "macropredictors_names = np.array([\"d_p\", \"e_p\", \"b_m\", \"ntis\", \"tbl\", \"tms\", \"dfy\", \"svar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sic2, sic2_counts = np.unique(data[\"sic2\"], return_counts=True)\n",
    "def map_sic2(x):\n",
    "    return np.where(sic2 == x)[0][0]\n",
    "data[\"sic2\"] = list(map(map_sic2, data[\"sic2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94,), (8,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_names.shape, macropredictors_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  \n",
    "  def __init__(self,sic2_x, character_x, macro_x, data_y, device=None):\n",
    "    self.device = device\n",
    "    self.device = 'cpu'\n",
    "    self.x_sic2 = torch.tensor(sic2_x.values,dtype=torch.int16,device=self.device)\n",
    "    self.x_character = torch.tensor(character_x.values,dtype=torch.float32,device=self.device)\n",
    "    self.x_macro = torch.tensor(macro_x.values,dtype=torch.float32,device=self.device)\n",
    "    self.y_train = torch.tensor(data_y.values,dtype=torch.float32,device=self.device)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    sic2_point = torch.eye(75, device=self.device)[self.x_sic2[idx]]\n",
    "    character_point = self.x_character[idx,]\n",
    "    multi_point = torch.kron(character_point, self.x_macro[idx,])\n",
    "    train_data = torch.cat((sic2_point, character_point, multi_point), 0)\n",
    "    return train_data,self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedBatchNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.batchnorm = nn.BatchNorm1d(dim)\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return self.batchnorm(x)\n",
    "        else:\n",
    "            x = x.transpose(-1,-2)\n",
    "            y = self.batchnorm(x)\n",
    "            y = y.transpose(-1,-2)\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            WrappedBatchNorm(921),\n",
    "            nn.Linear(921, 32),\n",
    "            nn.ReLU(),\n",
    "            WrappedBatchNorm(32),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            WrappedBatchNorm(16),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            # WrappedBatchNorm(8),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            # WrappedBatchNorm(4),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.ReLU(),\n",
    "            # WrappedBatchNorm(2),\n",
    "            nn.Linear(2, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "net = LeNet5()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet4(Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet4, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            WrappedBatchNorm(921),\n",
    "            nn.Linear(921, 32),\n",
    "            nn.ReLU(),\n",
    "            WrappedBatchNorm(32),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            WrappedBatchNorm(16),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            # WrappedBatchNorm(8),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            # WrappedBatchNorm(4),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "net = LeNet4()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet3(Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet3, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            WrappedBatchNorm(921),\n",
    "            nn.Linear(921, 32),\n",
    "            nn.ReLU(),\n",
    "            WrappedBatchNorm(32),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            WrappedBatchNorm(16),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            # WrappedBatchNorm(8),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "net = LeNet3()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet2(Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet2, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            WrappedBatchNorm(921),\n",
    "            nn.Linear(921, 32),\n",
    "            nn.ReLU(),\n",
    "            WrappedBatchNorm(32),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            WrappedBatchNorm(16),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "net = LeNet2()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet1(Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet1, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            WrappedBatchNorm(921),\n",
    "            nn.Linear(921, 32),\n",
    "            nn.ReLU(),\n",
    "            WrappedBatchNorm(32),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "net = LeNet1()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def traindata(device, model, epochs, optimizer, loss_function, l1_lambda, train_loader, valid_loader):\n",
    "    # Early stopping\n",
    "    last_loss = 100\n",
    "    patience = 5\n",
    "    trigger_times = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "            #######################################################\n",
    "            train_x, train_label = train_x.to(device), train_label.to(device)\n",
    "            #######################################################\n",
    "            optimizer.zero_grad()\n",
    "            predict_y = model(train_x).squeeze()\n",
    "            _loss = loss_function(predict_y, train_label)\n",
    "\n",
    "            # l1 penalty term\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        \n",
    "            _loss = _loss + l1_lambda * l1_norm\n",
    "\n",
    "            _loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Show progress\n",
    "            if idx % 10 == 0 or idx == len(train_loader):\n",
    "                print('[{}/{}, {}/{}] loss: {:.8}'.format(epoch, epochs, idx, len(train_loader), _loss.item()))\n",
    "\n",
    "        # # update swa model\n",
    "        # swa_model.update_parameters(model)\n",
    "        # swa_scheduler.step()\n",
    "        \n",
    "        # Early stopping        \n",
    "        if epoch >= 5:\n",
    "            current_loss = validation(model, device, valid_loader, loss_function)\n",
    "            print('The Current Loss:', current_loss)\n",
    "\n",
    "            if current_loss > last_loss - 1e-3:\n",
    "                trigger_times += 1\n",
    "                print('Trigger Times:', trigger_times)\n",
    "\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping!\\nStart to test process.')\n",
    "                    return model\n",
    "\n",
    "            else:\n",
    "                print('trigger times: 0')\n",
    "                trigger_times = 0\n",
    "\n",
    "            last_loss = current_loss\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def validation(model, device, valid_loader, loss_function):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    # Test validation data\n",
    "    with torch.no_grad():\n",
    "        for idx, (train_x, train_label) in enumerate(valid_loader):\n",
    "            #######################################################\n",
    "            train_x, train_label = train_x.to(device).unsqueeze(1), train_label.to(device)\n",
    "            #######################################################\n",
    "            predict_y = model(train_x).squeeze()\n",
    "            # print(predict_y.shape)\n",
    "            # print(train_label.shape)\n",
    "            _loss = loss_function(predict_y, train_label)\n",
    "            loss_total += _loss.item()\n",
    "\n",
    "    return loss_total / len(valid_loader)\n",
    "\n",
    "\n",
    "\n",
    "def test(device, model, test_loader, filename=None, vi=False, ensemble = 10):\n",
    "    model.eval()\n",
    "    y_predict_all = np.array([])\n",
    "    y_test_all = np.array([])\n",
    "    with torch.no_grad():\n",
    "        se = 0\n",
    "        denominator = 0\n",
    "        for idx, (x_test, y_test) in enumerate(test_loader):\n",
    "            #######################################################\n",
    "            x_test, y_test = x_test.to(device).unsqueeze(1), y_test.to(device)\n",
    "            #######################################################\n",
    "            y_test = y_test.cpu().numpy()\n",
    "            y_predict = np.zeros(y_test.shape)\n",
    "            for i in range(ensemble):\n",
    "                y_predict = y_predict + model(x_test).detach().squeeze().cpu().numpy()\n",
    "            # print(y_predict.shape)\n",
    "            y_predict = y_predict/ensemble\n",
    "            y_predict_all = np.append(y_predict_all, y_predict)\n",
    "            y_test_all = np.append(y_test_all, y_test)\n",
    "            se = se + np.sum((y_predict - y_test)**2)\n",
    "            denominator = denominator + np.sum(y_test**2)\n",
    "        roos = 1 - se/denominator\n",
    "        if vi:\n",
    "            print(roos)\n",
    "            return roos\n",
    "        else:\n",
    "            df = pd.DataFrame({'predict':y_predict_all, 'real':y_test_all})\n",
    "            df.to_csv(filename, mode='a', index=False, header=False)\n",
    "            print('Accuracy:', roos)\n",
    "\n",
    "\n",
    "def test_vi(device, model, test_loader, mask, filename, ensemble = 1):\n",
    "    print(\"Start variable importance computing!\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        roos_all = test(device, model, test_loader, vi=True)\n",
    "        se_all = np.zeros(102)\n",
    "        denominator = 0\n",
    "        for idx, (x_test, y_test) in enumerate(test_loader):\n",
    "            #######################################################\n",
    "            x_test, y_test = x_test.to(device), y_test.to('cpu').numpy()\n",
    "            #######################################################\n",
    "            # mask = 1 - torch.eye(921).to(device) # [D, D] off-diag are 1, diag are 0.\n",
    "            # mask = mask[:921-75,:]\n",
    "            x_test = x_test.unsqueeze(-2) * mask\n",
    "            # print(x_test.shape)\n",
    "            # print(x_test)\n",
    "            y_predict = np.zeros(x_test.shape[:2]).transpose(-1, -2)\n",
    "            for i in range(ensemble):\n",
    "                # random.seed(10)\n",
    "                temp = model(x_test).detach().cpu().numpy().squeeze(-1).transpose(-1, -2)\n",
    "                # print('temp:', temp.shape)\n",
    "                y_predict = y_predict + temp\n",
    "            y_predict = y_predict/ensemble\n",
    "            # print('y_predict:', y_predict.shape)\n",
    "            # print(y_predict - y_test)\n",
    "            # print('y_predict:', y_predict.shape)\n",
    "            # print('y_test:', y_test.shape)\n",
    "            se_all = se_all + np.sum((y_predict - y_test)**2, axis=1)\n",
    "            denominator = denominator + np.sum(y_test**2)\n",
    "        roos = 1 - se_all/denominator\n",
    "        print(roos.shape)\n",
    "        print(roos_all-roos)\n",
    "        print(\"Variable importance computing stopping!\")\n",
    "        with open(filename,'a') as fd:\n",
    "            writer = csv.writer(fd)\n",
    "            writer.writerow(roos_all-roos)\n",
    "        return roos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, train_index, val_index, test_index, filename_vi, filename_roos, mask, lr = 1e-2, l1_lambda = 1e-3, ):\n",
    "    # GPU device\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    print('Device state:', device)\n",
    "\n",
    "    epochs = 20\n",
    "    batch_size = 10000\n",
    "    val_batch_size = 100000\n",
    "    # lr = 1e-2\n",
    "    # l1_lambda = 1e-3\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    model = model().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Data\n",
    "    trainset = MyDataset(data.loc[train_index, \"sic2\"], \n",
    "                data.loc[train_index, character_names], \n",
    "                data.loc[train_index, macropredictors_names], \n",
    "                data.loc[train_index, \"RET\"], device)\n",
    "\n",
    "    validset = MyDataset(data.loc[val_index, \"sic2\"], \n",
    "                data.loc[val_index, character_names], \n",
    "                data.loc[val_index, macropredictors_names], \n",
    "                data.loc[val_index, \"RET\"], device)\n",
    "\n",
    "    testset = MyDataset(data.loc[test_index, \"sic2\"], \n",
    "                data.loc[test_index, character_names], \n",
    "                data.loc[test_index, macropredictors_names], \n",
    "                data.loc[test_index, \"RET\"], device)\n",
    "\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    validloader = DataLoader(validset, batch_size=val_batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    testloader = DataLoader(testset, batch_size=val_batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Train\n",
    "    model = traindata(device, model, epochs, optimizer, loss_function, l1_lambda, trainloader, validloader)\n",
    "\n",
    "    # Test\n",
    "    test(device, model, testloader, filename_roos)\n",
    "\n",
    "    # Test variable importance\n",
    "    # compute_variable_importance(device, model, data, train_index, val_batch_size, character_names, macropredictors_names, filename_vi)\n",
    "    # trainloader = DataLoader(trainset, batch_size=1000, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    # test_vi(device, model, trainloader, mask, filename_vi)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = datetime.strptime(\"1957-01-01\", \"%Y-%m-%d\")\n",
    "train_end = datetime.strptime(\"1974-12-31\", \"%Y-%m-%d\")\n",
    "val_start = datetime.strptime(\"1975-01-01\", \"%Y-%m-%d\")\n",
    "val_end = datetime.strptime(\"1986-12-31\", \"%Y-%m-%d\")\n",
    "test_start = datetime.strptime(\"1987-01-01\", \"%Y-%m-%d\")\n",
    "test_end = datetime.strptime(\"1987-12-31\", \"%Y-%m-%d\")\n",
    "filename_roos = \"L1_nn_result_27-28.csv\"\n",
    "filename_vi = \"L1_nn_vi.csv\"\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "mask = torch.ones(75*94).reshape(94,75).to(device)\n",
    "for i in range(9):\n",
    "    temp = 1-torch.eye(94).to(device)\n",
    "    mask = torch.cat((mask, temp), 1)\n",
    "for i in range(8):\n",
    "    temp = torch.ones(921).to(device)\n",
    "    temp[75+94+i*92:75+94+(i+1)*92] = 0\n",
    "    temp = temp.unsqueeze(0)\n",
    "    mask = torch.cat((mask, temp), 0)\n",
    "\n",
    "for i in range(30):\n",
    "    print(\"=====================\", i, \"===================\")\n",
    "    train_index = (data['DATE'] >= train_start) & (data['DATE'] <= train_end)\n",
    "    val_index = (data['DATE'] >= val_start) & (data['DATE'] <= val_end)\n",
    "    test_index = (data['DATE'] >= test_start) & (data['DATE'] <= test_end)\n",
    "    # print(data[train_index].head())\n",
    "    # print(data[train_index].tail())\n",
    "    \n",
    "    main(LeNet1, train_index, val_index, test_index, filename_vi, filename_roos, mask)\n",
    "\n",
    "    train_end = train_end + relativedelta(years=1)\n",
    "    val_start = val_start + relativedelta(years=1)\n",
    "    val_end = val_end + relativedelta(years=1)\n",
    "    test_start = test_start + relativedelta(years=1)\n",
    "    test_end = test_end + relativedelta(years=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f10aecbdcd0a7ca392daed1b0a2bb4e7bd6eec83b203e50374b332076e351b34"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
