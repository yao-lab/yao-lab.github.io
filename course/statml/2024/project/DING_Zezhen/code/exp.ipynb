{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0970d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, './input/g-research-crypto-forecasting')\n",
    "# import gresearch_crypto\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../tyche')\n",
    "import tool; from importlib import reload; reload(tool)\n",
    "from tool import Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle data process\n",
    "def kaggle_change_col_name(df):\n",
    "    names = {\n",
    "        'Open': 'open',\n",
    "        'High': 'high',\n",
    "        'Low': 'low',\n",
    "        'Close': 'close',\n",
    "        'Volume': 'volume',\n",
    "        'VWAP': 'vwap',\n",
    "        'Count': 'count'\n",
    "    }\n",
    "    df = df.rename(names)\n",
    "    return df\n",
    "\n",
    "def change_col_name(df):\n",
    "    names = {\n",
    "        'volccy': 'volume',\n",
    "    }\n",
    "    df = df.rename(names)\n",
    "    df = df.with_columns(pl.col('volume').cast(pl.Float64))\n",
    "    return df\n",
    "\n",
    "def kaggle_fix_data(df, all_timestamps=None):\n",
    "    if(all_timestamps is None):\n",
    "        start_time=1514764860000\n",
    "        end_time=1632182400000\n",
    "        all_timestamps = range(start_time, end_time, 60000)\n",
    "    full_df = pl.DataFrame({\n",
    "        'timestamp': all_timestamps\n",
    "    })\n",
    "    df = df.join(full_df, on='timestamp', how='outer')\n",
    "    df = df.with_columns(pl.col('timestamp_right').alias('timestamp'))\n",
    "    return df\n",
    "\n",
    "def ret(df):\n",
    "    for _ in [i*60 for i in [2, 5, 15, 30, 60, 120, 240, 480, 720, 1440, 2880, 10080]]:\n",
    "        lb = _ // 60\n",
    "        # df = df.with_columns(\n",
    "        #     ((pl.col('close').shift(-lb-1) / pl.col('close').shift(-1)) - 1).alias(f'ret_{_}s')\n",
    "        # )\n",
    "        df = df.with_columns(((pl.col('close').shift(-lb-1)/pl.col('close').shift(-1)).log()).alias(f'ret_{_}s')) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b625b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kaggle data\n",
    "def load_kaggle_kline():\n",
    "    kaggle_dfs = {}\n",
    "    TRAIN_CSV = './input/g-research-crypto-forecasting/train.csv'\n",
    "    ASSET_DETAILS_CSV = './input/g-research-crypto-forecasting/asset_details.csv'\n",
    "    data_df = pl.read_csv(TRAIN_CSV)\n",
    "    asset_df = pl.read_csv(ASSET_DETAILS_CSV)\n",
    "    asset_df = asset_df.sort('Asset_ID')\n",
    "    assets = asset_df['Asset_ID'].to_numpy()\n",
    "    asset_2_symbol = dict(zip(asset_df['Asset_ID'], asset_df['Symbol']))\n",
    "    symbol_2_wight = dict(zip(asset_df['Symbol'], asset_df['Weight']))\n",
    "\n",
    "    for asset_id in tqdm(assets):\n",
    "        symbol = asset_2_symbol[asset_id]\n",
    "        df = data_df.filter(pl.col('Asset_ID') == asset_id)\n",
    "\n",
    "        df = kaggle_change_col_name(df)\n",
    "        df = df.with_columns((pl.col('timestamp')*1000).alias('timestamp'))\n",
    "        df = df.sort('timestamp')\n",
    "        df = kaggle_fix_data(df)\n",
    "\n",
    "        df = df.with_columns((pl.col('timestamp')*1000).cast(pl.Datetime).alias('time'))\n",
    "        df = df.sort('timestamp')\n",
    "        df = df.with_columns(pl.lit(symbol).alias('symbol'))\n",
    "\n",
    "        df = ret(df)\n",
    "        kaggle_dfs[symbol] = df\n",
    "    return kaggle_dfs, asset_df\n",
    "\n",
    "\n",
    "def load_kline(symbols, file_root , file_names):\n",
    "    dfs = {}\n",
    "    for symbol in tqdm(symbols):\n",
    "        df = []\n",
    "        for file_name in file_names:\n",
    "            file_path = f\"{file_root}/{file_name}/OKEX_UM_1m/{symbol}-USDT-SWAP.csv\"\n",
    "            _ = pl.read_csv(file_path, dtypes={'vol': pl.Float64, 'volccy':pl.Float64,  'volccy_qoute': pl.Float64})\n",
    "            _ = _.fill_null(0)\n",
    "            if (_.shape[0] == 0):\n",
    "                continue\n",
    "            df.append(_)\n",
    "        df = pl.concat(df)\n",
    "        df = change_col_name(df)\n",
    "        df = df.group_by(df.columns).first()\n",
    "        df = df.with_columns((pl.col('timestamp')*1000).cast(pl.Datetime).alias('time'))\n",
    "        df = df.sort('timestamp')\n",
    "        df = df.with_columns(pl.lit(symbol).alias('symbol'))\n",
    "\n",
    "        # 生成Target\n",
    "        df = ret(df)\n",
    "        df = df.fill_null(0)\n",
    "        df = df.fill_nan(0)\n",
    "        dfs[symbol] = df\n",
    "        # print(symbol, (df.filter((pl.col('timestamp').diff()!=60000) | (pl.col('timestamp').diff(-1)!=-60000))).shape)\n",
    "    return dfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db155b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_target(df, feature):\n",
    "    p = 0.01\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(feature) < -p ).then(\n",
    "            -p).otherwise(pl.col(feature)).alias(f\"{feature}_clip\")\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(f\"{feature}_clip\") > p).then(\n",
    "            p).otherwise(pl.col(f\"{feature}_clip\")).alias(f\"{feature}_clip\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def gf_SM_A_M(df, feature, lb):\n",
    "    df = df.with_columns(df.rolling(index_column=\"time\", period=lb).agg([\n",
    "            pl.col(feature).mean().alias(f'bl.{feature}_mean_{lb}'),\n",
    "            pl.col(feature).std().alias(f'bl.{feature}_std_{lb}'),\n",
    "            pl.col(feature).median().alias(f'bl.{feature}_median_{lb}'),\n",
    "        ]\n",
    "    ))\n",
    "    return df\n",
    "\n",
    "def gf_EMA(df, feature, n):\n",
    "    alpha = 2 / (n + 1)\n",
    "    df = df.with_columns(\n",
    "        pl.col(feature).ewm_mean(alpha=alpha).alias(f'bl.{feature}_{n}_ema')\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def gf_MACD(df, feature, span1=12, span2=26, span3=9):\n",
    "    df = gf_EMA(df, feature, span1)\n",
    "    df = gf_EMA(df, feature, span2)\n",
    "    df = df.with_columns(((pl.col(f'bl.{feature}_{span1}_ema') / pl.col(f'bl.{feature}_{span2}_ema') - 1) * 100).alias(f'bl.{feature}_macd'))\n",
    "    df = gf_EMA(df, f'bl.{feature}_macd', span3)\n",
    "    return df\n",
    "\n",
    "def gf_Bollinger_Bands(df, feature, lb, no_of_std):\n",
    "    df = gf_SM_A_M(df, feature, lb)\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            (pl.col(f'bl.{feature}_mean_{lb}') + no_of_std * pl.col(f'bl.{feature}_std_{lb}')).alias(f'bl.{feature}_upper_band_{lb}'),\n",
    "            (pl.col(f'bl.{feature}_mean_{lb}') - no_of_std * pl.col(f'bl.{feature}_std_{lb}')).alias(f'bl.{feature}_lower_band_{lb}'),\n",
    "        ]\n",
    "    )\n",
    "    return df    \n",
    "\n",
    "def gf_RSI(df, feature, lb):\n",
    "    df = df.with_columns(\n",
    "        pl.col(feature).diff().fill_nan(0).alias(\"tp.delta\")\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"tp.delta\") > 0).then(pl.col(\"tp.delta\")).otherwise(0).alias(\"tp.up\"),\n",
    "        pl.when(pl.col(\"tp.delta\") < 0).then(-pl.col(\"tp.delta\")).otherwise(0).alias(\"tp.down\")\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        df.rolling(index_column=\"time\", period=lb).agg(\n",
    "            [\n",
    "                pl.col(\"tp.up\").mean().alias(\"tp.up_mean\"),\n",
    "                pl.col(\"tp.down\").mean().alias(\"tp.down_mean\"),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"tp.up_mean\") / pl.col(\"tp.down_mean\")).fill_nan(0).fill_null(0).alias(\"tp.rs\")\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        (100 - 100 / (1 + pl.col(\"tp.rs\"))).fill_nan(0).fill_null(0).alias(f'bl.{feature}_rsi_{lb}')\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def gf_alpha_RSI(df, feature, lb):\n",
    "    df = gf_RSI(df, feature, lb)\n",
    "    df = df.with_columns((pl.col(f'bl.{feature}_rsi_{lb}') / 100 * 2 - 1).fill_nan(0).fill_null(0).alias(f'al.{feature}_rsi_{lb}'))\n",
    "    return df\n",
    "\n",
    "def gf_alpha_back_ret(df, feature, n):\n",
    "    df = df.with_columns(\n",
    "        ((pl.col(feature) - pl.col(feature).shift(n))/ (pl.col(feature) + pl.col(feature).shift(n))).fill_nan(0).fill_null(0).alias(f'al.{feature}_ret_{n*60}s')\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def gf_alpha_sma(df, feature, lb):\n",
    "    df = gf_SM_A_M(df, feature, lb)\n",
    "    df = df.with_columns(((pl.col(f'bl.{feature}_mean_{lb}') - pl.col(feature))/(pl.col(f'bl.{feature}_mean_{lb}') + pl.col(feature))).fill_nan(0).fill_null(0).alias(f'al.{feature}_sma_{lb}'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def calc_slope(x):\n",
    "    slope = np.polyfit(range(len(x)), x, 1)[0]\n",
    "    return slope\n",
    "\n",
    "def gf_time(df):\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            pl.col(\"time\").dt.hour().alias(\"bl.timeofhour\"),\n",
    "            pl.col(\"time\").dt.weekday().alias(\"bl.timeofweek\"),\n",
    "        ]    \n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def gf_alpha_shadow(df, lb):\n",
    "    df = df.with_columns(df.rolling(index_column=\"time\", period=lb).agg(\n",
    "        [\n",
    "            pl.col('open').first().alias(f'bl.open_{lb}'),\n",
    "            pl.col('high').max().alias(f'bl.high_{lb}'),\n",
    "            pl.col('low').min().alias(f'bl.low_{lb}'),\n",
    "            pl.col('close').last().alias(f'bl.close_{lb}'),\n",
    "            pl.col('volume').sum().alias(f'bl.volume_{lb}'),\n",
    "        ]\n",
    "    ))\n",
    "\n",
    "    df = df.with_columns(\n",
    "        [ \n",
    "            (pl.col(f'bl.high_{lb}') - pl.max_horizontal(pl.col(f'bl.close_{lb}'), pl.col(f'bl.open_{lb}'))).alias(f'bl.upper_shadow_{lb}'),\n",
    "            (pl.min_horizontal(pl.col(f'bl.close_{lb}'), pl.col(f'bl.open_{lb}')) - pl.col(f'bl.low_{lb}')).alias(f'bl.lower_shadow_{lb}'),\n",
    "            (pl.col(f'bl.high_{lb}') - pl.col(f'bl.close_{lb}')).alias(f'bl.high_close_{lb}'),\n",
    "            (pl.col(f'bl.close_{lb}') - pl.col(f'bl.low_{lb}')).alias(f'bl.close_low_{lb}'),\n",
    "            (pl.col(f'bl.high_{lb}') - pl.col(f'bl.low_{lb}')).alias(f'bl.high_low_{lb}'),\n",
    "            (pl.col(f'bl.close_{lb}') - pl.col(f'bl.open_{lb}')).alias(f'bl.close_open_{lb}'),\n",
    "        ]\n",
    "    )  \n",
    "\n",
    "    df = df.with_columns(\n",
    "        [\n",
    "            ((pl.col(f'bl.upper_shadow_{lb}') - pl.col(f'bl.lower_shadow_{lb}')) /  (pl.col(f'bl.upper_shadow_{lb}') + pl.col(f'bl.lower_shadow_{lb}'))).fill_nan(0).alias(f'al.shadow_pressure_{lb}'),\n",
    "            ((pl.col(f'bl.high_close_{lb}') - pl.col(f'bl.close_low_{lb}')) / pl.col('close')).fill_nan(0).alias(f'al.from_high_low_{lb}'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Feature Engineering\n",
    "def get_features(df):\n",
    "    # df = df.with_row_index(\"index\")\n",
    "    df = df.sort('time')\n",
    "    LOOK_BACK_LIST = ['1m', '5m', '15m', '30m', '1h', '3h']\n",
    "    LOOK_BACK_LIST2 = [1, 5, 15, 30, 60, 180]\n",
    "\n",
    "    for lb in LOOK_BACK_LIST:\n",
    "        df = gf_alpha_shadow(df, lb)\n",
    "        df = gf_alpha_RSI(df, 'close', lb)\n",
    "        df = gf_alpha_sma(df, 'close', lb)\n",
    "\n",
    "    for n in LOOK_BACK_LIST2:\n",
    "        df = gf_alpha_back_ret(df, 'close', n)\n",
    "        df = gf_alpha_back_ret(df, 'volume', n)\n",
    "\n",
    "    df = gf_time(df)\n",
    "    df = gf_MACD(df, 'close')\n",
    "    for lb in LOOK_BACK_LIST:\n",
    "        df = gf_SM_A_M(df, 'close', lb)\n",
    "        df = gf_SM_A_M(df, 'volume', lb)\n",
    "        df = gf_Bollinger_Bands(df, 'close', lb, 2.5)\n",
    "\n",
    "    for n in LOOK_BACK_LIST2:\n",
    "        df = gf_EMA(df, 'close', n)\n",
    "        df = gf_EMA(df, 'volume', n)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2067d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_correlation(a, b, weights):\n",
    "    w = np.ravel(weights)\n",
    "    a = np.ravel(a)\n",
    "    b = np.ravel(b)\n",
    "\n",
    "    sum_w = np.sum(w)\n",
    "    mean_a = np.sum(a * w) / sum_w\n",
    "    mean_b = np.sum(b * w) / sum_w\n",
    "    var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n",
    "    var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n",
    "\n",
    "    cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n",
    "    corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "    return corr\n",
    "\n",
    "def total_corr(res, symbol_2_wight):\n",
    "    a = []\n",
    "    b = []\n",
    "    w = []\n",
    "    for symbol in symbols:\n",
    "        a.append(res[symbol][0])\n",
    "        b.append(res[symbol][1])\n",
    "        w.append(np.ones(len(res[symbol][0]))*symbol_2_wight[symbol])\n",
    "    a = np.concatenate(a)\n",
    "    b = np.concatenate(b)\n",
    "    w = np.concatenate(w)\n",
    "    corr = weighted_correlation(a,b,w)\n",
    "    # print(f\"TOTAL TEST CORR: {corr:.4f}\")\n",
    "    return corr\n",
    "\n",
    "def get_augment_data(df, select_features, targets):\n",
    "    for feature in select_features:\n",
    "        df = df.with_columns(-pl.col(feature).alias(f'{feature}'))\n",
    "    for target in targets:\n",
    "        df = df.with_columns(-pl.col(target).alias(f'{target}'))\n",
    "    return df\n",
    "    \n",
    "def fitting(train_df, test_df, target, model, data_select=True, data_augment=False):\n",
    "    clip_target = f'{target}_clip'\n",
    "    res = {}\n",
    "    test_dfs = {}\n",
    "    \n",
    "    train_df = train_df.sort('time')\n",
    "    features = [i for i in train_df.columns if 'al.' in i] # + [i for i in df.columns if 'bl.' in i]\n",
    "    if(data_select):\n",
    "        select_features = features # + ['open', 'high', 'low', 'close', 'volume']\n",
    "    else:\n",
    "        select_features = features\n",
    "\n",
    "    # train_df = train_df.fill_null(0)\n",
    "    # train_df = train_df.fill_nan(0)\n",
    "\n",
    "    # test_df = test_df.fill_null(0)\n",
    "    # test_df = test_df.fill_nan(0)\n",
    "\n",
    "    # for feature in select_features:\n",
    "    #     train_df = train_df.with_columns(pl.when(pl.col(feature).abs() == np.inf).then(0).otherwise(pl.col(feature)).alias(feature))\n",
    "    #     test_df = test_df.with_columns(pl.when(pl.col(feature).abs() == np.inf).then(0).otherwise(pl.col(feature)).alias(feature))\n",
    "\n",
    "    if(data_augment):\n",
    "        train_df_augment = get_augment_data(\n",
    "            train_df, select_features, [target, clip_target])\n",
    "        train_df_total = pl.concat([train_df, train_df_augment])\n",
    "    else:\n",
    "        train_df_total = train_df\n",
    "\n",
    "    # model = LGBMRegressor(num_leaves=5, max_depth=3, n_estimators=20)\n",
    "    # model = LGBMRegressor(num_leaves=8, max_depth=4, n_estimators=50)\n",
    "    # model = LGBMRegressor(num_leaves=10, max_depth=5, n_estimators=100)\n",
    "    model.fit(train_df_total[select_features], train_df_total[target])\n",
    "\n",
    "    train_df = train_df.with_columns(pl.Series('pred', model.predict(train_df[select_features])))\n",
    "    test_df = test_df.with_columns(pl.Series('pred', model.predict(test_df[select_features])))\n",
    "\n",
    "    symbols = test_df['symbol'].unique().sort()\n",
    "\n",
    "    for symbol in symbols:\n",
    "        train_df_new = train_df.filter(pl.col('symbol') == symbol)\n",
    "        test_df_new = test_df.filter(pl.col('symbol') == symbol)\n",
    "        print(symbol)\n",
    "        Tool.evaluation(train_df_new['pred'].to_numpy(),train_df_new[target].to_numpy())\n",
    "        Tool.evaluation(test_df_new['pred'].to_numpy(), test_df_new[target].to_numpy())\n",
    "        # train_corr = weighted_correlation(train_df_new['pred'].to_numpy(),train_df_new[target].to_numpy(), np.ones(train_df_new['pred'].shape[0]))\n",
    "        # test_corr = weighted_correlation(test_df_new['pred'].to_numpy(), test_df_new[target].to_numpy(), np.ones(test_df_new['pred'].shape[0]))\n",
    "        # print(f\"TRAIN CORR: {train_corr:.4f}, TEST CORR: {test_corr:.4f}\")\n",
    "        # Tool.show_single_feature(train_df_new, \"FR\", select_features,'pred', target, row_peroid=train_df_new.shape[0]//1, point_period=600, output_path=f\"./output/Train_{symbol}.html\")\n",
    "        # Tool.show_single_feature(test_df_new, \"FR\", select_features,'pred', target, row_peroid=test_df_new.shape[0]//1, point_period=600, output_path=f\"./output/Test_{symbol}.html\")\n",
    "        \n",
    "        res[symbol] = [test_df_new['pred'].to_numpy(), test_df_new[target].to_numpy()]\n",
    "        # plt.scatter(test_df[target], test_df['pred'], label='pred', alpha=0.1, s=1)\n",
    "        # plt.show()\n",
    "\n",
    "        test_dfs[symbol] = test_df_new\n",
    "    return res, test_dfs\n",
    "        \n",
    "def process_df(df, target):\n",
    "    df = df.filter(pl.col(target).is_null() == False)\n",
    "    df = df.filter(pl.col(target).is_nan() == False)\n",
    "    df = df.filter(pl.col('close').is_null() == False)\n",
    "    df = df.filter(pl.col('time').is_null() == False)\n",
    "    df = get_features(df)\n",
    "    df = get_clip_target(df, target)\n",
    "    features = [i for i in df.columns if 'al.' in i]\n",
    "    features.sort()\n",
    "    # Tool.show_feature_distribution(df, features)\n",
    "    return df\n",
    "    \n",
    "def select_data(df, start_time, end_time):\n",
    "    return df.filter((pl.col('time') >= start_time) & (pl.col('time') < end_time))\n",
    "\n",
    "\n",
    "def get_kaggle_target(dfs, asset_df):\n",
    "    \n",
    "    symbol_2_wight = dict(zip(asset_df['Symbol'], asset_df['Weight']))\n",
    "    m = np.zeros(dfs['BTC'].shape[0])\n",
    "\n",
    "    for symbol in tqdm(symbol_2_wight):\n",
    "        w = symbol_2_wight[symbol]\n",
    "        df = dfs[symbol].with_columns((pl.col('ret_900s') * w).alias('m'))\n",
    "        m = m + np.nan_to_num(df['m'].to_numpy()) / asset_df['Weight'].sum()\n",
    "\n",
    "    for symbol in tqdm(symbol_2_wight):\n",
    "        df = dfs[symbol].with_columns(pl.lit(m).alias('m'))\n",
    "        df = df.with_columns(pl.col('m').fill_null(0).fill_nan(0).alias('m'))\n",
    "        df = df.with_columns((pl.col('m') * pl.col('ret_900s')).fill_nan(0).fill_null(0).alias('rm'))\n",
    "        df = df.with_columns((pl.col('m') * pl.col('m')).alias('mm'))\n",
    "        df = df.with_columns((pl.col(\"rm\").rolling_sum(window_size=3750) / pl.col(\"mm\").rolling_sum(window_size=3750)).alias('beta'))\n",
    "        # df = df.with_columns(pl.col('beta').fill_null(0).alias('beta'))\n",
    "        df = df.with_columns((pl.col('ret_900s') - pl.col('beta') * pl.col('m')).alias('RE_Target'))\n",
    "        # print(symbol)\n",
    "        # print(df.filter(pl.col('Target').is_null() == False).shape)\n",
    "        # print(df.filter(pl.col('Target1').is_null() == False).shape)\n",
    "        # print(df.filter(pl.col('ret_900s').is_null() == False).shape)\n",
    "        # print(df.filter((pl.col('Target') - pl.col('Target1')).abs()>0.0001).shape[0] / df.shape[0])\n",
    "        dfs[symbol] = df\n",
    "        print(df.shape)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dfs, asset_df = load_kaggle_kline()\n",
    "asset_2_symbol = dict(zip(asset_df['Asset_ID'], asset_df['Symbol']))\n",
    "symbol_2_wight = dict(zip(asset_df['Symbol'], asset_df['Weight']))\n",
    "\n",
    "symbols = asset_df['Symbol'].to_numpy()\n",
    "symbols.sort()\n",
    "kaggle_dfs = get_kaggle_target(kaggle_dfs, asset_df)\n",
    "\n",
    "# symbols = symbols[symbols != 'XMR'] \n",
    "# symbols = pl.read_csv(\"../panel/kline/symbols.csv\")['symbol'].to_list()\n",
    "# file_root = \"..//panel/kline/\"\n",
    "# file_names = [\"update_okex_202404\", \"update_okex_202403\",\"update_okex_2022_2023\", \"update_okex_2021_2022\", \"update_okex_2020_2021\"]\n",
    "# dfs = load_kline(symbols[symbols != 'XMR'], file_root, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c145c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4918bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "target = 'ret_900s'\n",
    "kaggle_df_list = []\n",
    "kaggle_df_dict = {}\n",
    "\n",
    "for symbol in tqdm(symbols):\n",
    "    kaggle_df = process_df(kaggle_dfs[symbol].clone(), target)\n",
    "    kaggle_df_list.append(kaggle_df)\n",
    "    kaggle_df_dict[symbol] = kaggle_df\n",
    "    \n",
    "kaggle_df = pl.concat(kaggle_df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35470a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ret_900s'\n",
    "# target = 'Target'\n",
    "# target = 'RE_Target'\n",
    "# COMBINE = True\n",
    "COMBINE = True\n",
    "JOIN_FEATURE = True\n",
    "\n",
    "select_df = kaggle_df.clone()\n",
    "if(JOIN_FEATURE):\n",
    "    features = [i for i in kaggle_df.columns if 'al.' in i]\n",
    "    for symbol in ['BTC', 'ETH']:\n",
    "        df = kaggle_df_dict[symbol][features + ['timestamp']]\n",
    "        df = df.rename({i: f\"{i}.{symbol}\" for i in features})\n",
    "        select_df = select_df.join(df, on='timestamp', how='left')\n",
    "select_df = select_df.filter((pl.col(target).is_nan() == False) & (pl.col(target).is_null() == False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da664ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3year 9month never rolling\n",
    "\n",
    "COMBINE = True\n",
    "JOIN_FEATURE = True\n",
    "\n",
    "start_date = kaggle_df['time'].min()\n",
    "end_date = kaggle_df['time'].max()\n",
    "\n",
    "start_time = start_date\n",
    "mid_time = start_date + relativedelta(years=3)\n",
    "end_time = end_date\n",
    "train_df = select_data(select_df, start_time, mid_time)\n",
    "train_df.drop_nulls()\n",
    "test_df = select_data(select_df, mid_time, end_time)\n",
    "model = LGBMRegressor(num_leaves=10, max_depth=5, n_estimators=100)\n",
    "# model = Lasso(alpha=0.000001) \n",
    "# model = LinearRegression()\n",
    "test_dfs = []\n",
    "if(COMBINE):\n",
    "    res, tmp_dfs = fitting(train_df, test_df, target, model, data_select=True, data_augment=False)\n",
    "else:\n",
    "    res = {}\n",
    "    for symbol in symbols:\n",
    "        _, tmp_dfs = fitting(train_df.filter(pl.col(\"symbol\") == symbol), test_df.filter(pl.col(\"symbol\") == symbol), target, model, data_select=True, data_augment=True)\n",
    "        res[symbol] = _[symbol]\n",
    "\n",
    "\n",
    "corr = total_corr(res, symbol_2_wight)\n",
    "print(f\"TEST CORR: {corr:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rolling\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "TRAIN_MONTH = 36\n",
    "ROLLING_MONTH = 1\n",
    "SKIP_MONTH = 36 - TRAIN_MONTH\n",
    "\n",
    "start_date = select_df['time'].min()\n",
    "end_date = select_df['time'].max()\n",
    "start_time = start_date + relativedelta(months=SKIP_MONTH)\n",
    "mid_time = start_time + relativedelta(months=TRAIN_MONTH)\n",
    "end_time = mid_time + relativedelta(months=ROLLING_MONTH)\n",
    "\n",
    "corrs = []\n",
    "ress = []\n",
    "test_dfs = []\n",
    "while mid_time < end_date:\n",
    "    print(start_time, mid_time, end_time)\n",
    "    train_df = select_data(select_df, start_time, mid_time)\n",
    "    train_df.drop_nulls()\n",
    "    test_df = select_data(select_df, mid_time, end_time)\n",
    "    model = LGBMRegressor(num_leaves=10, max_depth=5, n_estimators=100)\n",
    "    # model = LinearRegression()\n",
    "    # model = Lasso(alpha=0.000001) \n",
    "    if(COMBINE):\n",
    "        res, tmp_dfs = fitting(train_df, test_df, target, model, data_select=True, data_augment=False)\n",
    "        test_dfs.append(tmp_dfs)\n",
    "    else:\n",
    "        res = {}\n",
    "        for symbol in symbols:\n",
    "            _, tmp_dfs = fitting(train_df.filter(pl.col(\"symbol\") == symbol).fill_nan(0), test_df.filter(pl.col(\"symbol\") == symbol).fill_nan(0), target, model, data_select=True, data_augment=False)\n",
    "            res[symbol] = _[symbol]\n",
    "    corr = total_corr(res, symbol_2_wight)\n",
    "    ress.append(res)\n",
    "    print(f\"TEST CORR: {corr:.4f}\")\n",
    "    corrs.append(corr)\n",
    "    start_time = start_time + relativedelta(months=1)\n",
    "    mid_time = mid_time + relativedelta(months=1)\n",
    "    end_time = min(end_time + relativedelta(months=1), end_date)\n",
    "\n",
    "res = {}\n",
    "for i in ress:\n",
    "    for k, v in i.items():\n",
    "        if(k not in res):\n",
    "            res[k] = []\n",
    "        res[k].append(v)\n",
    "for k in res:\n",
    "    res[k] = np.concatenate(res[k], axis=1)\n",
    "corr = total_corr(res, symbol_2_wight)\n",
    "print(f\"TEST CORR: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for i in ress:\n",
    "    r = {}\n",
    "    for k, v in i.items():\n",
    "        if(k not in res):\n",
    "            res[k] = []\n",
    "        res[k].append(v)\n",
    "        r[k] = v\n",
    "    corr = total_corr(r, symbol_2_wight)\n",
    "    print(f\"TEST CORR: {corr:.4f}\")\n",
    "\n",
    "for k in res:\n",
    "    res[k] = np.concatenate(res[k], axis=1)\n",
    "corr = total_corr(res, symbol_2_wight)\n",
    "print(f\"TEST CORR: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2aee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 0\n",
    "number2 = 0\n",
    "miss_data = {}\n",
    "total_data = {}\n",
    "for symbol in kaggle_dfs:\n",
    "    df = kaggle_dfs[symbol].filter(pl.col('Target').is_null() == False)\n",
    "    number += df.shape[0]\n",
    "    number2 += (df['timestamp'].max() - df['timestamp'].min())//60000 + 1 - df.shape[0]\n",
    "    miss_data[symbol] = (df['timestamp'].max() - df['timestamp'].min())//60000 + 1 - df.shape[0]\n",
    "    total_data[symbol] = df.shape[0]\n",
    "print(number, number2 / number)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_counts = {symbol: data for symbol, data in total_data.items()}\n",
    "missing_counts = {symbol: data for symbol, data in miss_data.items()}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.bar(data_counts.keys(), data_counts.values(), label='Data Count')\n",
    "plt.bar(missing_counts.keys(), missing_counts.values(), bottom=list(data_counts.values()), label='Missing Count')\n",
    "\n",
    "plt.title('Data and Missing Counts for Each Asset')\n",
    "plt.xlabel('Asset')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Tool.show_single_feature(kaggle_dfs['BTC'][1000:], '', [], row_peroid=kaggle_dfs['BTC'][1000:].shape[0], point_period=30*24*60)# output_path=f\"./output/BTC.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
