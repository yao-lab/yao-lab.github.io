{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have already did data preprocessing and feature engineering, here we only focus on the models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read the data and split the train & test according to date\n",
    "data = pd.read_csv(\"../m5-forecasting-data/train_data.csv\")\n",
    "x_train = data[data['date'] <= '2016-03-27']\n",
    "y_train = x_train['sell_price']\n",
    "x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n",
    "y_val = x_val['sell_price']\n",
    "test = data[(data['date'] > '2016-04-24')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[100]\ttraining's rmse: 2.3869\tvalid_1's rmse: 2.13703\n",
      "[200]\ttraining's rmse: 2.335\tvalid_1's rmse: 2.13161\n",
      "[300]\ttraining's rmse: 2.30045\tvalid_1's rmse: 2.12539\n",
      "[400]\ttraining's rmse: 2.27377\tvalid_1's rmse: 2.12436\n",
      "[500]\ttraining's rmse: 2.25216\tvalid_1's rmse: 2.12296\n",
      "[600]\ttraining's rmse: 2.23511\tvalid_1's rmse: 2.12073\n",
      "Early stopping, best iteration is:\n",
      "[647]\ttraining's rmse: 2.22812\tvalid_1's rmse: 2.11932\n",
      "Our val rmse score is 2.1193242240937646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "### train the lightgbm\n",
    "model = LGBMRegressor( n_estimators=50, learning_rate=0.1, subsample=0.75, max_depth=10 )\n",
    "model.fit(x_train, y_train, eval_set=[(x_train,y_train),(x_val,y_val)], eval_metric='rmse')\n",
    "valid_preds[X_valid.index] = model.predict(x_val)\n",
    "eval_preds[X_test.index] = model.predict(X_test)\n",
    "\n",
    "val_pred = model.predict(x_val[features])\n",
    "val_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\n",
    "print(f'Our val rmse score is {val_score}')\n",
    "y_pred = model.predict(test[features])\n",
    "\n",
    "### predict part\n",
    "predictions = test[['id', 'date', 'sell_prices']]\n",
    "predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'sell_prices').reset_index()\n",
    "predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "validation = submission[['id']].merge(predictions, on = 'id')\n",
    "final = pd.concat([validation, evaluation])\n",
    "final.to_csv('accu_lgbm_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "from ipywidgets import widgets, interactive\n",
    "import gc\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta \n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "import datetime as dt\n",
    "import random \n",
    "import os\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "pd.set_option('max_columns', 50)\n",
    "plt.style.use('bmh')\n",
    "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(x_train) * 0.67)\n",
    "test_size = len(x_train) - train_size\n",
    "\n",
    "x = x_train\n",
    "y = x_train['sell_price']\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True,dropout = 0.25)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size).to(device))\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        h_out = h_out.view(-1, self.hidden_size)\n",
    "        out = self.fc(h_out)\n",
    "        out = self.dropout(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.38196 valid loss:  0.40848 \n",
      "Epoch: 50, loss: 0.17385 valid loss:  0.11541 \n",
      "Epoch: 100, loss: 0.16984 valid loss:  0.10561 \n",
      "Epoch: 150, loss: 0.16380 valid loss:  0.09598 \n",
      "Epoch: 200, loss: 0.16025 valid loss:  0.10714 \n",
      "Epoch: 250, loss: 0.15275 valid loss:  0.11765 \n",
      "Epoch: 300, loss: 0.14970 valid loss:  0.10055 \n",
      "Epoch: 350, loss: 0.15538 valid loss:  0.09523 \n",
      "Epoch: 400, loss: 0.15899 valid loss:  0.09488 \n",
      "Epoch: 450, loss: 0.15080 valid loss:  0.09012 \n"
     ]
    }
   ],
   "source": [
    "#####  Parameters  ######################\n",
    "num_epochs = 500\n",
    "learning_rate = 1e-3\n",
    "input_size = 1\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_classes = 1\n",
    "\n",
    "model = myLSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "##### Optimzer and scheduler ####################\n",
    "criterion = torch.nn.MSELoss().to(device)    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=500,factor =0.5 ,min_lr=1e-7, eps=1e-08)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs): \n",
    "    model.train()\n",
    "    outputs = lstm(trainX)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Evaluate on test     \n",
    "    lstm.eval()\n",
    "    valid = lstm(testX.to(device))\n",
    "    vall_loss = criterion(valid, testY.to(device))\n",
    "    scheduler.step(vall_loss)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f \" %(epoch, loss.cpu().item(),vall_loss.cpu().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## submission \n",
    "timesteps = 14\n",
    "X_test = []\n",
    "X_test.append(test[0:timesteps])\n",
    "X_test = np.array(X_test)\n",
    "predictions = []\n",
    "\n",
    "for j in range(timesteps,timesteps + 28):\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    predicted_stock_price = regressor.predict(X_test[0,j - timesteps:j].reshape(1, timesteps, 30491))\n",
    "    testInput = np.column_stack((np.array(predicted_stock_price), pd.DataFrame(np.zeros((1969,1)))[1941:1969][0][1941 + j - timesteps]))\n",
    "    X_test = np.append(X_test, testInput).reshape(1,j + 1,30491)\n",
    "    sc = MinMaxScaler(feature_range = (0, 1))\n",
    "    predicted_stock_price = sc.inverse_transform(testInput)[:,0:30490]\n",
    "    predictions.append(predicted_stock_price)\n",
    "\n",
    "submission = pd.DataFrame(data=np.array(predictions).reshape(28,30490)).T\n",
    "submission = pd.concat((submission, submission), ignore_index=True)\n",
    "\n",
    "sample_submission = pd.read_csv(dataPath + \"/sample_submission.csv\")\n",
    "idColumn = sample_submission[[\"id\"]]\n",
    "submission[[\"id\"]] = idColumn  \n",
    "cols = list(submission.columns)\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "submission = submission[cols]\n",
    "\n",
    "colsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\n",
    "submission.columns = colsdeneme\n",
    "submission.to_csv(\"accu_lstm_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
