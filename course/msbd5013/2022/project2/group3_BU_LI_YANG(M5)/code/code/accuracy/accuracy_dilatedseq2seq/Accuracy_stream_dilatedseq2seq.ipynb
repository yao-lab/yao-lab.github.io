{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade category_encoders"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3gfaA9ke7Rq",
        "outputId": "f1a6ab41-583d-4439-b219-ac2e587743e4",
        "gather": {
          "logged": 1651458490485
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M5 - Accuracy"
      ],
      "metadata": {
        "id": "L3y5GrzgeFZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing to start \n",
        "## Loading packages"
      ],
      "metadata": {
        "id": "VCWOGFEOeFZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import category_encoders as ce\n",
        "import pickle as pkl\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "QMD75U7reFZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356a12d8-c655-4574-bc0e-fa1eb5d67970",
        "gather": {
          "logged": 1651458497062
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from importlib import import_module\n",
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "import sys"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "Ch-ekLQIeFZY",
        "gather": {
          "logged": 1651458508709
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.utils.data as data_utils\n",
        "import pickle as pkl\n",
        "import torch.nn as nn"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "X2YEBbq9eFZY",
        "gather": {
          "logged": 1651458508805
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from losses_and_metrics import loss_functions, metrics"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "A6XmOAdxeFZZ",
        "gather": {
          "logged": 1651458509011
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "rBbeLmWEeFZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "\n",
        "    resume_training = False\n",
        "    resume_from_fold = 1  # In case of k-fold training [1, k]\n",
        "\n",
        "    loss_fn = 'WRMSSELevel12Loss'\n",
        "    metric = 'WRMSSEMetric'\n",
        "    secondary_metric = 'RMSSELoss'\n",
        "    architecture = 'dilated_seq2seq'\n",
        "\n",
        "    # Running a sliding window training will help increase the training data\n",
        "    sliding_window = True  # Note: sliding window has not been tested with WRMSSELoss\n",
        "    window_length = 28 * 13\n",
        "\n",
        "    lag_and_roll_feats = True  # Note: Currently only works with dilated_seq2seq & seq2seq_w_attn_on_hid architectures\n",
        "    lags = list(range(27, 42))\n",
        "    rolling = [7, 14, 30, 60, 180]\n",
        "\n",
        "    # Regularization\n",
        "    add_random_noise = True\n",
        "    noise_rate = 0.5\n",
        "\n",
        "    # *** RNN *** #\n",
        "    # hidden dimension and no. of layers will be the same for both encoder and decoder\n",
        "    rnn_num_hidden = 128\n",
        "    rnn_num_layers = 2\n",
        "    bidirectional = True\n",
        "    enc_rnn_dropout = 0.2\n",
        "    dec_rnn_dropout = 0.0\n",
        "    teacher_forcing_ratio = 0.0\n",
        "\n",
        "    num_epochs = 2\n",
        "    batch_size = 160\n",
        "    learning_rate = 0.0003\n",
        "\n",
        "    # training, validation and test periods\n",
        "    training_ts = {'data_start_t': 1969 - 1 - (28 * 29), 'horizon_start_t': 1969 - 1 - (28 * 3),\n",
        "                   'horizon_end_t': 1969 - 1 - (28 * 2)}\n",
        "    validation_ts = {'data_start_t': 1969 - 1 - (28 * 15), 'horizon_start_t': 1969 - 1 - (28 * 2),\n",
        "                     'horizon_end_t': 1969 - 1 - (28 * 1)}\n",
        "    test_ts = {'data_start_t': 1969 - 1 - (28 * 14), 'horizon_start_t': 1969 - 1 - (28 * 1),\n",
        "               'horizon_end_t': 1969 - 1 - (28 * 0)}\n",
        "\n",
        "    # Parameters for k-fold training\n",
        "    k_fold = True\n",
        "    k_fold_splits = [(f_train_ts, f_val_ts) for f_train_ts, f_val_ts in\n",
        "                     zip([\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 31), 'horizon_start_t': 1969 - 1 - (28 * 5),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 4)},\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 30), 'horizon_start_t': 1969 - 1 - (28 * 4),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 3)},\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 29), 'horizon_start_t': 1969 - 1 - (28 * 3),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 2)}\n",
        "                     ], [\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 17), 'horizon_start_t': 1969 - 1 - (28 * 4),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 3)},\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 16), 'horizon_start_t': 1969 - 1 - (28 * 3),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 2)},\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 15), 'horizon_start_t': 1969 - 1 - (28 * 2),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 1)}\n",
        "                     ])]\n",
        "\n",
        "    data_file = './data.pickle'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "4xdErk4leFZa",
        "gather": {
          "logged": 1651458509591
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data <a class=\"anchor\" id=\"data\"></a>"
      ],
      "metadata": {
        "id": "mjbof3sIeFZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(input_data_dir='./m5-forecasting-accuracy', output_dir='.'):\n",
        "    train_data = pd.read_csv(f'{input_data_dir}/sales_train_evaluation.csv')\n",
        "    sell_prices = pd.read_csv(f'{input_data_dir}/sell_prices.csv')\n",
        "    calendar = pd.read_csv(f'{input_data_dir}/calendar.csv')\n",
        "\n",
        "    # ---- process calendar features ---- #\n",
        "    print('* Processing calendar features')\n",
        "\n",
        "    calendar.date = pd.to_datetime(calendar.date)\n",
        "    calendar['relative_year'] = 2016 - calendar.year\n",
        "\n",
        "    # convert month, day and weekday to cyclic encodings\n",
        "    calendar['month_sin'] = np.sin(2 * np.pi * calendar.month/12.0)\n",
        "    calendar['month_cos'] = np.cos(2 * np.pi * calendar.month/12.0)\n",
        "    calendar['day_sin'] = np.sin(2 * np.pi * calendar.date.dt.day/calendar.date.dt.days_in_month)\n",
        "    calendar['day_cos'] = np.cos(2 * np.pi * calendar.date.dt.day/calendar.date.dt.days_in_month)\n",
        "    calendar['weekday_sin'] = np.sin(2 * np.pi * calendar.wday/7.0)\n",
        "    calendar['weekday_cos'] = np.cos(2 * np.pi * calendar.wday/7.0)\n",
        "\n",
        "    # use same encoded labels for both the event name columns\n",
        "    cal_label = ['event_name_1', 'event_name_2']\n",
        "    cal_label_encoded_cols = ['event_name_1_enc', 'event_name_2_enc']\n",
        "    calendar[cal_label_encoded_cols] = calendar[cal_label]\n",
        "    cal_label_encoder = ce.OrdinalEncoder(cols=cal_label_encoded_cols)\n",
        "    cal_label_encoder.fit(calendar)\n",
        "    cal_label_encoder.mapping[1]['mapping'] = cal_label_encoder.mapping[0]['mapping']\n",
        "    calendar = cal_label_encoder.transform(calendar)\n",
        "\n",
        "    # subtract one from label encoded as pytorch uses 0-indexing\n",
        "    for col in cal_label_encoded_cols:\n",
        "        calendar[col] = calendar[col] - 1\n",
        "\n",
        "    calendar_df = calendar[['wm_yr_wk', 'd', 'snap_CA', 'snap_TX', 'snap_WI', 'relative_year',\n",
        "                            'month_sin', 'month_cos', 'day_sin', 'day_cos', 'weekday_sin', 'weekday_cos']\n",
        "                           + cal_label_encoded_cols]\n",
        "\n",
        "    # ---- Merge all dfs, keep calender_df features separate and just concat them for each batch ---- #\n",
        "    train_data.id = train_data.id.str[:-11]\n",
        "    sell_prices['id'] = sell_prices['item_id'] + '_' + sell_prices['store_id']\n",
        "\n",
        "    # add empty columns for future data\n",
        "    train_data = pd.concat([train_data, pd.DataFrame(columns=['d_'+str(i) for i in range(1942, 1970)])])\n",
        "\n",
        "    # Encode categorical features using either one-hot or label encoding (for embeddings)\n",
        "    print('* Encoding categorical features')\n",
        "    label = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
        "    label_encoded_cols = [str(i)+'_enc' for i in label]\n",
        "\n",
        "    train_data[label_encoded_cols] = train_data[label]\n",
        "    label_encoder = ce.OrdinalEncoder(cols=[str(i)+'_enc' for i in label])\n",
        "    label_encoder.fit(train_data)\n",
        "    train_data = label_encoder.transform(train_data)\n",
        "\n",
        "    # subtract one from label encoded as pytorch uses 0-indexing\n",
        "    for col in label_encoded_cols:\n",
        "        train_data[col] = train_data[col] - 1\n",
        "\n",
        "    # Reshape, change dtypes and add previous day sales\n",
        "    print('* Add previous day sales and merge sell prices')\n",
        "    data_df = pd.melt(train_data, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
        "                                           'item_id_enc', 'dept_id_enc', 'cat_id_enc', 'store_id_enc', 'state_id_enc'],\n",
        "                      var_name='d', value_vars=['d_'+str(i) for i in range(1, 1970)], value_name='sales')\n",
        "\n",
        "    # change dtypes to reduce memory usage\n",
        "    data_df[['sales']] = data_df[['sales']].fillna(-2).astype(np.int16)  # fill future sales as -2\n",
        "    calendar_df[['snap_CA', 'snap_TX', 'snap_WI', 'relative_year']] = calendar_df[\n",
        "        ['snap_CA', 'snap_TX', 'snap_WI', 'relative_year']].astype(np.int8)\n",
        "    calendar_df[cal_label_encoded_cols] = calendar_df[cal_label_encoded_cols].astype(np.int16)\n",
        "\n",
        "    data_df[label_encoded_cols] = data_df[label_encoded_cols].astype(np.int16)\n",
        "\n",
        "    # merge sell prices\n",
        "    data_df = data_df.merge(right=calendar_df[['d', 'wm_yr_wk']], on=['d'], how='left')\n",
        "    data_df = data_df.merge(right=sell_prices[['id', 'wm_yr_wk', 'sell_price']], on=['id', 'wm_yr_wk'], how='left')\n",
        "\n",
        "    data_df.sell_price = data_df.sell_price.fillna(0.0)\n",
        "    data_df['prev_day_sales'] = data_df.groupby(['id'])['sales'].shift(1)\n",
        "\n",
        "    # remove data for d_1\n",
        "    data_df.dropna(axis=0, inplace=True)\n",
        "    calendar_df = calendar_df[calendar_df.d != 'd_1']\n",
        "\n",
        "    # change dtypes\n",
        "    data_df[['prev_day_sales']] = data_df[['prev_day_sales']].astype(np.int16)\n",
        "\n",
        "    # remove category columns\n",
        "    del data_df['wm_yr_wk']\n",
        "    del data_df['item_id']\n",
        "    del data_df['dept_id']\n",
        "    del data_df['cat_id']\n",
        "    del data_df['store_id']\n",
        "    del data_df['state_id']\n",
        "\n",
        "    num_samples = data_df.id.nunique()\n",
        "    num_timesteps = data_df.d.nunique()\n",
        "    data_df = data_df.set_index(['id', 'd'])\n",
        "    \n",
        "    ids = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
        "    enc_dec_feats = ['sell_price'] + label_encoded_cols\n",
        "    enc_only_feats = data_df.columns.difference(['sales', 'sell_price', 'prev_day_sales'] + enc_dec_feats)\n",
        "\n",
        "    sales_data_ids = train_data[ids].values\n",
        "    Y = data_df.sales.values.reshape(num_timesteps, num_samples).T\n",
        "    X_enc_only_feats = np.array(data_df[enc_only_feats]).reshape(num_timesteps, num_samples, -1)\n",
        "    X_enc_dec_feats = np.array(data_df[enc_dec_feats]).reshape(num_timesteps, num_samples, -1)\n",
        "    X_prev_day_sales = data_df.prev_day_sales.values.reshape(num_timesteps, num_samples)\n",
        "    calendar_index = calendar_df.d\n",
        "    X_calendar = np.array(calendar_df.iloc[:, 2:])\n",
        "    X_calendar_cols = list(calendar_df.columns[2:])\n",
        "\n",
        "    # ---- Save processed data ---- #\n",
        "    print('* Save processed data')\n",
        "    data_dict = {'sales_data_ids': sales_data_ids, 'calendar_index': calendar_index,\n",
        "                 'X_prev_day_sales': X_prev_day_sales,\n",
        "                 'X_enc_only_feats': X_enc_only_feats, 'X_enc_dec_feats' : X_enc_dec_feats,\n",
        "                 'enc_dec_feat_names': enc_dec_feats, 'enc_only_feat_names': enc_only_feats,\n",
        "                 'X_calendar': X_calendar, 'X_calendar_cols': X_calendar_cols,\n",
        "                 'Y': Y,\n",
        "                 'cal_label_encoder': cal_label_encoder, 'label_encoder': label_encoder}\n",
        "\n",
        "    # pickle data\n",
        "    with open(f'{output_dir}/data.pickle', 'wb') as f:\n",
        "        pkl.dump(data_dict, f, protocol=pkl.HIGHEST_PROTOCOL)\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "LEiIZ9SpeFZc",
        "gather": {
          "logged": 1651458509727
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "read_data()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "* Processing calendar features\n* Encoding categorical features\n* Add previous day sales and merge sell prices\n* Save processed data\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "id": "3mQHw5X6eFZd",
        "outputId": "5fb83ec1-e368-4b8f-f31d-ec6f36a3a523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1651458699281
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate Data by level"
      ],
      "metadata": {
        "id": "4UqG147feFZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Level id|\tAggregation Level|\tNumber of series|\n",
        "|:----|:----|:----|\n",
        "|1|Unit sales of all products, aggregated for all stores/states|\t1|\n",
        "|2|Unit sales of all products, aggregated for each State|\t3|\n",
        "|3|Unit sales of all products, aggregated for each store| \t10|\n",
        "|4|Unit sales of all products, aggregated for each category|\t3|\n",
        "|5|Unit sales of all products, aggregated for each department|\t7|\n",
        "|6|Unit sales of all products, aggregated for each State and category|\t9|\n",
        "|7|Unit sales of all products, aggregated for each State and department|\t21|\n",
        "|8|Unit sales of all products, aggregated for each store and category|\t30|\n",
        "|9|Unit sales of all products, aggregated for each store and department|\t70|\n",
        "|10|Unit sales of product x, aggregated for all stores/states|\t3,049|\n",
        "|11|Unit sales of product x, aggregated for each State|\t9,147|\n",
        "|12|Unit sales of product x, aggregated for each store|\t30,490|\n",
        "| |**Total**|**42,840**|"
      ],
      "metadata": {
        "id": "dIM_I-CveFZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_aggregated_series(sales, sales_data_ids, agg_fn='sum'):\n",
        "    \"\"\"\n",
        "    Aggregates 30,490 level 12 series to generate data for all 42,840 series\n",
        "\n",
        "    Input data format:\n",
        "    sales: np array of shape (30490, num_timesteps)\n",
        "    sales_data_ids: np array of shape (30490, 5)\n",
        "                    with 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id' as the columns\n",
        "    agg_fn: function to be used for getting aggregated series' values ('mean' or 'sum')\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame({col: sales_data_ids[:, i] for col, i in\n",
        "                       zip(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], range(0, 5))})\n",
        "    df = pd.concat([df, pd.DataFrame(sales)], axis=1)\n",
        "    data_cols = [i for i in range(0, sales.shape[1])]\n",
        "\n",
        "    agg_indices, agg_series, agg_series_id = [], [], []\n",
        "\n",
        "    # Level 1\n",
        "    agg = np.sum(sales, 0) if agg_fn == 'sum' else np.mean(sales, 0)\n",
        "    agg_series.append(agg.reshape(1, -1))\n",
        "    agg_series_id.append(np.array(['Level1_Total_X']))\n",
        "\n",
        "    # Level 2\n",
        "    agg = df.groupby(['state_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level2_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 3\n",
        "    agg = df.groupby(['store_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level3_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 4\n",
        "    agg = df.groupby(['cat_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level4_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 5\n",
        "    agg = df.groupby(['dept_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level5_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 6\n",
        "    agg = df.groupby(['state_id', 'cat_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level6_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 7\n",
        "    agg = df.groupby(['state_id', 'dept_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level7_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 8\n",
        "    agg = df.groupby(['store_id', 'cat_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level8_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 9\n",
        "    agg = df.groupby(['store_id', 'dept_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level9_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 10\n",
        "    agg = df.groupby(['item_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level10_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 11\n",
        "    agg = df.groupby(['state_id', 'item_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level11_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 12\n",
        "    agg = df.set_index(['item_id', 'store_id'])[data_cols]\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level12_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Get affected_hierarchy_ids - all the series affected on updating each Level 12 series\n",
        "    affected_hierarchy_ids = np.empty((30490, 12), np.int32)\n",
        "\n",
        "    # Level 1\n",
        "    affected_hierarchy_ids[:, 0] = 0\n",
        "    fill_id, fill_col = 1, 1\n",
        "    # Level 2\n",
        "    for k, v in agg_indices[0].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 3\n",
        "    for k, v in agg_indices[1].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 4\n",
        "    for k, v in agg_indices[2].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 5\n",
        "    for k, v in agg_indices[3].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 6\n",
        "    for k, v in agg_indices[4].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 7\n",
        "    for k, v in agg_indices[5].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 8\n",
        "    for k, v in agg_indices[6].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 9\n",
        "    for k, v in agg_indices[7].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 10\n",
        "    for k, v in agg_indices[8].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 11\n",
        "    for k, v in agg_indices[9].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 12\n",
        "    affected_hierarchy_ids[:, fill_col] = fill_id + np.arange(0, 30490)\n",
        "\n",
        "    return np.concatenate(agg_series, axis=0), np.concatenate(agg_series_id, axis=0).\\\n",
        "        astype('<U28'), affected_hierarchy_ids"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "Cq-C2qqNeFZe",
        "gather": {
          "logged": 1651458699419
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights_all_levels(sales, sell_price, sales_data_ids):\n",
        "    \"\"\"\n",
        "    Generates weights for all 42,840 series\n",
        "\n",
        "    Input data format:\n",
        "    sales: np array of shape (30490, 28)\n",
        "    sell_price: np array of shape (30490, 28)\n",
        "\n",
        "    sales_data_ids: np array of shape (30490, 5)\n",
        "                with 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id' as the columns\n",
        "    \"\"\"\n",
        "\n",
        "    assert (sales.shape == sell_price.shape), \"Sell price and Sales arrays have different sizes\"\n",
        "    assert (sales.shape[1] == 28), \"Number of timesteps provided weight calculation is not equal to 28\"\n",
        "\n",
        "    # Get actual dollar sales for last 28 days for all 42,840 series\n",
        "    dollar_sales = sales * sell_price\n",
        "    agg_series, agg_series_id, _ = get_aggregated_series(dollar_sales, sales_data_ids)\n",
        "\n",
        "    # Sum up the actual dollar sales for all 28 timesteps\n",
        "    agg_series = agg_series.sum(1)\n",
        "\n",
        "    # Calculate total sales for each level\n",
        "    level_totals = agg_series[np.core.defchararray.find(agg_series_id, f'Level1_') == 0].sum()\n",
        "\n",
        "    # Calculate weight for each series\n",
        "    weights = agg_series / level_totals\n",
        "\n",
        "    return weights, agg_series_id\n",
        "\n",
        "\n",
        "def get_weights_level_12(sales, sell_price):\n",
        "    \"\"\"\n",
        "    Generates weights for only 30,490 level 12 series\n",
        "\n",
        "    Input data format:\n",
        "    sales: np array of shape (30490, 28)\n",
        "    sell_price: np array of shape (30490, 28)\n",
        "    \"\"\"\n",
        "\n",
        "    assert (sales.shape == sell_price.shape), \"Sell price and Sales arrays have different sizes\"\n",
        "    assert (sales.shape[1] == 28), \"Number of timesteps provided weight calculation is not equal to 28\"\n",
        "\n",
        "    # Get actual dollar sales for last 28 days and sum them up\n",
        "    dollar_sales = (sales * sell_price).sum(1)\n",
        "    # Calculate weight for each series\n",
        "    weights = dollar_sales / dollar_sales.sum()\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def update_preds_acc_hierarchy(prev_preds, preds, affected_ids):\n",
        "    \"\"\"\n",
        "    prev_preds: Previously stored predictions for all 42,840 series (42840, n_timesteps)\n",
        "    preds: Current batch predictions (batch_size, n_timesteps)\n",
        "    affected_ids: the ids of all the series affected by the series in preds (30490, 12)\n",
        "    \"\"\"\n",
        "\n",
        "    # get the change in predictions for the batch series\n",
        "    change_preds = (preds - prev_preds[affected_ids[:, -1]]).repeat_interleave(12, dim=0)\n",
        "\n",
        "    affected_ids = affected_ids.flatten()\n",
        "    prev_preds = prev_preds.index_add(0, affected_ids, change_preds)\n",
        "\n",
        "    return prev_preds\n"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "uiklkYU-eFZf",
        "gather": {
          "logged": 1651458699586
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "gg2RbtbHeFZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(data_utils.Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset\n",
        "\n",
        "    Let:\n",
        "    training period timesteps = [0, N]\n",
        "    prediction period timesteps = [N+1, N+P]\n",
        "\n",
        "    Arguments:\n",
        "    X_prev_day_sales : previous day sales for training period ([0, N])\n",
        "    X_enc_only_feats : aggregated series' previous day sales for training period ([0, N])\n",
        "    X_enc_dec_feats : sell price and categorical features for training and prediction period ([0, N+P])\n",
        "    X_calendar : calendar features for training and prediction period ([0, N+P])\n",
        "    X_last_day_sales : the actual sales for the day before the start of the prediction period (for timestep N)\n",
        "                       (this will serve as the first timestep's input for the decoder)\n",
        "    Y : actual sales, denoting targets for prediction period ([N+1, N+P])\n",
        "\n",
        "    Returns:\n",
        "    List of torch arrays:\n",
        "    x_enc: concatenated encoder features (except embedding)\n",
        "    x_enc_emb: concatenated encoder embedding features\n",
        "    x_dec: concatenated decoder features (except embedding)\n",
        "    x_dec_emb: concatenated decoder embedding features\n",
        "    x_last_day_sales: the actual sales for the day before the start of the prediction period\n",
        "    y: targets (only in training phase)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X_prev_day_sales, X_enc_only_feats, X_enc_dec_feats, X_calendar, norm_factor, norm_factor_sell_p,\n",
        "                 window_time_range, lagged_feats=None, rolling_feats=None, Y=None, rmsse_denominator=None,\n",
        "                 wrmsse_weights=None, window_id=None, config=None, is_training=True):\n",
        "\n",
        "        self.X_prev_day_sales = X_prev_day_sales\n",
        "        self.X_enc_only_feats = X_enc_only_feats\n",
        "        self.X_enc_dec_feats = X_enc_dec_feats\n",
        "        self.X_calendar = X_calendar\n",
        "        self.norm_factor = norm_factor\n",
        "        self.norm_factor_sell_p = norm_factor_sell_p\n",
        "        self.window_time_range = window_time_range\n",
        "        self.window_id = window_id\n",
        "        self.lagged_feats = lagged_feats\n",
        "        self.rolling_feats = rolling_feats\n",
        "        self.config = config\n",
        "        self.is_training = is_training\n",
        "\n",
        "        if Y is not None:\n",
        "            self.Y = torch.from_numpy(Y).float()\n",
        "            self.rmsse_denominator = torch.from_numpy(rmsse_denominator).float()\n",
        "            self.wrmsse_weights = torch.from_numpy(wrmsse_weights).float()\n",
        "        else:\n",
        "            self.Y = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.norm_factor.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.window_id is not None:\n",
        "            time_range = self.window_time_range[self.window_id[idx]]\n",
        "            scale = self.rmsse_denominator[idx - (self.window_id[idx] * 30490)]\n",
        "            weight = self.wrmsse_weights[idx - (self.window_id[idx] * 30490)]\n",
        "            ids_idx = idx - (self.window_id[idx] * 30490)\n",
        "            window_id = self.window_id[idx]\n",
        "        else:\n",
        "            time_range = self.window_time_range\n",
        "            ids_idx = idx\n",
        "            window_id = 0\n",
        "            if self.Y is not None:\n",
        "                scale = self.rmsse_denominator[idx]\n",
        "                weight = self.wrmsse_weights[idx]\n",
        "\n",
        "        # Filter data for time range of the selected window, also normalize prev_day_sales and sell_price\n",
        "        norm_factor = self.norm_factor[idx]\n",
        "        X_calendar = self.X_calendar[time_range[0]:time_range[2]]\n",
        "\n",
        "        X_prev_day_sales = self.X_prev_day_sales[time_range[0]:time_range[1], ids_idx] / norm_factor\n",
        "        X_prev_day_sales_dec = self.X_prev_day_sales[time_range[1]:time_range[2], ids_idx] / norm_factor\n",
        "        X_prev_day_sales[X_prev_day_sales < 0] = -1.0\n",
        "        X_prev_day_sales_dec[X_prev_day_sales_dec < 0] = -1.0\n",
        "\n",
        "        if self.lagged_feats is not None:\n",
        "            X_lag_feats_enc = self.lagged_feats[time_range[0]:time_range[1], ids_idx] / norm_factor\n",
        "            X_lag_feats_dec = self.lagged_feats[time_range[1]:time_range[2], ids_idx] / norm_factor\n",
        "            X_lag_feats_enc[X_lag_feats_enc < 0] = -1.0\n",
        "            X_lag_feats_dec[X_lag_feats_dec < 0] = -1.0\n",
        "            # rolling features for decoder will be calculated on the fly (by including predictions for the prev steps)\n",
        "            X_roll_feats_enc = self.rolling_feats[time_range[0]:time_range[1], ids_idx] / norm_factor\n",
        "\n",
        "        # If training and if enabled in config, multiply sales features by random noise\n",
        "        # (new value will be lower bound by 0)\n",
        "        if self.config.add_random_noise and self.is_training:\n",
        "            if len(X_prev_day_sales[X_prev_day_sales >= 0]) > 0:\n",
        "                random_noise = np.clip(np.random.normal(1, X_prev_day_sales[X_prev_day_sales >= 0].std(),\n",
        "                                                        time_range[2] - time_range[0]), 0, None)\n",
        "                noise = np.ones_like(random_noise)\n",
        "                mask = np.random.choice([0, 1], size=noise.shape, p=((1 - self.config.noise_rate),\n",
        "                                                                     self.config.noise_rate)).astype(np.bool)\n",
        "                noise[mask] = random_noise[mask]\n",
        "\n",
        "                X_prev_day_sales[X_prev_day_sales >= 0] *= noise[:time_range[1] - time_range[0]][X_prev_day_sales >= 0]\n",
        "                X_prev_day_sales_dec[X_prev_day_sales_dec >= 0] *= noise[time_range[1]\n",
        "                                                                         - time_range[2]:][X_prev_day_sales_dec >= 0]\n",
        "\n",
        "            if self.lagged_feats is not None:\n",
        "                # lagged features\n",
        "                if len(X_lag_feats_enc[X_lag_feats_enc >= 0]) > 0:\n",
        "                    random_noise = np.clip(np.random.normal(1, X_lag_feats_enc[X_lag_feats_enc >= 0].std(0),\n",
        "                                                            [time_range[2] - time_range[0],\n",
        "                                                             X_lag_feats_enc.shape[1]]), 0, None)\n",
        "                    noise = np.ones_like(random_noise)\n",
        "                    mask = np.random.choice([0, 1], size=noise.shape, p=((1 - self.config.noise_rate),\n",
        "                                                                         self.config.noise_rate)).astype(np.bool)\n",
        "                    noise[mask] = random_noise[mask]\n",
        "\n",
        "                    X_lag_feats_enc[X_lag_feats_enc >= 0] *= noise[:time_range[1] - time_range[0]][X_lag_feats_enc >= 0]\n",
        "                    X_lag_feats_dec[X_lag_feats_dec >= 0] *= noise[time_range[1]\n",
        "                                                                   - time_range[2]:][X_lag_feats_dec >= 0]\n",
        "\n",
        "                # rolling features\n",
        "                random_noise = np.clip(np.random.normal(1, X_roll_feats_enc[:, :len(self.config.rolling)].std(0),\n",
        "                                                        [time_range[1] - time_range[0],\n",
        "                                                         len(self.config.rolling)]), 0, None)\n",
        "                noise = np.ones_like(random_noise)\n",
        "                mask = np.random.choice([0, 1], size=noise.shape, p=((1 - self.config.noise_rate),\n",
        "                                                                     self.config.noise_rate)).astype(np.bool)\n",
        "                noise[mask] = random_noise[mask]\n",
        "\n",
        "                X_roll_feats_enc[:, :len(self.config.rolling)] *= noise\n",
        "                X_roll_feats_enc[:, len(self.config.rolling):] *= noise\n",
        "\n",
        "        X_enc_dec_feats = self.X_enc_dec_feats[time_range[0]:time_range[2], ids_idx]\n",
        "\n",
        "        # Directly dividing the sell price column leads to memory explosion\n",
        "        norm_factor_sell_p = np.ones_like(X_enc_dec_feats, np.float64)\n",
        "        norm_factor_sell_p[:, 0] = self.norm_factor_sell_p[idx]\n",
        "        X_enc_dec_feats = X_enc_dec_feats / norm_factor_sell_p\n",
        "\n",
        "        if self.Y is not None:\n",
        "            Y = self.Y[ids_idx, time_range[1]:time_range[2]]\n",
        "\n",
        "        enc_timesteps = time_range[1] - time_range[0]\n",
        "        dec_timesteps = time_range[2] - time_range[0] - enc_timesteps\n",
        "        num_embedding = 5\n",
        "        num_cal_embedding = 2\n",
        "\n",
        "        # input data for encoder\n",
        "        x_enc_dec_feats_enc = X_enc_dec_feats[:enc_timesteps, :-num_embedding].reshape(enc_timesteps, -1)\n",
        "\n",
        "        x_prev_day_sales_enc = X_prev_day_sales.reshape(-1, 1)\n",
        "        x_sales_feats_enc = x_prev_day_sales_enc if self.lagged_feats is None \\\n",
        "            else np.concatenate([x_prev_day_sales_enc, X_lag_feats_enc, X_roll_feats_enc], 1)\n",
        "\n",
        "        x_calendar_enc = X_calendar[:enc_timesteps, :-num_cal_embedding]\n",
        "        x_calendar_enc_emb = X_calendar[:enc_timesteps, -num_cal_embedding:].reshape(enc_timesteps, -1)\n",
        "\n",
        "        x_enc = np.concatenate([x_enc_dec_feats_enc, x_calendar_enc, x_sales_feats_enc], axis=1)\n",
        "        x_enc_emb = X_enc_dec_feats[:enc_timesteps, -num_embedding:].reshape(enc_timesteps, -1)\n",
        "\n",
        "        # input data for decoder\n",
        "        x_enc_dec_feats_dec = X_enc_dec_feats[enc_timesteps:, :-num_embedding].reshape(dec_timesteps, -1)\n",
        "        x_calendar_dec = X_calendar[enc_timesteps:, :-num_cal_embedding]\n",
        "        x_calendar_dec_emb = X_calendar[enc_timesteps:, -num_cal_embedding:].reshape(dec_timesteps, -1)\n",
        "\n",
        "        x_prev_day_sales_dec = X_prev_day_sales_dec.reshape(-1, 1)\n",
        "        x_sales_feats_dec = x_prev_day_sales_dec if self.lagged_feats is None \\\n",
        "            else np.concatenate([x_prev_day_sales_dec, X_lag_feats_dec], 1)\n",
        "\n",
        "        x_dec = np.concatenate([x_enc_dec_feats_dec, x_calendar_dec], axis=1)\n",
        "        x_dec_emb = X_enc_dec_feats[enc_timesteps:, -num_embedding:].reshape(dec_timesteps, -1)\n",
        "\n",
        "        if self.Y is None:\n",
        "            return [[torch.from_numpy(x_enc).float(), torch.from_numpy(x_enc_emb).long(),\n",
        "                     torch.from_numpy(x_calendar_enc_emb).long(),\n",
        "                     torch.from_numpy(x_dec).float(), torch.from_numpy(x_dec_emb).long(),\n",
        "                     torch.from_numpy(x_calendar_dec_emb).long(),\n",
        "                     torch.from_numpy(x_sales_feats_dec).float()], norm_factor]\n",
        "\n",
        "        return [[torch.from_numpy(x_enc).float(), torch.from_numpy(x_enc_emb).long(),\n",
        "                 torch.from_numpy(x_calendar_enc_emb).long(),\n",
        "                 torch.from_numpy(x_dec).float(), torch.from_numpy(x_dec_emb).long(),\n",
        "                 torch.from_numpy(x_calendar_dec_emb).long(),\n",
        "                 torch.from_numpy(x_sales_feats_dec).float()],\n",
        "                Y, torch.from_numpy(np.array(norm_factor)).float(),\n",
        "                ids_idx,\n",
        "                [scale, weight],\n",
        "                window_id]"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "id": "1uho53_deFZf",
        "gather": {
          "logged": 1651458699768
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        # load data\n",
        "        with open(f'{self.config.data_file}', 'rb') as f:\n",
        "            data_dict = pkl.load(f)\n",
        "\n",
        "        self.ids = data_dict['sales_data_ids']\n",
        "        self.enc_dec_feat_names = data_dict['enc_dec_feat_names']\n",
        "        self.sell_price_i = self.enc_dec_feat_names.index('sell_price')\n",
        "        self.X_prev_day_sales = data_dict['X_prev_day_sales']\n",
        "        self.X_enc_only_feats = data_dict['X_enc_only_feats']\n",
        "        self.X_enc_dec_feats = data_dict['X_enc_dec_feats']\n",
        "        self.X_calendar = data_dict['X_calendar']\n",
        "        self.enc_dec_feat_names = data_dict['enc_dec_feat_names']\n",
        "        self.Y = data_dict['Y']\n",
        "\n",
        "        # for prev_day_sales, set value as -1 for the period the product was not actively sold\n",
        "        self.X_prev_day_sales_unsold_negative = self.X_prev_day_sales.copy()\n",
        "        for idx, first_non_zero_idx in enumerate((self.X_prev_day_sales != 0).argmax(axis=0)):\n",
        "            self.X_prev_day_sales_unsold_negative[:first_non_zero_idx, idx] = -1\n",
        "\n",
        "        self.n_windows = 1\n",
        "\n",
        "    def create_train_loader(self, data_start_t=None, horizon_start_t=None, horizon_end_t=None):\n",
        "        if (data_start_t is None) | (horizon_start_t is None) | (horizon_end_t is None):\n",
        "            data_start_t = self.config.training_ts['data_start_t']\n",
        "            horizon_start_t = self.config.training_ts['horizon_start_t']\n",
        "            horizon_end_t = self.config.training_ts['horizon_end_t']\n",
        "\n",
        "        # Run a sliding window of length \"window_length\" and train for the next month of each window\n",
        "        if self.config.sliding_window:\n",
        "            window_length = self.config.window_length\n",
        "            window_time_range, norm_factor, norm_factor_sell_p = [], [], []\n",
        "            weights, scales = [], []\n",
        "\n",
        "            for idx, i in enumerate(range(data_start_t + window_length, horizon_end_t, 28)):\n",
        "                w_data_start_t, w_horizon_start_t = data_start_t + (idx * 28), i\n",
        "                w_horizon_end_t = w_horizon_start_t + 28\n",
        "                window_time_range.append([w_data_start_t - data_start_t, w_horizon_start_t - data_start_t,\n",
        "                                          w_horizon_end_t - data_start_t])\n",
        "\n",
        "                # calculate denominator for rmsse loss\n",
        "                squared_movement = ((self.Y.T[:w_horizon_start_t] -\n",
        "                                     self.X_prev_day_sales[:w_horizon_start_t]).astype(np.int64) ** 2)\n",
        "                actively_sold_in_range = (self.X_prev_day_sales[:w_horizon_start_t] != 0).argmax(axis=0)\n",
        "                rmsse_den = []\n",
        "                for idx_active_sell, first_active_sell_idx in enumerate(actively_sold_in_range):\n",
        "                    den = squared_movement[first_active_sell_idx:, idx_active_sell].mean()\n",
        "                    den = den if den != 0 else 1\n",
        "                    rmsse_den.append(den)\n",
        "                scales.append(np.array(rmsse_den))\n",
        "\n",
        "                # Get weights for WRMSSE and SPL loss\n",
        "                w_weights = get_weights_level_12(self.Y[:, w_horizon_start_t - 28:w_horizon_start_t],\n",
        "                                                 self.X_enc_dec_feats[w_horizon_start_t - 28:w_horizon_start_t, :,\n",
        "                                                 self.sell_price_i].T)\n",
        "                weights.append(w_weights)\n",
        "\n",
        "                # Normalize sale features by dividing by mean of each series (as per the selected input window)\n",
        "                w_X_prev_day_sales_calc = self.X_prev_day_sales[w_data_start_t:w_horizon_start_t]\n",
        "                w_norm_factor = np.mean(w_X_prev_day_sales_calc, 0)\n",
        "                w_norm_factor[w_norm_factor == 0] = 1.\n",
        "\n",
        "                w_X_sell_p = self.X_enc_dec_feats[w_data_start_t:w_horizon_start_t, :, self.sell_price_i].copy().astype(\n",
        "                    float)\n",
        "                w_norm_factor_sell_p = np.median(w_X_sell_p, 0)\n",
        "                w_norm_factor_sell_p[w_norm_factor_sell_p == 0] = 1.\n",
        "                norm_factor.append(w_norm_factor)\n",
        "                norm_factor_sell_p.append(w_norm_factor_sell_p)\n",
        "\n",
        "            self.n_windows = idx + 1\n",
        "            scales = np.concatenate(scales, 0)\n",
        "            weights = np.concatenate(weights, 0)\n",
        "            norm_factor = np.concatenate(norm_factor, 0)\n",
        "            norm_factor_sell_p = np.concatenate(norm_factor_sell_p, 0)\n",
        "            window_time_range = np.array(window_time_range)\n",
        "            window_id = np.arange(idx + 1).repeat(self.X_enc_dec_feats.shape[1])\n",
        "\n",
        "        else:\n",
        "            # calculate denominator for rmsse loss\n",
        "            squared_movement = ((self.Y.T[:horizon_start_t] -\n",
        "                                 self.X_prev_day_sales[:horizon_start_t]).astype(np.int64) ** 2)\n",
        "            actively_sold_in_range = (self.X_prev_day_sales[:horizon_start_t] != 0).argmax(axis=0)\n",
        "            rmsse_den = []\n",
        "            for idx_active_sell, first_active_sell_idx in enumerate(actively_sold_in_range):\n",
        "                den = squared_movement[first_active_sell_idx:, idx_active_sell].mean()\n",
        "                den = den if den != 0 else 1\n",
        "                rmsse_den.append(den)\n",
        "\n",
        "            # Get weights for WRMSSE and SPL loss\n",
        "            weights = get_weights_level_12(self.Y[:, horizon_start_t - 28:horizon_start_t],\n",
        "                                           self.X_enc_dec_feats[horizon_start_t - 28:horizon_start_t, :,\n",
        "                                           self.sell_price_i].T)\n",
        "\n",
        "            # Normalize sale features by dividing by mean of each series (as per the selected input window)\n",
        "            X_prev_day_sales_calc = self.X_prev_day_sales[data_start_t:horizon_start_t]\n",
        "            norm_factor = np.mean(X_prev_day_sales_calc, 0)\n",
        "            norm_factor[norm_factor == 0] = 1.\n",
        "\n",
        "            X_sell_p = self.X_enc_dec_feats[data_start_t:horizon_start_t, :, self.sell_price_i].copy().astype(float)\n",
        "            norm_factor_sell_p = np.median(X_sell_p, 0)\n",
        "            norm_factor_sell_p[norm_factor_sell_p == 0] = 1.\n",
        "\n",
        "            window_time_range = np.array([0, horizon_start_t - data_start_t, horizon_end_t - data_start_t])\n",
        "            scales = np.array(rmsse_den)\n",
        "            window_id = None\n",
        "\n",
        "        # Add rolling and lag features\n",
        "        if self.config.lag_and_roll_feats:\n",
        "            max_prev_ts_req = max(self.config.lags + self.config.rolling)\n",
        "            lagged_feats = []\n",
        "            for lag_i in np.array(sorted(self.config.lags, reverse=True)):\n",
        "                lag_i_feat = np.roll(self.X_prev_day_sales_unsold_negative[data_start_t - max_prev_ts_req:]\n",
        "                                     .astype(np.int32), lag_i, axis=0)\n",
        "                lag_i_feat[:lag_i] = 0\n",
        "                lagged_feats.append(lag_i_feat)\n",
        "            lagged_feats = np.stack(lagged_feats, axis=2)[max_prev_ts_req:]\n",
        "\n",
        "            rolling_feats, roll_i_means, roll_i_stds = [], [], []\n",
        "            roll_df = pd.DataFrame(self.X_prev_day_sales[data_start_t - max_prev_ts_req:].astype(np.int32))\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_mean = pd.DataFrame(roll_df).rolling(roll_i, axis=0).mean().fillna(0).values\n",
        "                roll_i_means.append(roll_i_feat_mean)\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_std = pd.DataFrame(roll_df).rolling(roll_i, axis=0).std().fillna(0).values\n",
        "                roll_i_stds.append(roll_i_feat_std)\n",
        "            rolling_feats = np.stack(roll_i_means + roll_i_stds, 2)[max_prev_ts_req:]\n",
        "        else:\n",
        "            lagged_feats, rolling_feats = None, None\n",
        "\n",
        "        dataset = CustomDataset(self.X_prev_day_sales_unsold_negative[data_start_t:],\n",
        "                                self.X_enc_only_feats[data_start_t:],\n",
        "                                self.X_enc_dec_feats[data_start_t:],\n",
        "                                self.X_calendar[data_start_t:],\n",
        "                                norm_factor, norm_factor_sell_p, window_time_range,\n",
        "                                lagged_feats, rolling_feats,\n",
        "                                Y=self.Y[:, data_start_t:],\n",
        "                                rmsse_denominator=scales, wrmsse_weights=weights, window_id=window_id,\n",
        "                                config=self.config)\n",
        "\n",
        "        return torch.utils.data.DataLoader(dataset=dataset, batch_size=self.config.batch_size, shuffle=True,\n",
        "                                           num_workers=3, pin_memory=True)\n",
        "\n",
        "    def create_val_loader(self, data_start_t=None, horizon_start_t=None, horizon_end_t=None):\n",
        "        if (data_start_t is None) | (horizon_start_t is None) | (horizon_end_t is None):\n",
        "            data_start_t = self.config.validation_ts['data_start_t']\n",
        "            horizon_start_t = self.config.validation_ts['horizon_start_t']\n",
        "            horizon_end_t = self.config.validation_ts['horizon_end_t']\n",
        "\n",
        "        # calculate denominator for rmsse loss\n",
        "        squared_movement = ((self.Y.T[:horizon_start_t] -\n",
        "                             self.X_prev_day_sales[:horizon_start_t]).astype(np.int64) ** 2)\n",
        "        actively_sold_in_range = (self.X_prev_day_sales[:horizon_start_t] != 0).argmax(axis=0)\n",
        "        rmsse_den = []\n",
        "        for idx, first_active_sell_idx in enumerate(actively_sold_in_range):\n",
        "            den = squared_movement[first_active_sell_idx:, idx].mean()\n",
        "            den = den if den != 0 else 1\n",
        "            rmsse_den.append(den)\n",
        "\n",
        "        # Get weights for WRMSSE and SPL loss\n",
        "        weights = get_weights_level_12(self.Y[:, horizon_start_t-28:horizon_start_t],\n",
        "                                       self.X_enc_dec_feats[horizon_start_t-28:horizon_start_t, :, self.sell_price_i].T)\n",
        "\n",
        "        # Normalize sale features by dividing by mean of each series (as per the selected input window)\n",
        "        X_prev_day_sales_calc = self.X_prev_day_sales[data_start_t:horizon_start_t]\n",
        "        norm_factor = np.mean(X_prev_day_sales_calc, 0)\n",
        "        norm_factor[norm_factor == 0] = 1.\n",
        "\n",
        "        X_sell_p = self.X_enc_dec_feats[data_start_t:horizon_start_t, :, self.sell_price_i].copy().astype(float)\n",
        "        norm_factor_sell_p = np.median(X_sell_p, 0)\n",
        "        norm_factor_sell_p[norm_factor_sell_p == 0] = 1.\n",
        "\n",
        "        window_time_range = [0, horizon_start_t - data_start_t, horizon_end_t - data_start_t]\n",
        "\n",
        "        # Add rolling and lag features\n",
        "        if self.config.lag_and_roll_feats:\n",
        "            max_prev_ts_req = max(self.config.lags + self.config.rolling)\n",
        "            lagged_feats = []\n",
        "            for lag_i in np.array(sorted(self.config.lags, reverse=True)):\n",
        "                lag_i_feat = np.roll(self.X_prev_day_sales_unsold_negative[data_start_t - max_prev_ts_req:]\n",
        "                                     .astype(np.int32), lag_i, axis=0)\n",
        "                lag_i_feat[:lag_i] = 0\n",
        "                lagged_feats.append(lag_i_feat)\n",
        "            lagged_feats = np.stack(lagged_feats, axis=2)[max_prev_ts_req:]\n",
        "\n",
        "            rolling_feats, roll_i_means, roll_i_stds = [], [], []\n",
        "            roll_df = pd.DataFrame(self.X_prev_day_sales[data_start_t - max_prev_ts_req:].astype(np.int32))\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_mean = pd.DataFrame(roll_df).rolling(roll_i, axis=0).mean().fillna(0).values\n",
        "                roll_i_means.append(roll_i_feat_mean)\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_std = pd.DataFrame(roll_df).rolling(roll_i, axis=0).std().fillna(0).values\n",
        "                roll_i_stds.append(roll_i_feat_std)\n",
        "            rolling_feats = np.stack(roll_i_means + roll_i_stds, 2)[max_prev_ts_req:]\n",
        "        else:\n",
        "            lagged_feats, rolling_feats = None, None\n",
        "\n",
        "        dataset = CustomDataset(self.X_prev_day_sales_unsold_negative[data_start_t:],\n",
        "                                self.X_enc_only_feats[data_start_t:],\n",
        "                                self.X_enc_dec_feats[data_start_t:],\n",
        "                                self.X_calendar[data_start_t:],\n",
        "                                norm_factor, norm_factor_sell_p, window_time_range,\n",
        "                                lagged_feats, rolling_feats,\n",
        "                                Y=self.Y[:, data_start_t:],\n",
        "                                rmsse_denominator=np.array(rmsse_den), wrmsse_weights=weights,\n",
        "                                config=self.config, is_training=False)\n",
        "\n",
        "        return torch.utils.data.DataLoader(dataset=dataset, batch_size=self.config.batch_size, num_workers=3,\n",
        "                                           pin_memory=True)\n",
        "\n",
        "    def create_test_loader(self, data_start_t=None, horizon_start_t=None, horizon_end_t=None):\n",
        "        if (data_start_t is None) | (horizon_start_t is None) | (horizon_end_t is None):\n",
        "            data_start_t = self.config.test_ts['data_start_t']\n",
        "            horizon_start_t = self.config.test_ts['horizon_start_t']\n",
        "            horizon_end_t = self.config.test_ts['horizon_end_t']\n",
        "\n",
        "        # Normalize sale features by dividing by mean of each series (as per the selected input window)\n",
        "        X_prev_day_sales_calc = self.X_prev_day_sales[data_start_t:horizon_start_t]\n",
        "        norm_factor = np.mean(X_prev_day_sales_calc, 0)\n",
        "        norm_factor[norm_factor == 0] = 1.\n",
        "\n",
        "        X_sell_p = self.X_enc_dec_feats[data_start_t:horizon_start_t, :, self.sell_price_i].copy().astype(float)\n",
        "        norm_factor_sell_p = np.median(X_sell_p, 0)\n",
        "        norm_factor_sell_p[norm_factor_sell_p == 0] = 1.\n",
        "\n",
        "        window_time_range = [0, horizon_start_t - data_start_t, horizon_end_t - data_start_t]\n",
        "\n",
        "        # Add rolling and lag features\n",
        "        if self.config.lag_and_roll_feats:\n",
        "            max_prev_ts_req = max(self.config.lags + self.config.rolling)\n",
        "            lagged_feats = []\n",
        "            for lag_i in np.array(sorted(self.config.lags, reverse=True)):\n",
        "                lag_i_feat = np.roll(self.X_prev_day_sales_unsold_negative[data_start_t - max_prev_ts_req:]\n",
        "                                     .astype(np.int32), lag_i, axis=0)\n",
        "                lag_i_feat[:lag_i] = 0\n",
        "                lagged_feats.append(lag_i_feat)\n",
        "            lagged_feats = np.stack(lagged_feats, axis=2)[max_prev_ts_req:]\n",
        "\n",
        "            rolling_feats, roll_i_means, roll_i_stds = [], [], []\n",
        "            roll_df = pd.DataFrame(self.X_prev_day_sales[data_start_t - max_prev_ts_req:].astype(np.int32))\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_mean = pd.DataFrame(roll_df).rolling(roll_i, axis=0).mean().fillna(0).values\n",
        "                roll_i_means.append(roll_i_feat_mean)\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_std = pd.DataFrame(roll_df).rolling(roll_i, axis=0).std().fillna(0).values\n",
        "                roll_i_stds.append(roll_i_feat_std)\n",
        "            rolling_feats = np.stack(roll_i_means + roll_i_stds, 2)[max_prev_ts_req:]\n",
        "        else:\n",
        "            lagged_feats, rolling_feats = None, None\n",
        "\n",
        "        dataset = CustomDataset(self.X_prev_day_sales_unsold_negative[data_start_t:],\n",
        "                                self.X_enc_only_feats[data_start_t:],\n",
        "                                self.X_enc_dec_feats[data_start_t:],\n",
        "                                self.X_calendar[data_start_t:],\n",
        "                                norm_factor, norm_factor_sell_p, window_time_range,\n",
        "                                lagged_feats, rolling_feats, config=self.config, is_training=False)\n",
        "\n",
        "        return torch.utils.data.DataLoader(dataset=dataset, batch_size=self.config.batch_size, num_workers=3,\n",
        "                                           pin_memory=True)\n",
        "\n",
        "    def get_weights_and_scaling(self, data_start_t, horizon_start_t, horizon_end_t):\n",
        "        \"\"\"Returns aggregated target, weights and rmsse scaling factors for series of all 12 levels\"\"\"\n",
        "\n",
        "        # Get aggregated series\n",
        "        agg_series_Y, agg_series_id, _ = get_aggregated_series(self.Y[:, :horizon_end_t], self.ids)\n",
        "        agg_target = agg_series_Y[:, horizon_start_t:]\n",
        "        agg_series_Y = agg_series_Y[:, :horizon_start_t]\n",
        "        agg_series_prev_day_sales, _, _ = get_aggregated_series(self.X_prev_day_sales.T[:, :horizon_start_t], self.ids)\n",
        "\n",
        "        # calculate denominator for rmsse loss\n",
        "        squared_movement = ((agg_series_Y.T - agg_series_prev_day_sales.T).astype(np.int64) ** 2)\n",
        "        actively_sold_in_range = (agg_series_prev_day_sales.T != 0).argmax(axis=0)\n",
        "        rmsse_den = []\n",
        "        for idx, first_active_sell_idx in enumerate(actively_sold_in_range):\n",
        "            den = squared_movement[first_active_sell_idx:, idx].mean()\n",
        "            den = den if den != 0 else 1\n",
        "            rmsse_den.append(den)\n",
        "\n",
        "        # Get weights\n",
        "        weights, _ = get_weights_all_levels(self.Y[:, horizon_start_t-28:horizon_start_t],\n",
        "                                            self.X_enc_dec_feats[horizon_start_t-28:horizon_start_t, :,\n",
        "                                            self.sell_price_i].T,\n",
        "                                            self.ids)\n",
        "\n",
        "        return agg_target, weights, np.array(rmsse_den)\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "id": "mivQv3ASeFZg",
        "gather": {
          "logged": 1651458700213
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training utils"
      ],
      "metadata": {
        "id": "4pvI6cBieFZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model checkpoint"
      ],
      "metadata": {
        "id": "pFpBT0JFeFZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelCheckpoint:\n",
        "    def __init__(self, weight_dir='./weights', config=Config):\n",
        "        self.weight_dir = weight_dir\n",
        "        self.config = config\n",
        "        file_prefix = '' if config.fold is None else f'fold_{config.fold}_'\n",
        "        self.filename = os.path.join(self.weight_dir, file_prefix + 'model_latest_checkpoint.pth.tar')\n",
        "        self.best_filename = os.path.join(self.weight_dir, file_prefix + 'model_best.pth.tar')\n",
        "\n",
        "    def save(self, is_best, min_val_error, num_bad_epochs, epoch, model, optimizer, scheduler=None):\n",
        "        scheduler_save = scheduler if scheduler is None else scheduler.state_dict()\n",
        "        save_dict = {\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'min_val_error': min_val_error,\n",
        "            'num_bad_epochs': num_bad_epochs,\n",
        "            'scheduler': scheduler_save\n",
        "        }\n",
        "        torch.save(save_dict, self.filename)\n",
        "        if is_best:\n",
        "            shutil.copyfile(self.filename, self.best_filename)\n",
        "\n",
        "    def load(self, model, optimizer=None, scheduler=None, load_best=False):\n",
        "        load_filename = self.best_filename if load_best else self.filename\n",
        "        if os.path.isfile(load_filename):\n",
        "            checkpoint = torch.load(load_filename, map_location=self.config.device)\n",
        "            model.load_state_dict(checkpoint['model'])\n",
        "            if optimizer is not None:\n",
        "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            if scheduler is not None:\n",
        "                scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "            min_val_error = checkpoint['min_val_error']\n",
        "            num_bad_epochs = checkpoint['num_bad_epochs']\n",
        "        else:\n",
        "            raise FileNotFoundError(f'No checkpoint found at {load_filename}')\n",
        "\n",
        "        return model, optimizer, scheduler, [start_epoch, min_val_error, num_bad_epochs]"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "id": "RsbrRFA6eFZh",
        "gather": {
          "logged": 1651458700543
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early Stopping"
      ],
      "metadata": {
        "id": "T6t2nnykeFZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping(object):\n",
        "    \"\"\"\n",
        "    author:https://github.com/stefanonardo\n",
        "    source: https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d\n",
        "    \"\"\"\n",
        "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.best = None\n",
        "        self.num_bad_epochs = 0\n",
        "        self.is_better = None\n",
        "        self._init_is_better(mode, min_delta, percentage)\n",
        "\n",
        "        if patience == 0:\n",
        "            self.is_better = lambda a, b: True\n",
        "            self.step = lambda a: False\n",
        "\n",
        "    def step(self, metrics):\n",
        "        if self.best is None:\n",
        "            self.best = metrics\n",
        "            return False\n",
        "\n",
        "        if np.isnan(metrics):\n",
        "            return True\n",
        "\n",
        "        if self.is_better(metrics, self.best):\n",
        "            self.num_bad_epochs = 0\n",
        "            self.best = metrics\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "\n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _init_is_better(self, mode, min_delta, percentage):\n",
        "        if mode not in {'min', 'max'}:\n",
        "            raise ValueError('mode ' + mode + ' is unknown!')\n",
        "        if not percentage:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - min_delta\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + min_delta\n",
        "        else:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - (\n",
        "                            best * min_delta / 100)\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + (\n",
        "                            best * min_delta / 100)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "id": "XouvNEwKeFZh",
        "gather": {
          "logged": 1651458700997
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "QFFJPqweeFZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.terminal_width = shutil.get_terminal_size((80, 20)).columns\n",
        "\n",
        "        # Model\n",
        "        print(f' Model: {self.config.architecture} '.center(self.terminal_width, '*'))\n",
        "        model_type = import_module('models.' + self.config.architecture)\n",
        "        create_model = getattr(model_type, 'create_model')\n",
        "        self.model = create_model(self.config)\n",
        "        print(self.model, end='\\n\\n')\n",
        "\n",
        "        # Loss, Optimizer and LRScheduler\n",
        "        self.criterion = getattr(loss_functions, self.config.loss_fn)(self.config)\n",
        "        self.optimizer = torch.optim.RMSprop(self.model.parameters(), lr=self.config.learning_rate, alpha=0.95)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5,\n",
        "                                                                    patience=3, verbose=True)\n",
        "        self.early_stopping = EarlyStopping(patience=10)\n",
        "        self.loss_agg = np.sum if config.loss_fn == 'WRMSSELevel12Loss' else np.mean\n",
        "\n",
        "        # Metric\n",
        "        self.metric = getattr(metrics, config.metric)()\n",
        "        self.metric_2 = getattr(loss_functions, config.secondary_metric)()\n",
        "\n",
        "        print(f' Loading Data '.center(self.terminal_width, '*'))\n",
        "        data_loader = DataLoader(self.config)\n",
        "        self.ids = data_loader.ids\n",
        "\n",
        "        self.train_loader = data_loader.create_train_loader()\n",
        "        self.val_loader = data_loader.create_val_loader()\n",
        "        self.train_metric_t, self.train_metric_w, self.train_metric_s = data_loader.get_weights_and_scaling(\n",
        "            self.config.training_ts['data_start_t'], self.config.training_ts['horizon_start_t'],\n",
        "            self.config.training_ts['horizon_end_t'])\n",
        "        self.val_metric_t, self.val_metric_w, self.val_metric_s = data_loader.get_weights_and_scaling(\n",
        "            self.config.validation_ts['data_start_t'], self.config.validation_ts['horizon_start_t'],\n",
        "            self.config.validation_ts['horizon_end_t'])\n",
        "        self.n_windows = data_loader.n_windows\n",
        "\n",
        "        self.start_epoch, self.min_val_error = 1, None\n",
        "        # Load checkpoint if training is to be resumed\n",
        "        self.model_checkpoint = ModelCheckpoint(config=self.config)\n",
        "        if config.resume_training:\n",
        "            self.model, self.optimizer, self.scheduler, [self.start_epoch, self.min_val_error, num_bad_epochs] = \\\n",
        "                self.model_checkpoint.load(self.model, self.optimizer, self.scheduler)\n",
        "            self.early_stopping.best = self.min_val_error\n",
        "            self.early_stopping.num_bad_epochs = num_bad_epochs\n",
        "            print(f'Resuming model training from epoch {self.start_epoch}')\n",
        "        else:\n",
        "            # remove previous logs, if any\n",
        "            if self.config.fold is None:\n",
        "                logs = glob.glob('./logs/.*') + glob.glob('./logs/*')\n",
        "                for f in logs:\n",
        "                    try:\n",
        "                        os.remove(f)\n",
        "                    except IsADirectoryError:\n",
        "                        shutil.rmtree(f)\n",
        "            else:\n",
        "                logs = glob.glob(f'./logs/fold_{self.config.fold}/.*') + glob.glob(f'./logs/fold_{self.config.fold}/*')\n",
        "                for f in logs:\n",
        "                    os.remove(f)\n",
        "\n",
        "        # logging\n",
        "        self.writer = SummaryWriter(f'logs') if self.config.fold is None \\\n",
        "            else SummaryWriter(f'logs/fold_{self.config.fold}')\n",
        "\n",
        "    def _get_val_loss_and_err(self):\n",
        "        self.model.eval()\n",
        "        progbar = tqdm(self.val_loader)\n",
        "        progbar.set_description(\"             \")\n",
        "        losses, sec_metric, epoch_preds, epoch_ids, epoch_ids_idx = [], [], [], [], []\n",
        "        for i, [x, y, norm_factor, ids_idx, loss_input, _] in enumerate(progbar):\n",
        "            x = [inp.to(self.config.device) for inp in x]\n",
        "            y = y.to(self.config.device)\n",
        "            norm_factor = norm_factor.to(self.config.device)\n",
        "            loss_input = [inp.to(self.config.device) for inp in loss_input]\n",
        "            epoch_ids.append(self.ids[ids_idx])\n",
        "            epoch_ids_idx.append(ids_idx.numpy())\n",
        "\n",
        "            preds = self.model(*x) * norm_factor[:, None]\n",
        "            epoch_preds.append(preds.data.cpu().numpy())\n",
        "            loss = self.criterion(preds, y, *loss_input)\n",
        "            losses.append(loss.data.cpu().numpy())\n",
        "            sec_metric.append(self.metric_2(preds, y, *loss_input).data.cpu().numpy())\n",
        "\n",
        "        # Sort to remove shuffle applied by the dataset loader\n",
        "        sort_idx = np.argsort(np.concatenate(epoch_ids_idx, axis=0))\n",
        "        epoch_preds = np.concatenate(epoch_preds, axis=0)[sort_idx]\n",
        "        epoch_ids = np.concatenate(epoch_ids, axis=0)[sort_idx]\n",
        "        validation_agg_preds, _, _ = get_aggregated_series(epoch_preds, epoch_ids)\n",
        "        val_error = self.metric.get_error(validation_agg_preds, self.val_metric_t, self.val_metric_s, self.val_metric_w)\n",
        "\n",
        "        return self.loss_agg(losses), val_error, np.mean(sec_metric)\n",
        "\n",
        "    def train(self):\n",
        "        print(f' Training '.center(self.terminal_width, '*'), end='\\n\\n')\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.config.num_epochs + 1):\n",
        "            print(f' Epoch [{epoch}/{self.config.num_epochs}] '.center(self.terminal_width, 'x'))\n",
        "            self.model.train()\n",
        "            progbar = tqdm(self.train_loader)\n",
        "            losses, sec_metric, epoch_preds, epoch_ids, epoch_ids_idx = [], [], [], [], []\n",
        "\n",
        "            for i, [x, y, norm_factor, ids_idx, loss_input, window_id] in enumerate(progbar):\n",
        "                x = [inp.to(self.config.device) for inp in x]\n",
        "                y = y.to(self.config.device)\n",
        "                norm_factor = norm_factor.to(self.config.device)\n",
        "                loss_input = [inp.to(self.config.device) for inp in loss_input]\n",
        "\n",
        "                # Forward + Backward + Optimize\n",
        "                self.optimizer.zero_grad()\n",
        "                preds = self.model(*x) * norm_factor[:, None]\n",
        "\n",
        "                if self.config.sliding_window:\n",
        "                    if torch.sum(window_id == self.n_windows - 1) > 0:\n",
        "                        epoch_ids.append(self.ids[ids_idx[window_id == self.n_windows - 1]].reshape(-1, 5))\n",
        "                        epoch_ids_idx.append(ids_idx[window_id == self.n_windows - 1].numpy())\n",
        "                        epoch_preds.append(preds[window_id == self.n_windows - 1].data.cpu().numpy().reshape(-1, 28))\n",
        "                else:\n",
        "                    epoch_ids.append(self.ids[ids_idx])\n",
        "                    epoch_ids_idx.append(ids_idx.numpy())\n",
        "                    epoch_preds.append(preds.data.cpu().numpy())\n",
        "\n",
        "                sec_metric.append(self.metric_2(preds, y, *loss_input).data.cpu().numpy())\n",
        "\n",
        "                loss = self.criterion(preds, y, *loss_input)\n",
        "                losses.append(loss.data.cpu().numpy())\n",
        "\n",
        "                if self.config.loss_fn == 'WRMSSELevel12Loss':\n",
        "                    progbar.set_description(\"loss = %0.3f \" % np.round(\n",
        "                        (len(self.train_loader) / (i + 1)) * self.loss_agg(losses) / self.n_windows, 3))\n",
        "                else:\n",
        "                    progbar.set_description(\"loss = %0.3f \" % np.round(self.loss_agg(losses), 3))\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # Get training and validation loss and error\n",
        "            # Sort to remove shuffle applied by the dataset loader\n",
        "            sort_idx = np.argsort(np.concatenate(epoch_ids_idx, axis=0))\n",
        "            epoch_preds = np.concatenate(epoch_preds, axis=0)[sort_idx]\n",
        "            epoch_ids = np.concatenate(epoch_ids, axis=0)[sort_idx]\n",
        "            training_agg_preds, _, _ = get_aggregated_series(epoch_preds, epoch_ids)\n",
        "            if self.config.loss_fn == 'WRMSSELevel12Loss':\n",
        "                train_loss = self.loss_agg(losses) / self.n_windows\n",
        "            else:\n",
        "                train_loss = self.loss_agg(losses)\n",
        "            train_error = self.metric.get_error(training_agg_preds, self.train_metric_t,\n",
        "                                                self.train_metric_s, self.train_metric_w)\n",
        "            train_error_2 = np.mean(sec_metric)\n",
        "\n",
        "            val_loss, val_error, val_error_2 = self._get_val_loss_and_err()\n",
        "\n",
        "            print(f'Training Loss: {train_loss:.4f}, Training Error: {train_error:.4f}, '\n",
        "                  f'Training Secondary Error: {train_error_2:.4f}\\n'\n",
        "                  f'Validation Loss: {val_loss:.4f}, Validation Error: {val_error:.4f}, '\n",
        "                  f'Validation Secondary Error: {val_error_2:.4f}')\n",
        "\n",
        "            # Change learning rate according to scheduler\n",
        "            self.scheduler.step(val_error)\n",
        "\n",
        "            # save checkpoint and best model\n",
        "            if self.min_val_error is None:\n",
        "                self.min_val_error = val_error\n",
        "                is_best = True\n",
        "                print(f'Best model obtained at the end of epoch {epoch}')\n",
        "            else:\n",
        "                if val_error < self.min_val_error:\n",
        "                    self.min_val_error = val_error\n",
        "                    is_best = True\n",
        "                    print(f'Best model obtained at the end of epoch {epoch}')\n",
        "                else:\n",
        "                    is_best = False\n",
        "            self.model_checkpoint.save(is_best, self.min_val_error, self.early_stopping.num_bad_epochs,\n",
        "                                       epoch, self.model, self.optimizer, self.scheduler)\n",
        "\n",
        "            # write logs\n",
        "            self.writer.add_scalar(f'{self.config.loss_fn}/train', train_loss, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.loss_fn}/val', val_loss, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.metric}/train', train_error, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.metric}/val', val_error, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.secondary_metric}/train', train_error_2, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.secondary_metric}/val', val_error_2, epoch * i)\n",
        "\n",
        "            # Early Stopping\n",
        "            if self.early_stopping.step(val_error):\n",
        "                print(f' Training Stopped'.center(self.terminal_width, '*'))\n",
        "                print(f'Early stopping triggered after epoch {epoch}')\n",
        "                break\n",
        "\n",
        "        self.writer.close()"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "id": "tCqzqg9teFZi",
        "gather": {
          "logged": 1651458701107
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## "
      ],
      "metadata": {
        "id": "REB60mYaeFZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sys.stdout = open('train.log', 'w')\n",
        "# sys.stderr = sys.stdout\n",
        "config = Config\n",
        "terminal_width = shutil.get_terminal_size((80, 20)).columns\n",
        "# Check if k-fold training is enabled\n",
        "if config.k_fold:\n",
        "    print(f' K-fold Training '.center(terminal_width, '*'))\n",
        "\n",
        "    # If resuming model training, start training from specified fold\n",
        "    start_fold = config.resume_from_fold - 1 if config.resume_training else 0\n",
        "\n",
        "    # Loop over all folds and train model using the corresponding fold config\n",
        "    for fold, [fold_train_ts, fold_val_ts] in enumerate(config.k_fold_splits):\n",
        "        if fold < start_fold:\n",
        "            continue\n",
        "        config.fold = fold + 1\n",
        "        print()\n",
        "        print(f' Fold [{config.fold}/{len(config.k_fold_splits)}] '.center(terminal_width, '*'))\n",
        "        config.training_ts, config.validation_ts = fold_train_ts, fold_val_ts\n",
        "\n",
        "        trainer = Trainer(config)\n",
        "        trainer.train()\n",
        "        config.resume_training = False  # Train future folds from the beginning\n",
        "else:\n",
        "    config.fold = None\n",
        "    trainer = Trainer(config)\n",
        "    trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "******************************* K-fold Training ********************************\n\n********************************** Fold [1/3] **********************************\n**************************** Model: dilated_seq2seq ****************************\nSeq2Seq(\n  (encoder): Encoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3049, 50)\n      (1): Embedding(7, 4)\n      (2): Embedding(3, 2)\n      (3): Embedding(10, 5)\n      (4): Embedding(3, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (drnns): ModuleList(\n      (0): DRNN(\n        (rnn_dropouts): ModuleList(\n          (0): Dropout(p=0.2, inplace=False)\n        )\n        (cells): Sequential(\n          (0): LSTM(132, 128)\n          (1): LSTM(128, 128)\n        )\n      )\n      (1): DRNN(\n        (rnn_dropouts): ModuleList(\n          (0): Dropout(p=0.2, inplace=False)\n        )\n        (cells): Sequential(\n          (0): LSTM(132, 128)\n          (1): LSTM(128, 128)\n        )\n      )\n    )\n  )\n  (decoder): Decoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3049, 50)\n      (1): Embedding(7, 4)\n      (2): Embedding(3, 2)\n      (3): Embedding(10, 5)\n      (4): Embedding(3, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (rnn): LSTM(132, 128, num_layers=2, bidirectional=True)\n    (pred): Linear(in_features=256, out_features=1, bias=True)\n  )\n  (fc_h0): Linear(in_features=6, out_features=4, bias=True)\n  (fc_h1): Linear(in_features=6, out_features=4, bias=True)\n)\n\n********************************* Loading Data *********************************\n*********************************** Training ***********************************\n\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [1/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.7816, Training Error: 0.9868, Training Secondary Error: 0.7463\nValidation Loss: 0.8672, Validation Error: 0.8662, Validation Secondary Error: 0.7288\nBest model obtained at the end of epoch 1\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [2/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.7618, Training Error: 0.8747, Training Secondary Error: 0.7364\nValidation Loss: 0.8605, Validation Error: 0.7556, Validation Secondary Error: 0.7282\nBest model obtained at the end of epoch 2\n\n********************************** Fold [2/3] **********************************\n**************************** Model: dilated_seq2seq ****************************\nSeq2Seq(\n  (encoder): Encoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3049, 50)\n      (1): Embedding(7, 4)\n      (2): Embedding(3, 2)\n      (3): Embedding(10, 5)\n      (4): Embedding(3, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (drnns): ModuleList(\n      (0): DRNN(\n        (rnn_dropouts): ModuleList(\n          (0): Dropout(p=0.2, inplace=False)\n        )\n        (cells): Sequential(\n          (0): LSTM(132, 128)\n          (1): LSTM(128, 128)\n        )\n      )\n      (1): DRNN(\n        (rnn_dropouts): ModuleList(\n          (0): Dropout(p=0.2, inplace=False)\n        )\n        (cells): Sequential(\n          (0): LSTM(132, 128)\n          (1): LSTM(128, 128)\n        )\n      )\n    )\n  )\n  (decoder): Decoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3049, 50)\n      (1): Embedding(7, 4)\n      (2): Embedding(3, 2)\n      (3): Embedding(10, 5)\n      (4): Embedding(3, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (rnn): LSTM(132, 128, num_layers=2, bidirectional=True)\n    (pred): Linear(in_features=256, out_features=1, bias=True)\n  )\n  (fc_h0): Linear(in_features=6, out_features=4, bias=True)\n  (fc_h1): Linear(in_features=6, out_features=4, bias=True)\n)\n\n********************************* Loading Data *********************************\n*********************************** Training ***********************************\n\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [1/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.7885, Training Error: 0.9088, Training Secondary Error: 0.7356\nValidation Loss: 0.8462, Validation Error: 0.9013, Validation Secondary Error: 0.7308\nBest model obtained at the end of epoch 1\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [2/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.7675, Training Error: 0.8428, Training Secondary Error: 0.7257\nValidation Loss: 0.8370, Validation Error: 0.5795, Validation Secondary Error: 0.7273\nBest model obtained at the end of epoch 2\n\n********************************** Fold [3/3] **********************************\n**************************** Model: dilated_seq2seq ****************************\nSeq2Seq(\n  (encoder): Encoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3049, 50)\n      (1): Embedding(7, 4)\n      (2): Embedding(3, 2)\n      (3): Embedding(10, 5)\n      (4): Embedding(3, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (drnns): ModuleList(\n      (0): DRNN(\n        (rnn_dropouts): ModuleList(\n          (0): Dropout(p=0.2, inplace=False)\n        )\n        (cells): Sequential(\n          (0): LSTM(132, 128)\n          (1): LSTM(128, 128)\n        )\n      )\n      (1): DRNN(\n        (rnn_dropouts): ModuleList(\n          (0): Dropout(p=0.2, inplace=False)\n        )\n        (cells): Sequential(\n          (0): LSTM(132, 128)\n          (1): LSTM(128, 128)\n        )\n      )\n    )\n  )\n  (decoder): Decoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3049, 50)\n      (1): Embedding(7, 4)\n      (2): Embedding(3, 2)\n      (3): Embedding(10, 5)\n      (4): Embedding(3, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (rnn): LSTM(132, 128, num_layers=2, bidirectional=True)\n    (pred): Linear(in_features=256, out_features=1, bias=True)\n  )\n  (fc_h0): Linear(in_features=6, out_features=4, bias=True)\n  (fc_h1): Linear(in_features=6, out_features=4, bias=True)\n)\n\n********************************* Loading Data *********************************\n*********************************** Training ***********************************\n\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [1/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.7954, Training Error: 0.8237, Training Secondary Error: 0.7473\nValidation Loss: 0.8448, Validation Error: 0.8857, Validation Secondary Error: 0.7566\nBest model obtained at the end of epoch 1\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [2/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.7747, Training Error: 0.7245, Training Secondary Error: 0.7372\nValidation Loss: 0.8406, Validation Error: 1.0651, Validation Secondary Error: 0.7558\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "loss = 0.782 : 100%|██████████| 2668/2668 [15:45<00:00,  2.82it/s]\n             : 100%|██████████| 191/191 [00:22<00:00,  8.65it/s]\nloss = 0.762 : 100%|██████████| 2668/2668 [15:46<00:00,  2.82it/s]\n             : 100%|██████████| 191/191 [00:22<00:00,  8.52it/s]\nloss = 0.789 : 100%|██████████| 2668/2668 [15:44<00:00,  2.83it/s]\n             : 100%|██████████| 191/191 [00:22<00:00,  8.67it/s]\nloss = 0.768 : 100%|██████████| 2668/2668 [15:48<00:00,  2.81it/s]\n             : 100%|██████████| 191/191 [00:22<00:00,  8.59it/s]\nloss = 0.795 : 100%|██████████| 2668/2668 [15:49<00:00,  2.81it/s]\n             : 100%|██████████| 191/191 [00:22<00:00,  8.56it/s]\nloss = 0.775 : 100%|██████████| 2668/2668 [15:57<00:00,  2.79it/s]\n             : 100%|██████████| 191/191 [00:22<00:00,  8.44it/s]\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "id": "JyjUiIQmeFZj",
        "outputId": "6d32db23-06f4-44c2-ad42-9d000337221b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "gather": {
          "logged": 1651465032249
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "lFr1COK2oIOu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Accuracy_stream.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}