{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timesteps = 28\n",
    "startDay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "import gc #importing garbage collector\n",
    "import time\n",
    "import sys\n",
    "from scipy import signal\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "SEED = 42\n",
    "#Pandas - Displaying more rorws and columns\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('m5-forecasting-accuracy/sales_train_evaluation.csv')\n",
    "df_prices = pd.read_csv('m5-forecasting-accuracy/sell_prices.csv')\n",
    "df_days = pd.read_csv('m5-forecasting-accuracy/calendar.csv')\n",
    "\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "df_prices = reduce_mem_usage(df_prices)\n",
    "df_days = reduce_mem_usage(df_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train=df_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_days['is_workday'] = 0\n",
    "df_days['is_workday'].loc[df_days['wday']>2] =1\n",
    "df_days['is_workday'] = df_days['is_workday'].astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_days['is_event_day'] = [1 if x ==False else 0 for x in df_days['event_name_1'].isnull()] \n",
    "df_days['is_event_day'] = df_days['is_event_day'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_days[\"date\"] = pd.to_datetime(df_days['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_days['week'] = df_days[\"date\"].dt.week\n",
    "df_days['week'] = df_days['week'].astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_days['num_events_week'] = df_days.groupby(by=['year','week'])['is_event_day'].transform('sum')\n",
    "df_days['num_events_week'] = df_days['num_events_week'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_days['is_event_week'] = [1 if x >0 else 0 for x in df_days['num_events_week']]\n",
    "df_days['is_event_week'] = df_days['is_event_week'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_days.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_after_event = df_days[df_days['is_event_day']==1].index.shift(1,freq='D')\n",
    "df_days['is_event_day_after'] = 0\n",
    "df_days['is_event_day_after'][df_days.index.isin(day_after_event)] = 1\n",
    "df_days['is_event_day_after'] = df_days['is_event_day_after'].astype(np.int8)\n",
    "\n",
    "del day_after_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_before_event = df_days[df_days['is_event_day']==1].index.shift(-1,freq='D')\n",
    "df_days['is_event_day_before'] = 0\n",
    "df_days['is_event_day_before'][df_days.index.isin(day_before_event)] = 1\n",
    "df_days['is_event_day_before'] = df_days['is_event_day_before'].astype(np.int8)\n",
    "\n",
    "del day_before_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_days.loc[:, \"is_sport_event\"] = ((df_days[\"event_type_1\"] == \"Sporting\") | (df_days[\"event_type_2\"] == \"Sporting\")).astype(\"int8\")\n",
    "df_days.loc[:, \"is_cultural_event\"] = ((df_days[\"event_type_1\"] == \"Cultural\") | (df_days[\"event_type_2\"] == \"Cultural\")).astype(\"int8\")\n",
    "df_days.loc[:, \"is_national_event\"] = ((df_days[\"event_type_1\"] == \"National\") | (df_days[\"event_type_2\"] == \"National\")).astype(\"int8\")\n",
    "df_days.loc[:, \"is_religious_event\"] = ((df_days[\"event_type_1\"] == \"Religious\") | (df_days[\"event_type_2\"] == \"Religious\")).astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days[['wm_yr_wk','month','year']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_full = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Train and Test Data Option a: Limited features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startDay = 0\n",
    "timesteps = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daysBeforeEventTest = df_days['is_event_day_before'][1941:1969]\n",
    "daysBeforeEvent = df_days['is_event_day_before'][startDay:1941]\n",
    "daysBeforeEvent.index = df_train.iloc[6:,:].index[startDay:1941]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_train, daysBeforeEvent], axis = 1)\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final = df_final[startDay:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "#Scale the features using min-max scaler in range 0-1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "dt_scaled = scaler.fit_transform(df_final.iloc[6:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(timesteps, 1941 - startDay):\n",
    "    X_train.append(dt_scaled[i-timesteps:i])\n",
    "    y_train.append(dt_scaled[i][0:dt_scaled.shape[1]-1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "print('Shape of X_train :'+str(X_train.shape))\n",
    "print('Shape of X_train :'+str(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = df_final[-timesteps:]\n",
    "inputs = scaler.transform(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[-timesteps:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining rmse as loss function\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del  df_final, df_train_full, df_days, df_train, dt_scaled, time, sys, signal, reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import tensorflow_probability as tfp\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import tensorflow_probability as tfp\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the RNN\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "layer_1_units=40\n",
    "model.add(LSTM(units = layer_1_units, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "layer_2_units=400\n",
    "model.add(LSTM(units = layer_2_units, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "layer_3_units=400\n",
    "model.add(LSTM(units = layer_3_units))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = y_train.shape[1]))\n",
    "\n",
    "# Compiling the RNN\n",
    "model.compile(optimizer = 'adam', loss = root_mean_squared_error)\n",
    "# alternative loss 'mse' or wrmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(fit.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with Limited Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "X_test.append(inputs[0:timesteps])\n",
    "X_test = np.array(X_test)\n",
    "predictions = []\n",
    "\n",
    "for j in range(timesteps,timesteps + 28):\n",
    "    predicted_volume = model.predict(X_test[0,j - timesteps:j].reshape(1, timesteps, 30491))\n",
    "    testInput = np.column_stack((np.array(predicted_volume), daysBeforeEventTest[0 + j - timesteps]))\n",
    "    X_test = np.append(X_test, testInput).reshape(1,j + 1,30491)\n",
    "    predicted_volume = scaler.inverse_transform(testInput)[:,0:30490]\n",
    "    predictions.append(predicted_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "submission = submission.T\n",
    "    \n",
    "submission = pd.concat((submission, submission), ignore_index=True)\n",
    "\n",
    "sample_submission = pd.read_csv('m5-forecasting-accuracy/sample_submission.csv')\n",
    "    \n",
    "idColumn = sample_submission[[\"id\"]]\n",
    "    \n",
    "submission[[\"id\"]] = idColumn  \n",
    "\n",
    "cols = list(submission.columns)\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "submission = submission[cols]\n",
    "\n",
    "colsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\n",
    "\n",
    "submission.columns = colsdeneme\n",
    "\n",
    "submission.to_csv(\"submission_evaluation_newloss_nodrop.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
