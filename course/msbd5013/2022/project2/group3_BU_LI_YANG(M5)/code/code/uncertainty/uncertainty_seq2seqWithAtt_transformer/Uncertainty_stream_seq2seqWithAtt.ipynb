{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Packages"
      ],
      "metadata": {
        "id": "gWMWvWATiVyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade category_encoders"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3pMszaWisDB",
        "outputId": "99d2c4fb-0aa8-4445-89df-f468e67aa3c2",
        "gather": {
          "logged": 1651329649458
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import category_encoders as ce\n",
        "import pickle as pkl\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "\n",
        "# Data Loader\n",
        "import sys\n",
        "\n",
        "sys.path.extend(['..'])\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.utils.data as data_utils\n",
        "import pickle as pkl\n",
        "\n",
        "# from utils.data_utils import *"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBkx7EMhhx3j",
        "outputId": "ec07828b-6347-4858-8c4c-bed710c909f2",
        "gather": {
          "logged": 1651329650629
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions Implementation"
      ],
      "metadata": {
        "id": "zXzsdDhIjcIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "Nf2rQrUtiaQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(input_data_dir='./m5-forecasting-uncertainty', output_dir='.'):\n",
        "    train_data = pd.read_csv(f'{input_data_dir}/sales_train_evaluation.csv')\n",
        "    sell_prices = pd.read_csv(f'{input_data_dir}/sell_prices.csv')\n",
        "    calendar = pd.read_csv(f'{input_data_dir}/calendar.csv')\n",
        "\n",
        "    # ---- process calendar features ---- #\n",
        "    print('* Processing calendar features')\n",
        "\n",
        "    calendar.date = pd.to_datetime(calendar.date)\n",
        "    calendar['relative_year'] = 2016 - calendar.year\n",
        "\n",
        "    # convert month, day and weekday to cyclic encodings\n",
        "    calendar['month_sin'] = np.sin(2 * np.pi * calendar.month/12.0)\n",
        "    calendar['month_cos'] = np.cos(2 * np.pi * calendar.month/12.0)\n",
        "    calendar['day_sin'] = np.sin(2 * np.pi * calendar.date.dt.day/calendar.date.dt.days_in_month)\n",
        "    calendar['day_cos'] = np.cos(2 * np.pi * calendar.date.dt.day/calendar.date.dt.days_in_month)\n",
        "    calendar['weekday_sin'] = np.sin(2 * np.pi * calendar.wday/7.0)\n",
        "    calendar['weekday_cos'] = np.cos(2 * np.pi * calendar.wday/7.0)\n",
        "\n",
        "    # use same encoded labels for both the event name columns\n",
        "    cal_label = ['event_name_1', 'event_name_2']\n",
        "    cal_label_encoded_cols = ['event_name_1_enc', 'event_name_2_enc']\n",
        "    calendar[cal_label_encoded_cols] = calendar[cal_label]\n",
        "    cal_label_encoder = ce.OrdinalEncoder(cols=cal_label_encoded_cols)\n",
        "    cal_label_encoder.fit(calendar)\n",
        "    cal_label_encoder.mapping[1]['mapping'] = cal_label_encoder.mapping[0]['mapping']\n",
        "    calendar = cal_label_encoder.transform(calendar)\n",
        "\n",
        "    # subtract one from label encoded as pytorch uses 0-indexing\n",
        "    for col in cal_label_encoded_cols:\n",
        "        calendar[col] = calendar[col] - 1\n",
        "\n",
        "    calendar_df = calendar[['wm_yr_wk', 'd', 'snap_CA', 'snap_TX', 'snap_WI', 'relative_year',\n",
        "                            'month_sin', 'month_cos', 'day_sin', 'day_cos', 'weekday_sin', 'weekday_cos']\n",
        "                           + cal_label_encoded_cols]\n",
        "\n",
        "    # ---- Merge all dfs, keep calender_df features separate and just concat them for each batch ---- #\n",
        "    train_data.id = train_data.id.str[:-11]\n",
        "    sell_prices['id'] = sell_prices['item_id'] + '_' + sell_prices['store_id']\n",
        "\n",
        "    # add empty columns for future data\n",
        "    train_data = pd.concat([train_data, pd.DataFrame(columns=['d_'+str(i) for i in range(1942, 1970)])])\n",
        "\n",
        "    # Encode categorical features using either one-hot or label encoding (for embeddings)\n",
        "    print('* Encoding categorical features')\n",
        "    label = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
        "    label_encoded_cols = [str(i)+'_enc' for i in label]\n",
        "\n",
        "    train_data[label_encoded_cols] = train_data[label]\n",
        "    label_encoder = ce.OrdinalEncoder(cols=[str(i)+'_enc' for i in label])\n",
        "    label_encoder.fit(train_data)\n",
        "    train_data = label_encoder.transform(train_data)\n",
        "\n",
        "    # subtract one from label encoded as pytorch uses 0-indexing\n",
        "    for col in label_encoded_cols:\n",
        "        train_data[col] = train_data[col] - 1\n",
        "\n",
        "    # Reshape, change dtypes and add previous day sales\n",
        "    print('* Add previous day sales and merge sell prices')\n",
        "    data_df = pd.melt(train_data, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
        "                                           'item_id_enc', 'dept_id_enc', 'cat_id_enc', 'store_id_enc', 'state_id_enc'],\n",
        "                      var_name='d', value_vars=['d_'+str(i) for i in range(1, 1970)], value_name='sales')\n",
        "\n",
        "    # change dtypes to reduce memory usage\n",
        "    data_df[['sales']] = data_df[['sales']].fillna(-2).astype(np.int16)  # fill future sales as -2\n",
        "    calendar_df[['snap_CA', 'snap_TX', 'snap_WI', 'relative_year']] = calendar_df[\n",
        "        ['snap_CA', 'snap_TX', 'snap_WI', 'relative_year']].astype(np.int8)\n",
        "    calendar_df[cal_label_encoded_cols] = calendar_df[cal_label_encoded_cols].astype(np.int16)\n",
        "\n",
        "    data_df[label_encoded_cols] = data_df[label_encoded_cols].astype(np.int16)\n",
        "\n",
        "    # merge sell prices\n",
        "    data_df = data_df.merge(right=calendar_df[['d', 'wm_yr_wk']], on=['d'], how='left')\n",
        "    data_df = data_df.merge(right=sell_prices[['id', 'wm_yr_wk', 'sell_price']], on=['id', 'wm_yr_wk'], how='left')\n",
        "\n",
        "    data_df.sell_price = data_df.sell_price.fillna(0.0)\n",
        "    data_df['prev_day_sales'] = data_df.groupby(['id'])['sales'].shift(1)\n",
        "\n",
        "    # remove data for d_1\n",
        "    data_df.dropna(axis=0, inplace=True)\n",
        "    calendar_df = calendar_df[calendar_df.d != 'd_1']\n",
        "\n",
        "    # change dtypes\n",
        "    data_df[['prev_day_sales']] = data_df[['prev_day_sales']].astype(np.int16)\n",
        "\n",
        "    # ---- Add previous day totals of aggregated series as features ---- #\n",
        "    # print('* Add previous day totals of aggregated series as features')\n",
        "    # # total\n",
        "    # data_df = data_df.merge(right=\n",
        "    #                         data_df.groupby(['d'])[['prev_day_sales']].sum().astype(\n",
        "    #                             np.int32).add_suffix('_all').reset_index(),\n",
        "    #                         on=['d'], how='left')\n",
        "    # # category level\n",
        "    # data_df = data_df.merge(right=data_df.groupby(['d', 'cat_id'])[['prev_day_sales']].sum().astype(\n",
        "    #                             np.int32).reset_index().pivot(\n",
        "    #                             index='d', columns='cat_id', values='prev_day_sales').add_prefix('prev_d_cat_'),\n",
        "    #                         on=['d'], how='left')\n",
        "    # # state level\n",
        "    # data_df = data_df.merge(right=\n",
        "    #                         data_df.groupby(['d', 'state_id'])[['prev_day_sales']].sum().astype(\n",
        "    #                             np.int32).reset_index().pivot(\n",
        "    #                             index='d', columns='state_id', values='prev_day_sales').add_prefix('prev_d_state_'),\n",
        "    #                         on=['d'], how='left')\n",
        "    # # store level\n",
        "    # data_df = data_df.merge(right=\n",
        "    #                         data_df.groupby(['d', 'store_id'])[['prev_day_sales']].sum().astype(\n",
        "    #                             np.int32).reset_index().pivot(\n",
        "    #                             index='d', columns='store_id', values='prev_day_sales').add_prefix('prev_d_store_'),\n",
        "    #                         on=['d'], how='left')\n",
        "    # # department level\n",
        "    # data_df = data_df.merge(right=\n",
        "    #                         data_df.groupby(['d', 'dept_id'])[['prev_day_sales']].sum().astype(\n",
        "    #                             np.int32).reset_index().pivot(\n",
        "    #                             index='d', columns='dept_id', values='prev_day_sales').add_prefix('prev_d_dept_'),\n",
        "    #                         on=['d'], how='left')\n",
        "\n",
        "    # remove category columns\n",
        "    del data_df['wm_yr_wk']\n",
        "    del data_df['item_id']\n",
        "    del data_df['dept_id']\n",
        "    del data_df['cat_id']\n",
        "    del data_df['store_id']\n",
        "    del data_df['state_id']\n",
        "\n",
        "    num_samples = data_df.id.nunique()\n",
        "    num_timesteps = data_df.d.nunique()\n",
        "    data_df = data_df.set_index(['id', 'd'])\n",
        "    \n",
        "    ids = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
        "    enc_dec_feats = ['sell_price'] + label_encoded_cols\n",
        "    enc_only_feats = data_df.columns.difference(['sales', 'sell_price', 'prev_day_sales'] + enc_dec_feats)\n",
        "\n",
        "    sales_data_ids = train_data[ids].values\n",
        "    Y = data_df.sales.values.reshape(num_timesteps, num_samples).T\n",
        "    X_enc_only_feats = np.array(data_df[enc_only_feats]).reshape(num_timesteps, num_samples, -1)\n",
        "    X_enc_dec_feats = np.array(data_df[enc_dec_feats]).reshape(num_timesteps, num_samples, -1)\n",
        "    X_prev_day_sales = data_df.prev_day_sales.values.reshape(num_timesteps, num_samples)\n",
        "    calendar_index = calendar_df.d\n",
        "    X_calendar = np.array(calendar_df.iloc[:, 2:])\n",
        "    X_calendar_cols = list(calendar_df.columns[2:])\n",
        "\n",
        "    # # for prev_day_sales and sales (y), set value as -1 for the period the product was not actively sold\n",
        "    # for idx, first_non_zero_idx in enumerate((X_prev_day_sales != 0).argmax(axis=0)):\n",
        "    #     X_prev_day_sales[:first_non_zero_idx, idx] = -1\n",
        "    # for idx, first_non_zero_idx in enumerate((Y != 0).argmax(axis=1)):\n",
        "    #     Y[idx, :first_non_zero_idx] = -1\n",
        "\n",
        "    # ---- Save processed data ---- #\n",
        "    print('* Save processed data')\n",
        "    data_dict = {'sales_data_ids': sales_data_ids, 'calendar_index': calendar_index,\n",
        "                 'X_prev_day_sales': X_prev_day_sales,\n",
        "                 'X_enc_only_feats': X_enc_only_feats, 'X_enc_dec_feats' : X_enc_dec_feats,\n",
        "                 'enc_dec_feat_names': enc_dec_feats, 'enc_only_feat_names': enc_only_feats,\n",
        "                 'X_calendar': X_calendar, 'X_calendar_cols': X_calendar_cols,\n",
        "                 'Y': Y,\n",
        "                 'cal_label_encoder': cal_label_encoder, 'label_encoder': label_encoder}\n",
        "\n",
        "    # pickle data\n",
        "    with open(f'{output_dir}/data.pickle', 'wb') as f:\n",
        "        pkl.dump(data_dict, f, protocol=pkl.HIGHEST_PROTOCOL)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "P7uY28cVhx1f",
        "gather": {
          "logged": 1651329651111
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Utils"
      ],
      "metadata": {
        "id": "bAydeJHEk68N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_aggregated_series(sales, sales_data_ids, agg_fn='sum'):\n",
        "    \"\"\"\n",
        "    Aggregates 30,490 level 12 series to generate data for all 42,840 series\n",
        "\n",
        "    Input data format:\n",
        "    sales: np array of shape (30490, num_timesteps)\n",
        "    sales_data_ids: np array of shape (30490, 5)\n",
        "                    with 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id' as the columns\n",
        "    agg_fn: function to be used for getting aggregated series' values ('mean' or 'sum')\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame({col: sales_data_ids[:, i] for col, i in\n",
        "                       zip(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], range(0, 5))})\n",
        "    df = pd.concat([df, pd.DataFrame(sales)], axis=1)\n",
        "    data_cols = [i for i in range(0, sales.shape[1])]\n",
        "\n",
        "    agg_indices, agg_series, agg_series_id = [], [], []\n",
        "\n",
        "    # Level 1\n",
        "    agg = np.sum(sales, 0) if agg_fn == 'sum' else np.mean(sales, 0)\n",
        "    agg_series.append(agg.reshape(1, -1))\n",
        "    agg_series_id.append(np.array(['Level1_Total_X']))\n",
        "\n",
        "    # Level 2\n",
        "    agg = df.groupby(['state_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level2_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 3\n",
        "    agg = df.groupby(['store_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level3_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 4\n",
        "    agg = df.groupby(['cat_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level4_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 5\n",
        "    agg = df.groupby(['dept_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level5_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 6\n",
        "    agg = df.groupby(['state_id', 'cat_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level6_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 7\n",
        "    agg = df.groupby(['state_id', 'dept_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level7_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 8\n",
        "    agg = df.groupby(['store_id', 'cat_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level8_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 9\n",
        "    agg = df.groupby(['store_id', 'dept_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level9_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 10\n",
        "    agg = df.groupby(['item_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append(('Level10_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 11\n",
        "    agg = df.groupby(['state_id', 'item_id'])[data_cols]\n",
        "    agg_indices.append(agg.indices)\n",
        "    agg = agg.agg(agg_fn)\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level11_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 12\n",
        "    agg = df.set_index(['item_id', 'store_id'])[data_cols]\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level12_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Get affected_hierarchy_ids - all the series affected on updating each Level 12 series\n",
        "    affected_hierarchy_ids = np.empty((30490, 12), np.int32)\n",
        "\n",
        "    # Level 1\n",
        "    affected_hierarchy_ids[:, 0] = 0\n",
        "    fill_id, fill_col = 1, 1\n",
        "    # Level 2\n",
        "    for k, v in agg_indices[0].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 3\n",
        "    for k, v in agg_indices[1].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 4\n",
        "    for k, v in agg_indices[2].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 5\n",
        "    for k, v in agg_indices[3].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 6\n",
        "    for k, v in agg_indices[4].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 7\n",
        "    for k, v in agg_indices[5].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 8\n",
        "    for k, v in agg_indices[6].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 9\n",
        "    for k, v in agg_indices[7].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 10\n",
        "    for k, v in agg_indices[8].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 11\n",
        "    for k, v in agg_indices[9].items():\n",
        "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
        "        fill_id += 1\n",
        "    fill_col += 1\n",
        "    # Level 12\n",
        "    affected_hierarchy_ids[:, fill_col] = fill_id + np.arange(0, 30490)\n",
        "\n",
        "    return np.concatenate(agg_series, axis=0), np.concatenate(agg_series_id, axis=0).\\\n",
        "        astype('<U28'), affected_hierarchy_ids\n",
        "\n",
        "\n",
        "def get_weights_all_levels(sales, sell_price, sales_data_ids):\n",
        "    \"\"\"\n",
        "    Generates weights for all 42,840 series\n",
        "\n",
        "    Input data format:\n",
        "    sales: np array of shape (30490, 28)\n",
        "    sell_price: np array of shape (30490, 28)\n",
        "\n",
        "    sales_data_ids: np array of shape (30490, 5)\n",
        "                with 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id' as the columns\n",
        "    \"\"\"\n",
        "\n",
        "    assert (sales.shape == sell_price.shape), \"Sell price and Sales arrays have different sizes\"\n",
        "    assert (sales.shape[1] == 28), \"Number of timesteps provided weight calculation is not equal to 28\"\n",
        "\n",
        "    # Get actual dollar sales for last 28 days for all 42,840 series\n",
        "    dollar_sales = sales * sell_price\n",
        "    agg_series, agg_series_id, _ = get_aggregated_series(dollar_sales, sales_data_ids)\n",
        "\n",
        "    # Sum up the actual dollar sales for all 28 timesteps\n",
        "    agg_series = agg_series.sum(1)\n",
        "\n",
        "    # Calculate total sales for each level\n",
        "    level_totals = agg_series[np.core.defchararray.find(agg_series_id, f'Level1_') == 0].sum()\n",
        "\n",
        "    # Calculate weight for each series\n",
        "    weights = agg_series / level_totals\n",
        "\n",
        "    return weights, agg_series_id\n",
        "\n",
        "\n",
        "def get_aggregated_encodings(encoded_feats, sales_data_ids):\n",
        "    \"\"\"\n",
        "    Aggregates 30,490 level 12 series to generate encoding data for all 42,840 series\n",
        "    The grouped features at each level are encoded as num_categories + 1\n",
        "\n",
        "    Input data format:\n",
        "    encoded_feats: np array of shape (30490, num_timesteps, 5)\n",
        "                    with 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id' as the columns\n",
        "    sales_data_ids: np array of shape (30490, 5)\n",
        "                    with 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id' as the columns\n",
        "    \"\"\"\n",
        "\n",
        "    num_timesteps = encoded_feats.shape[1]\n",
        "    # Note - assuming the encoded value is same for all the timesteps of a series\n",
        "    encoded_feats = encoded_feats[:, 0]\n",
        "    df = pd.DataFrame({col: sales_data_ids[:, i] for col, i in\n",
        "                       zip(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], range(0, 5))})\n",
        "    df = pd.concat([df, pd.DataFrame(encoded_feats)], axis=1)\n",
        "    data_cols = [i for i in range(0, encoded_feats.shape[1])]\n",
        "    encode_value = np.array([df[i].nunique() for i in data_cols])\n",
        "\n",
        "    agg_series, agg_series_id = [], []\n",
        "\n",
        "    # Level 1\n",
        "    agg_series.append(encode_value.reshape(1, -1))\n",
        "    agg_series_id.append(np.array(['Level1_Total_X']))\n",
        "\n",
        "    # Level 2\n",
        "    agg = df.groupby(['state_id'])[4].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, 4] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append(('Level2_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 3\n",
        "    agg = df.groupby(['store_id'])[3].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, 3] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append(('Level3_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 4\n",
        "    agg = df.groupby(['cat_id'])[2].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, 2] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append(('Level4_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 5\n",
        "    agg = df.groupby(['dept_id'])[1].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, 1] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append(('Level5_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 6\n",
        "    agg = df.groupby(['state_id', 'cat_id'])[[4, 2]].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, [4, 2]] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append('Level6_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 7\n",
        "    agg = df.groupby(['state_id', 'dept_id'])[[4, 1]].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, [4, 1]] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append('Level7_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 8\n",
        "    agg = df.groupby(['store_id', 'cat_id'])[[3, 2]].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, [3, 2]] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append('Level8_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 9\n",
        "    agg = df.groupby(['store_id', 'dept_id'])[[3, 1]].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, [3, 1]] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append('Level9_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 10\n",
        "    agg = df.groupby(['item_id'])[0].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, 0] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append(('Level10_' + agg.index.values + '_X'))\n",
        "\n",
        "    # Level 11\n",
        "    agg = df.groupby(['state_id', 'item_id'])[[4, 0]].mean()\n",
        "    encoded = np.repeat(encode_value[np.newaxis, :], agg.shape[0], axis=0)\n",
        "    encoded[:, [4, 0]] = agg.values\n",
        "    agg_series.append(encoded)\n",
        "    agg_series_id.append('Level11_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    # Level 12\n",
        "    agg = df.set_index(['item_id', 'store_id'])[data_cols]\n",
        "    agg_series.append(agg.values)\n",
        "    agg_series_id.append('Level12_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
        "\n",
        "    agg_series = np.repeat(np.concatenate(agg_series, axis=0)[:, np.newaxis, :], num_timesteps, axis=1)\n",
        "\n",
        "    return agg_series, np.concatenate(agg_series_id, axis=0).astype('<U28')\n",
        "\n",
        "\n",
        "def update_preds_acc_hierarchy(prev_preds, preds, affected_ids):\n",
        "    \"\"\"\n",
        "    prev_preds: Previously stored predictions for all 42,840 series (42840, n_timesteps)\n",
        "    preds: Current batch predictions (batch_size, n_timesteps)\n",
        "    affected_ids: the ids of all the series affected by the series in preds (30490, 12)\n",
        "    \"\"\"\n",
        "\n",
        "    # get the change in predictions for the batch series\n",
        "    change_preds = (preds - prev_preds[affected_ids[:, -1]]).repeat_interleave(12, dim=0)\n",
        "\n",
        "    affected_ids = affected_ids.flatten()\n",
        "    prev_preds = prev_preds.index_add(0, affected_ids, change_preds)\n",
        "\n",
        "    return prev_preds"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "MrD8_sdYk6sO",
        "gather": {
          "logged": 1651329654702
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader"
      ],
      "metadata": {
        "id": "Hvfw3LS5idQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset (Input Pipeline)\n",
        "class CustomDataset(data_utils.Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset\n",
        "\n",
        "    Let:\n",
        "    training period timesteps = [0, N]\n",
        "    prediction period timesteps = [N+1, N+P]\n",
        "\n",
        "    Arguments:\n",
        "    X_prev_day_sales : previous day sales for training period ([0, N])\n",
        "    X_enc_only_feats : aggregated series' previous day sales for training period ([0, N])\n",
        "    X_enc_dec_feats : sell price and categorical features for training and prediction period ([0, N+P])\n",
        "    X_calendar : calendar features for training and prediction period ([0, N+P])\n",
        "    X_last_day_sales : the actual sales for the day before the start of the prediction period (for timestep N)\n",
        "                       (this will serve as the first timestep's input for the decoder)\n",
        "    Y : actual sales, denoting targets for prediction period ([N+1, N+P])\n",
        "\n",
        "    Returns:\n",
        "    List of torch arrays:\n",
        "    x_enc: concatenated encoder features (except embedding)\n",
        "    x_enc_emb: concatenated encoder embedding features\n",
        "    x_dec: concatenated decoder features (except embedding)\n",
        "    x_dec_emb: concatenated decoder embedding features\n",
        "    x_last_day_sales: the actual sales for the day before the start of the prediction period\n",
        "    y: targets (only in training phase)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X_prev_day_sales, X_enc_only_feats, X_enc_dec_feats, X_calendar, norm_factor, norm_factor_sell_p,\n",
        "                 window_time_range, lagged_feats=None, rolling_feats=None, Y=None, rmsse_denominator=None,\n",
        "                 wrmsse_weights=None, window_id=None, config=None, is_training=True):\n",
        "\n",
        "        self.X_prev_day_sales = X_prev_day_sales\n",
        "        self.X_enc_only_feats = X_enc_only_feats\n",
        "        self.X_enc_dec_feats = X_enc_dec_feats\n",
        "        self.X_calendar = X_calendar\n",
        "        self.norm_factor = norm_factor\n",
        "        self.norm_factor_sell_p = norm_factor_sell_p\n",
        "        self.window_time_range = window_time_range\n",
        "        self.window_id = window_id\n",
        "        self.lagged_feats = lagged_feats\n",
        "        self.rolling_feats = rolling_feats\n",
        "        self.config = config\n",
        "        self.is_training = is_training\n",
        "\n",
        "        if Y is not None:\n",
        "            self.Y = torch.from_numpy(Y).float()\n",
        "            self.rmsse_denominator = torch.from_numpy(rmsse_denominator).float()\n",
        "            self.wrmsse_weights = torch.from_numpy(wrmsse_weights).float()\n",
        "        else:\n",
        "            self.Y = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.norm_factor.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.window_id is not None:\n",
        "            time_range = self.window_time_range[self.window_id[idx]]\n",
        "            scale = self.rmsse_denominator[idx - (self.window_id[idx] * 42840)]\n",
        "            weight = self.wrmsse_weights[idx - (self.window_id[idx] * 42840)]\n",
        "            ids_idx = idx - (self.window_id[idx] * 42840)\n",
        "            window_id = self.window_id[idx]\n",
        "        else:\n",
        "            time_range = self.window_time_range\n",
        "            ids_idx = idx\n",
        "            window_id = 0\n",
        "            if self.Y is not None:\n",
        "                scale = self.rmsse_denominator[idx]\n",
        "                weight = self.wrmsse_weights[idx]\n",
        "\n",
        "        # Filter data for time range of the selected window, also normalize prev_day_sales and sell_price\n",
        "        norm_factor = self.norm_factor[idx]\n",
        "        X_calendar = self.X_calendar[time_range[0]:time_range[2]]\n",
        "\n",
        "        X_prev_day_sales = self.X_prev_day_sales[time_range[0]:time_range[1], ids_idx] / norm_factor\n",
        "        X_prev_day_sales_dec = self.X_prev_day_sales[time_range[1]:time_range[2], ids_idx] / norm_factor\n",
        "        X_prev_day_sales[X_prev_day_sales < 0] = -1.0\n",
        "        X_prev_day_sales_dec[X_prev_day_sales_dec < 0] = -1.0\n",
        "\n",
        "        if self.lagged_feats is not None:\n",
        "            X_lag_feats_enc = self.lagged_feats[time_range[0]:time_range[1], ids_idx] / norm_factor\n",
        "            X_lag_feats_dec = self.lagged_feats[time_range[1]:time_range[2], ids_idx] / norm_factor\n",
        "            X_lag_feats_enc[X_lag_feats_enc < 0] = -1.0\n",
        "            X_lag_feats_dec[X_lag_feats_dec < 0] = -1.0\n",
        "            # rolling features for decoder will be calculated on the fly (by including predictions for the prev steps)\n",
        "            X_roll_feats_enc = self.rolling_feats[time_range[0]:time_range[1], ids_idx] / norm_factor\n",
        "\n",
        "        # If training and if enabled in config, multiply sales features by random noise\n",
        "        # (new value will be lower bound by 0)\n",
        "        if self.config.add_random_noise and self.is_training:\n",
        "            if len(X_prev_day_sales[X_prev_day_sales >= 0]) > 0:\n",
        "                random_noise = np.clip(np.random.normal(1, X_prev_day_sales[X_prev_day_sales >= 0].std(),\n",
        "                                                        time_range[2] - time_range[0]), 0, None)\n",
        "                noise = np.ones_like(random_noise)\n",
        "                mask = np.random.choice([0, 1], size=noise.shape, p=((1 - self.config.noise_rate),\n",
        "                                                                     self.config.noise_rate)).astype(np.bool)\n",
        "                noise[mask] = random_noise[mask]\n",
        "\n",
        "                X_prev_day_sales[X_prev_day_sales >= 0] *= noise[:time_range[1] - time_range[0]][X_prev_day_sales >= 0]\n",
        "                X_prev_day_sales_dec[X_prev_day_sales_dec >= 0] *= noise[time_range[1]\n",
        "                                                                         - time_range[2]:][X_prev_day_sales_dec >= 0]\n",
        "\n",
        "            if self.lagged_feats is not None:\n",
        "                # lagged features\n",
        "                if len(X_lag_feats_enc[X_lag_feats_enc >= 0]) > 0:\n",
        "                    random_noise = np.clip(np.random.normal(1, X_lag_feats_enc[X_lag_feats_enc >= 0].std(0),\n",
        "                                                            [time_range[2] - time_range[0],\n",
        "                                                             X_lag_feats_enc.shape[1]]), 0, None)\n",
        "                    noise = np.ones_like(random_noise)\n",
        "                    mask = np.random.choice([0, 1], size=noise.shape, p=((1 - self.config.noise_rate),\n",
        "                                                                         self.config.noise_rate)).astype(np.bool)\n",
        "                    noise[mask] = random_noise[mask]\n",
        "\n",
        "                    X_lag_feats_enc[X_lag_feats_enc >= 0] *= noise[:time_range[1] - time_range[0]][X_lag_feats_enc >= 0]\n",
        "                    X_lag_feats_dec[X_lag_feats_dec >= 0] *= noise[time_range[1]\n",
        "                                                                   - time_range[2]:][X_lag_feats_dec >= 0]\n",
        "\n",
        "                # rolling features\n",
        "                random_noise = np.clip(np.random.normal(1, X_roll_feats_enc[:, :len(self.config.rolling)].std(0),\n",
        "                                                        [time_range[1] - time_range[0],\n",
        "                                                         len(self.config.rolling)]), 0, None)\n",
        "                noise = np.ones_like(random_noise)\n",
        "                mask = np.random.choice([0, 1], size=noise.shape, p=((1 - self.config.noise_rate),\n",
        "                                                                     self.config.noise_rate)).astype(np.bool)\n",
        "                noise[mask] = random_noise[mask]\n",
        "\n",
        "                X_roll_feats_enc[:, :len(self.config.rolling)] *= noise\n",
        "                X_roll_feats_enc[:, len(self.config.rolling):] *= noise\n",
        "\n",
        "        X_enc_dec_feats = self.X_enc_dec_feats[time_range[0]:time_range[2], ids_idx]\n",
        "\n",
        "        # Directly dividing the sell price column leads to memory explosion\n",
        "        norm_factor_sell_p = np.ones_like(X_enc_dec_feats, np.float64)\n",
        "        norm_factor_sell_p[:, 0] = self.norm_factor_sell_p[idx]\n",
        "        X_enc_dec_feats = X_enc_dec_feats / norm_factor_sell_p\n",
        "\n",
        "        if self.Y is not None:\n",
        "            Y = self.Y[ids_idx, time_range[1]:time_range[2]]\n",
        "\n",
        "        enc_timesteps = time_range[1] - time_range[0]\n",
        "        dec_timesteps = time_range[2] - time_range[0] - enc_timesteps\n",
        "        num_embedding = 5\n",
        "        num_cal_embedding = 2\n",
        "\n",
        "        # input data for encoder\n",
        "        x_enc_dec_feats_enc = X_enc_dec_feats[:enc_timesteps, :-num_embedding].reshape(enc_timesteps, -1)\n",
        "\n",
        "        x_prev_day_sales_enc = X_prev_day_sales.reshape(-1, 1)\n",
        "        x_sales_feats_enc = x_prev_day_sales_enc if self.lagged_feats is None \\\n",
        "            else np.concatenate([x_prev_day_sales_enc, X_lag_feats_enc, X_roll_feats_enc], 1)\n",
        "\n",
        "        x_calendar_enc = X_calendar[:enc_timesteps, :-num_cal_embedding]\n",
        "        x_calendar_enc_emb = X_calendar[:enc_timesteps, -num_cal_embedding:].reshape(enc_timesteps, -1)\n",
        "\n",
        "        x_enc = np.concatenate([x_enc_dec_feats_enc, x_calendar_enc, x_sales_feats_enc], axis=1)\n",
        "        x_enc_emb = X_enc_dec_feats[:enc_timesteps, -num_embedding:].reshape(enc_timesteps, -1)\n",
        "\n",
        "        # input data for decoder\n",
        "        x_enc_dec_feats_dec = X_enc_dec_feats[enc_timesteps:, :-num_embedding].reshape(dec_timesteps, -1)\n",
        "        x_calendar_dec = X_calendar[enc_timesteps:, :-num_cal_embedding]\n",
        "        x_calendar_dec_emb = X_calendar[enc_timesteps:, -num_cal_embedding:].reshape(dec_timesteps, -1)\n",
        "\n",
        "        x_prev_day_sales_dec = X_prev_day_sales_dec.reshape(-1, 1)\n",
        "        x_sales_feats_dec = x_prev_day_sales_dec if self.lagged_feats is None \\\n",
        "            else np.concatenate([x_prev_day_sales_dec, X_lag_feats_dec], 1)\n",
        "\n",
        "        x_dec = np.concatenate([x_enc_dec_feats_dec, x_calendar_dec], axis=1)\n",
        "        x_dec_emb = X_enc_dec_feats[enc_timesteps:, -num_embedding:].reshape(dec_timesteps, -1)\n",
        "\n",
        "        if self.Y is None:\n",
        "            return [[torch.from_numpy(x_enc).float(), torch.from_numpy(x_enc_emb).long(),\n",
        "                     torch.from_numpy(x_calendar_enc_emb).long(),\n",
        "                     torch.from_numpy(x_dec).float(), torch.from_numpy(x_dec_emb).long(),\n",
        "                     torch.from_numpy(x_calendar_dec_emb).long(),\n",
        "                     torch.from_numpy(x_sales_feats_dec).float()], norm_factor]\n",
        "\n",
        "        return [[torch.from_numpy(x_enc).float(), torch.from_numpy(x_enc_emb).long(),\n",
        "                 torch.from_numpy(x_calendar_enc_emb).long(),\n",
        "                 torch.from_numpy(x_dec).float(), torch.from_numpy(x_dec_emb).long(),\n",
        "                 torch.from_numpy(x_calendar_dec_emb).long(),\n",
        "                 torch.from_numpy(x_sales_feats_dec).float()],\n",
        "                Y, torch.from_numpy(np.array(norm_factor)).float(),\n",
        "                ids_idx,\n",
        "                [scale, weight],\n",
        "                window_id]\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        # load data\n",
        "        with open(f'{self.config.data_file}', 'rb') as f:\n",
        "            data_dict = pkl.load(f)\n",
        "\n",
        "        self.ids = data_dict['sales_data_ids']\n",
        "        self.enc_dec_feat_names = data_dict['enc_dec_feat_names']\n",
        "        self.sell_price_i = self.enc_dec_feat_names.index('sell_price')\n",
        "\n",
        "        self.X_prev_day_sales, self.agg_ids, _ = get_aggregated_series(data_dict['X_prev_day_sales'].T, self.ids)\n",
        "        self.X_prev_day_sales = self.X_prev_day_sales.T\n",
        "\n",
        "        self.X_enc_dec_feats = data_dict['X_enc_dec_feats']\n",
        "        self.sell_price_l12 = self.X_enc_dec_feats[:, :, self.sell_price_i]\n",
        "        sell_price_all, _, _ = get_aggregated_series(self.X_enc_dec_feats[:, :, self.sell_price_i].T, self.ids, 'mean')\n",
        "        encodings_all, _ = get_aggregated_encodings(self.X_enc_dec_feats[:, :, 1:].transpose(1, 0, 2), self.ids)\n",
        "        self.X_enc_dec_feats = np.concatenate([sell_price_all[:, :, np.newaxis], encodings_all], axis=2) \\\n",
        "            .transpose(1, 0, 2)\n",
        "\n",
        "        self.Y_l12 = data_dict['Y']\n",
        "        self.Y, _, _ = get_aggregated_series(data_dict['Y'], self.ids)\n",
        "        self.Y = self.Y\n",
        "\n",
        "        # for prev_day_sales, set value as -1 for the period the product was not actively sold\n",
        "        self.X_prev_day_sales_unsold_negative = self.X_prev_day_sales.copy()\n",
        "        for idx, first_non_zero_idx in enumerate((self.X_prev_day_sales != 0).argmax(axis=0)):\n",
        "            self.X_prev_day_sales_unsold_negative[:first_non_zero_idx, idx] = -1\n",
        "\n",
        "        self.X_enc_only_feats = data_dict['X_enc_only_feats']\n",
        "        self.X_calendar = data_dict['X_calendar']\n",
        "        self.n_windows = 1\n",
        "\n",
        "    def create_train_loader(self, data_start_t=None, horizon_start_t=None, horizon_end_t=None):\n",
        "        if (data_start_t is None) | (horizon_start_t is None) | (horizon_end_t is None):\n",
        "            data_start_t = self.config.training_ts['data_start_t']\n",
        "            horizon_start_t = self.config.training_ts['horizon_start_t']\n",
        "            horizon_end_t = self.config.training_ts['horizon_end_t']\n",
        "\n",
        "        # Run a sliding window of length \"window_length\" and train for the next month of each window\n",
        "        if self.config.sliding_window:\n",
        "            window_length = self.config.window_length\n",
        "            window_time_range, norm_factor, norm_factor_sell_p = [], [], []\n",
        "            weights, scales = [], []\n",
        "\n",
        "            for idx, i in enumerate(range(data_start_t + window_length, horizon_end_t, 28)):\n",
        "                w_data_start_t, w_horizon_start_t = data_start_t + (idx * 28), i\n",
        "                w_horizon_end_t = w_horizon_start_t + 28\n",
        "                window_time_range.append([w_data_start_t - data_start_t, w_horizon_start_t - data_start_t,\n",
        "                                          w_horizon_end_t - data_start_t])\n",
        "\n",
        "                # calculate denominator for rmsse loss\n",
        "                absolute_movement = np.absolute(self.Y.T[:w_horizon_start_t] -\n",
        "                                                self.X_prev_day_sales[:w_horizon_start_t]).astype(np.int64)\n",
        "                actively_sold_in_range = (self.X_prev_day_sales[:w_horizon_start_t] != 0).argmax(axis=0)\n",
        "                rmsse_den = []\n",
        "                for idx_active_sell, first_active_sell_idx in enumerate(actively_sold_in_range):\n",
        "                    den = absolute_movement[first_active_sell_idx:, idx_active_sell].mean()\n",
        "                    den = den if den != 0 else 1\n",
        "                    rmsse_den.append(den)\n",
        "                scales.append(np.array(rmsse_den))\n",
        "\n",
        "                # Get weights for WRMSSE and SPL loss\n",
        "                w_weights, _ = get_weights_all_levels(self.Y_l12[:, w_horizon_start_t - 28:w_horizon_start_t],\n",
        "                                                      self.sell_price_l12[w_horizon_start_t - 28:w_horizon_start_t,\n",
        "                                                      :].T,\n",
        "                                                      self.ids)\n",
        "                weights.append(w_weights)\n",
        "\n",
        "                # Normalize sale features by dividing by mean of each series (as per the selected input window)\n",
        "                w_X_prev_day_sales_calc = self.X_prev_day_sales[w_data_start_t:w_horizon_start_t]\n",
        "                w_norm_factor = np.mean(w_X_prev_day_sales_calc, 0)\n",
        "                w_norm_factor[w_norm_factor == 0] = 1.\n",
        "\n",
        "                w_X_sell_p = self.X_enc_dec_feats[w_data_start_t:w_horizon_start_t, :, self.sell_price_i].copy().astype(\n",
        "                    float)\n",
        "                w_norm_factor_sell_p = np.median(w_X_sell_p, 0)\n",
        "                w_norm_factor_sell_p[w_norm_factor_sell_p == 0] = 1.\n",
        "                norm_factor.append(w_norm_factor)\n",
        "                norm_factor_sell_p.append(w_norm_factor_sell_p)\n",
        "\n",
        "            self.n_windows = idx + 1\n",
        "            scales = np.concatenate(scales, 0)\n",
        "            weights = np.concatenate(weights, 0)\n",
        "            norm_factor = np.concatenate(norm_factor, 0)\n",
        "            norm_factor_sell_p = np.concatenate(norm_factor_sell_p, 0)\n",
        "            window_time_range = np.array(window_time_range)\n",
        "            window_id = np.arange(idx + 1).repeat(self.X_enc_dec_feats.shape[1])\n",
        "\n",
        "        else:\n",
        "            # calculate denominator for rmsse loss\n",
        "            absolute_movement = np.absolute(self.Y.T[:horizon_start_t] -\n",
        "                                            self.X_prev_day_sales[:horizon_start_t]).astype(np.int64)\n",
        "            actively_sold_in_range = (self.X_prev_day_sales[:horizon_start_t] != 0).argmax(axis=0)\n",
        "            rmsse_den = []\n",
        "            for idx, first_active_sell_idx in enumerate(actively_sold_in_range):\n",
        "                den = absolute_movement[first_active_sell_idx:, idx].mean()\n",
        "                den = den if den != 0 else 1\n",
        "                rmsse_den.append(den)\n",
        "\n",
        "            # Get weights for WRMSSE and SPL loss\n",
        "            weights, _ = get_weights_all_levels(self.Y_l12[:, horizon_start_t - 28:horizon_start_t],\n",
        "                                                self.sell_price_l12[horizon_start_t - 28:horizon_start_t, :].T,\n",
        "                                                self.ids)\n",
        "\n",
        "            # Normalize sale features by dividing by mean of each series (as per the selected input window)\n",
        "            X_prev_day_sales_calc = self.X_prev_day_sales[data_start_t:horizon_start_t]\n",
        "            norm_factor = np.mean(X_prev_day_sales_calc, 0)\n",
        "            norm_factor[norm_factor == 0] = 1.\n",
        "\n",
        "            X_sell_p = self.X_enc_dec_feats[data_start_t:horizon_start_t, :, self.sell_price_i].copy().astype(float)\n",
        "            norm_factor_sell_p = np.median(X_sell_p, 0)\n",
        "            norm_factor_sell_p[norm_factor_sell_p == 0] = 1.\n",
        "\n",
        "            window_time_range = np.array([0, horizon_start_t - data_start_t, horizon_end_t - data_start_t])\n",
        "            scales = np.array(rmsse_den)\n",
        "            window_id = None\n",
        "\n",
        "        # Add rolling and lag features\n",
        "        if self.config.lag_and_roll_feats:\n",
        "            max_prev_ts_req = max(self.config.lags + self.config.rolling)\n",
        "            lagged_feats = []\n",
        "            for lag_i in np.array(sorted(self.config.lags, reverse=True)):\n",
        "                lag_i_feat = np.roll(self.X_prev_day_sales_unsold_negative[data_start_t - max_prev_ts_req:]\n",
        "                                     .astype(np.int32), lag_i, axis=0)\n",
        "                lag_i_feat[:lag_i] = 0\n",
        "                lagged_feats.append(lag_i_feat)\n",
        "            lagged_feats = np.stack(lagged_feats, axis=2)[max_prev_ts_req:]\n",
        "\n",
        "            rolling_feats, roll_i_means, roll_i_stds = [], [], []\n",
        "            roll_df = pd.DataFrame(self.X_prev_day_sales[data_start_t - max_prev_ts_req:].astype(np.int32))\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_mean = pd.DataFrame(roll_df).rolling(roll_i, axis=0).mean().fillna(0).values\n",
        "                roll_i_means.append(roll_i_feat_mean)\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_std = pd.DataFrame(roll_df).rolling(roll_i, axis=0).std().fillna(0).values\n",
        "                roll_i_stds.append(roll_i_feat_std)\n",
        "            rolling_feats = np.stack(roll_i_means + roll_i_stds, 2)[max_prev_ts_req:]\n",
        "        else:\n",
        "            lagged_feats, rolling_feats = None, None\n",
        "\n",
        "        dataset = CustomDataset(self.X_prev_day_sales_unsold_negative[data_start_t:],\n",
        "                                self.X_enc_only_feats[data_start_t:],\n",
        "                                self.X_enc_dec_feats[data_start_t:],\n",
        "                                self.X_calendar[data_start_t:],\n",
        "                                norm_factor, norm_factor_sell_p, window_time_range,\n",
        "                                lagged_feats, rolling_feats,\n",
        "                                Y=self.Y[:, data_start_t:],\n",
        "                                rmsse_denominator=scales, wrmsse_weights=weights, window_id=window_id,\n",
        "                                config=self.config)\n",
        "\n",
        "        return torch.utils.data.DataLoader(dataset=dataset, batch_size=self.config.batch_size, shuffle=True,\n",
        "                                           num_workers=3, pin_memory=True)\n",
        "\n",
        "    def create_val_loader(self, data_start_t=None, horizon_start_t=None, horizon_end_t=None):\n",
        "        if (data_start_t is None) | (horizon_start_t is None) | (horizon_end_t is None):\n",
        "            data_start_t = self.config.validation_ts['data_start_t']\n",
        "            horizon_start_t = self.config.validation_ts['horizon_start_t']\n",
        "            horizon_end_t = self.config.validation_ts['horizon_end_t']\n",
        "\n",
        "        # calculate denominator for rmsse loss\n",
        "        absolute_movement = np.absolute(self.Y.T[:horizon_start_t] -\n",
        "                                        self.X_prev_day_sales[:horizon_start_t]).astype(np.int64)\n",
        "        actively_sold_in_range = (self.X_prev_day_sales[:horizon_start_t] != 0).argmax(axis=0)\n",
        "        rmsse_den = []\n",
        "        for idx, first_active_sell_idx in enumerate(actively_sold_in_range):\n",
        "            den = absolute_movement[first_active_sell_idx:, idx].mean()\n",
        "            den = den if den != 0 else 1\n",
        "            rmsse_den.append(den)\n",
        "\n",
        "        # Get weights for WRMSSE and SPL loss\n",
        "        weights, _ = get_weights_all_levels(self.Y_l12[:, horizon_start_t - 28:horizon_start_t],\n",
        "                                            self.sell_price_l12[horizon_start_t - 28:horizon_start_t, :].T,\n",
        "                                            self.ids)\n",
        "\n",
        "        # Normalize sale features by dividing by mean of each series (as per the selected input window)\n",
        "        X_prev_day_sales_calc = self.X_prev_day_sales[data_start_t:horizon_start_t]\n",
        "        norm_factor = np.mean(X_prev_day_sales_calc, 0)\n",
        "        norm_factor[norm_factor == 0] = 1.\n",
        "\n",
        "        X_sell_p = self.X_enc_dec_feats[data_start_t:horizon_start_t, :, self.sell_price_i].copy().astype(float)\n",
        "        norm_factor_sell_p = np.median(X_sell_p, 0)\n",
        "        norm_factor_sell_p[norm_factor_sell_p == 0] = 1.\n",
        "\n",
        "        window_time_range = [0, horizon_start_t - data_start_t, horizon_end_t - data_start_t]\n",
        "\n",
        "        # Add rolling and lag features\n",
        "        if self.config.lag_and_roll_feats:\n",
        "            max_prev_ts_req = max(self.config.lags + self.config.rolling)\n",
        "            lagged_feats = []\n",
        "            for lag_i in np.array(sorted(self.config.lags, reverse=True)):\n",
        "                lag_i_feat = np.roll(self.X_prev_day_sales_unsold_negative[data_start_t - max_prev_ts_req:]\n",
        "                                     .astype(np.int32), lag_i, axis=0)\n",
        "                lag_i_feat[:lag_i] = 0\n",
        "                lagged_feats.append(lag_i_feat)\n",
        "            lagged_feats = np.stack(lagged_feats, axis=2)[max_prev_ts_req:]\n",
        "\n",
        "            rolling_feats, roll_i_means, roll_i_stds = [], [], []\n",
        "            roll_df = pd.DataFrame(self.X_prev_day_sales[data_start_t - max_prev_ts_req:].astype(np.int32))\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_mean = pd.DataFrame(roll_df).rolling(roll_i, axis=0).mean().fillna(0).values\n",
        "                roll_i_means.append(roll_i_feat_mean)\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_std = pd.DataFrame(roll_df).rolling(roll_i, axis=0).std().fillna(0).values\n",
        "                roll_i_stds.append(roll_i_feat_std)\n",
        "            rolling_feats = np.stack(roll_i_means + roll_i_stds, 2)[max_prev_ts_req:]\n",
        "        else:\n",
        "            lagged_feats, rolling_feats = None, None\n",
        "\n",
        "        dataset = CustomDataset(self.X_prev_day_sales_unsold_negative[data_start_t:],\n",
        "                                self.X_enc_only_feats[data_start_t:],\n",
        "                                self.X_enc_dec_feats[data_start_t:],\n",
        "                                self.X_calendar[data_start_t:],\n",
        "                                norm_factor, norm_factor_sell_p, window_time_range,\n",
        "                                lagged_feats, rolling_feats,\n",
        "                                Y=self.Y[:, data_start_t:],\n",
        "                                rmsse_denominator=np.array(rmsse_den), wrmsse_weights=weights,\n",
        "                                config=self.config, is_training=False)\n",
        "\n",
        "        return torch.utils.data.DataLoader(dataset=dataset, batch_size=self.config.batch_size, num_workers=3,\n",
        "                                           pin_memory=True)\n",
        "\n",
        "    def create_test_loader(self, data_start_t=None, horizon_start_t=None, horizon_end_t=None):\n",
        "        if (data_start_t is None) | (horizon_start_t is None) | (horizon_end_t is None):\n",
        "            data_start_t = self.config.test_ts['data_start_t']\n",
        "            horizon_start_t = self.config.test_ts['horizon_start_t']\n",
        "            horizon_end_t = self.config.test_ts['horizon_end_t']\n",
        "\n",
        "        # Normalize sale features by dividing by mean of each series (as per the selected input window)\n",
        "        X_prev_day_sales_calc = self.X_prev_day_sales[data_start_t:horizon_start_t]\n",
        "        norm_factor = np.mean(X_prev_day_sales_calc, 0)\n",
        "        norm_factor[norm_factor == 0] = 1.\n",
        "\n",
        "        X_sell_p = self.X_enc_dec_feats[data_start_t:horizon_start_t, :, self.sell_price_i].copy().astype(float)\n",
        "        norm_factor_sell_p = np.median(X_sell_p, 0)\n",
        "        norm_factor_sell_p[norm_factor_sell_p == 0] = 1.\n",
        "\n",
        "        window_time_range = [0, horizon_start_t - data_start_t, horizon_end_t - data_start_t]\n",
        "\n",
        "        # Add rolling and lag features\n",
        "        if self.config.lag_and_roll_feats:\n",
        "            max_prev_ts_req = max(self.config.lags + self.config.rolling)\n",
        "            lagged_feats = []\n",
        "            for lag_i in np.array(sorted(self.config.lags, reverse=True)):\n",
        "                lag_i_feat = np.roll(self.X_prev_day_sales_unsold_negative[data_start_t - max_prev_ts_req:]\n",
        "                                     .astype(np.int32), lag_i, axis=0)\n",
        "                lag_i_feat[:lag_i] = 0\n",
        "                lagged_feats.append(lag_i_feat)\n",
        "            lagged_feats = np.stack(lagged_feats, axis=2)[max_prev_ts_req:]\n",
        "\n",
        "            rolling_feats, roll_i_means, roll_i_stds = [], [], []\n",
        "            roll_df = pd.DataFrame(self.X_prev_day_sales[data_start_t - max_prev_ts_req:].astype(np.int32))\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_mean = pd.DataFrame(roll_df).rolling(roll_i, axis=0).mean().fillna(0).values\n",
        "                roll_i_means.append(roll_i_feat_mean)\n",
        "            for roll_i in self.config.rolling:\n",
        "                roll_i_feat_std = pd.DataFrame(roll_df).rolling(roll_i, axis=0).std().fillna(0).values\n",
        "                roll_i_stds.append(roll_i_feat_std)\n",
        "            rolling_feats = np.stack(roll_i_means + roll_i_stds, 2)[max_prev_ts_req:]\n",
        "        else:\n",
        "            lagged_feats, rolling_feats = None, None\n",
        "\n",
        "        dataset = CustomDataset(self.X_prev_day_sales_unsold_negative[data_start_t:],\n",
        "                                self.X_enc_only_feats[data_start_t:],\n",
        "                                self.X_enc_dec_feats[data_start_t:],\n",
        "                                self.X_calendar[data_start_t:],\n",
        "                                norm_factor, norm_factor_sell_p, window_time_range,\n",
        "                                lagged_feats, rolling_feats, config=self.config, is_training=False)\n",
        "\n",
        "        return torch.utils.data.DataLoader(dataset=dataset, batch_size=self.config.batch_size, num_workers=3,\n",
        "                                           pin_memory=True)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "3ZhSHVNMifjm",
        "gather": {
          "logged": 1651329654947
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "Vh7m2gq-lSKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "\n",
        "    resume_training = False\n",
        "    resume_from_fold = 1  # In case of k-fold training [1, k]\n",
        "\n",
        "    loss_fn = 'SPLLoss'\n",
        "    metric = 'SPLMetric'\n",
        "    secondary_metric = 'WRMSSEMetric'\n",
        "    architecture = 'seq2seq_w_attn_on_hid'\n",
        "\n",
        "    # Running a sliding window training will help increase the training data\n",
        "    sliding_window = True  # Note: sliding window has not been tested with WRMSSELoss\n",
        "    window_length = 28 * 13\n",
        "\n",
        "    lag_and_roll_feats = True  # Note: Currently only works with dilated_seq2seq & seq2seq_w_attn_on_hid architectures\n",
        "    lags = list(range(27, 42))\n",
        "    rolling = [7, 14, 30, 60, 180]\n",
        "\n",
        "    # Regularization\n",
        "    add_random_noise = True\n",
        "    noise_rate = 0.5\n",
        "\n",
        "    # *** RNN *** #\n",
        "    # hidden dimension and no. of layers will be the same for both encoder and decoder\n",
        "    rnn_num_hidden = 128\n",
        "    rnn_num_layers = 2\n",
        "    bidirectional = True\n",
        "    enc_rnn_dropout = 0.2\n",
        "    dec_rnn_dropout = 0.0\n",
        "    teacher_forcing_ratio = 0.0\n",
        "\n",
        "    # *** Transformer *** #\n",
        "    enc_nhead = 4\n",
        "    enc_nlayers = 2\n",
        "    enc_dropout = 0.1\n",
        "    dec_nhead = 4\n",
        "    dec_nlayers = 2\n",
        "    dec_dropout = 0.1\n",
        "\n",
        "    # num_epochs = 200\n",
        "    num_epochs = 2\n",
        "    batch_size = 160\n",
        "    learning_rate = 0.0003\n",
        "\n",
        "    # training, validation and test periods\n",
        "    training_ts = {'data_start_t': 1969 - 1 - (28 * 29), 'horizon_start_t': 1969 - 1 - (28 * 3),\n",
        "                   'horizon_end_t': 1969 - 1 - (28 * 2)}\n",
        "    validation_ts = {'data_start_t': 1969 - 1 - (28 * 15), 'horizon_start_t': 1969 - 1 - (28 * 2),\n",
        "                     'horizon_end_t': 1969 - 1 - (28 * 1)}\n",
        "    test_ts = {'data_start_t': 1969 - 1 - (28 * 14), 'horizon_start_t': 1969 - 1 - (28 * 1),\n",
        "               'horizon_end_t': 1969 - 1 - (28 * 0)}\n",
        "\n",
        "    # Parameters for k-fold training\n",
        "    k_fold = True\n",
        "    k_fold_splits = [(f_train_ts, f_val_ts) for f_train_ts, f_val_ts in\n",
        "                     zip([\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 31), 'horizon_start_t': 1969 - 1 - (28 * 5),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 4)},\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 30), 'horizon_start_t': 1969 - 1 - (28 * 4),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 3)},\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 29), 'horizon_start_t': 1969 - 1 - (28 * 3),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 2)}\n",
        "                     ], [\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 17), 'horizon_start_t': 1969 - 1 - (28 * 4),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 3)},\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 16), 'horizon_start_t': 1969 - 1 - (28 * 3),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 2)},\n",
        "                         {'data_start_t': 1969 - 1 - (28 * 15), 'horizon_start_t': 1969 - 1 - (28 * 2),\n",
        "                          'horizon_end_t': 1969 - 1 - (28 * 1)}\n",
        "                     ])]\n",
        "\n",
        "    data_file = './data.pickle'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "yiCI9ddclSAi",
        "gather": {
          "logged": 1651329655243
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Utils"
      ],
      "metadata": {
        "id": "KLH8JYvslJgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ModelCheckpoint:\n",
        "    def __init__(self, weight_dir='./weights', config=Config):\n",
        "        self.weight_dir = weight_dir\n",
        "        self.config = config\n",
        "        file_prefix = '' if config.fold is None else f'fold_{config.fold}_'\n",
        "        self.filename = os.path.join(self.weight_dir, file_prefix + 'model_latest_checkpoint.pth.tar')\n",
        "        self.best_filename = os.path.join(self.weight_dir, file_prefix + 'model_best.pth.tar')\n",
        "\n",
        "    def save(self, is_best, min_val_error, num_bad_epochs, epoch, model, optimizer, scheduler=None):\n",
        "        scheduler_save = scheduler if scheduler is None else scheduler.state_dict()\n",
        "        save_dict = {\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'min_val_error': min_val_error,\n",
        "            'num_bad_epochs': num_bad_epochs,\n",
        "            'scheduler': scheduler_save\n",
        "        }\n",
        "        torch.save(save_dict, self.filename)\n",
        "        if is_best:\n",
        "            shutil.copyfile(self.filename, self.best_filename)\n",
        "\n",
        "    def load(self, model, optimizer=None, scheduler=None, load_best=False):\n",
        "        load_filename = self.best_filename if load_best else self.filename\n",
        "        if os.path.isfile(load_filename):\n",
        "            checkpoint = torch.load(load_filename, map_location=self.config.device)\n",
        "            model.load_state_dict(checkpoint['model'])\n",
        "            if optimizer is not None:\n",
        "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            if scheduler is not None:\n",
        "                scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "            min_val_error = checkpoint['min_val_error']\n",
        "            num_bad_epochs = checkpoint['num_bad_epochs']\n",
        "        else:\n",
        "            raise FileNotFoundError(f'No checkpoint found at {load_filename}')\n",
        "\n",
        "        return model, optimizer, scheduler, [start_epoch, min_val_error, num_bad_epochs]\n",
        "\n",
        "\n",
        "class EarlyStopping(object):\n",
        "    \"\"\"\n",
        "    author:https://github.com/stefanonardo\n",
        "    source: https://gist.github.com/stefanonardo/693d96ceb2f531fa05db530f3e21517d\n",
        "    \"\"\"\n",
        "    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.best = None\n",
        "        self.num_bad_epochs = 0\n",
        "        self.is_better = None\n",
        "        self._init_is_better(mode, min_delta, percentage)\n",
        "\n",
        "        if patience == 0:\n",
        "            self.is_better = lambda a, b: True\n",
        "            self.step = lambda a: False\n",
        "\n",
        "    def step(self, metrics):\n",
        "        if self.best is None:\n",
        "            self.best = metrics\n",
        "            return False\n",
        "\n",
        "        if np.isnan(metrics):\n",
        "            return True\n",
        "\n",
        "        if self.is_better(metrics, self.best):\n",
        "            self.num_bad_epochs = 0\n",
        "            self.best = metrics\n",
        "        else:\n",
        "            self.num_bad_epochs += 1\n",
        "\n",
        "        if self.num_bad_epochs >= self.patience:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _init_is_better(self, mode, min_delta, percentage):\n",
        "        if mode not in {'min', 'max'}:\n",
        "            raise ValueError('mode ' + mode + ' is unknown!')\n",
        "        if not percentage:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - min_delta\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + min_delta\n",
        "        else:\n",
        "            if mode == 'min':\n",
        "                self.is_better = lambda a, best: a < best - (\n",
        "                            best * min_delta / 100)\n",
        "            if mode == 'max':\n",
        "                self.is_better = lambda a, best: a > best + (\n",
        "                            best * min_delta / 100)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "VHHijC3Pifh9",
        "gather": {
          "logged": 1651329655405
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rJOpkUvDm-12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WorkFlow"
      ],
      "metadata": {
        "id": "L5hlzj6Ujsxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Data"
      ],
      "metadata": {
        "id": "vj0Sd7ItjvZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Processing Data:\\n')\n",
        "read_data()\n",
        "print('\\nCompleted')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Processing Data:\n\n* Processing calendar features\n* Encoding categorical features\n* Add previous day sales and merge sell prices\n* Save processed data\n\nCompleted\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDNxQoQ5ifd-",
        "outputId": "2271a01f-58a4-414b-942b-250a48f0f2fd",
        "gather": {
          "logged": 1651329842138
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Data"
      ],
      "metadata": {
        "id": "JohU_-Jzo8wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from importlib import import_module\n",
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# from data_loader.data_generator import DataLoader\n",
        "# from utils.data_utils import *\n",
        "# from utils.training_utils import ModelCheckpoint, EarlyStopping\n",
        "from losses_and_metrics import loss_functions, metrics\n",
        "# from config import Config\n",
        "\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.terminal_width = shutil.get_terminal_size((80, 20)).columns\n",
        "\n",
        "        # Model\n",
        "        print(f' Model: {self.config.architecture} '.center(self.terminal_width, '*'))\n",
        "        model_type = import_module('models.' + self.config.architecture)\n",
        "        create_model = getattr(model_type, 'create_model')\n",
        "        self.model = create_model(self.config)\n",
        "        print(self.model, end='\\n\\n')\n",
        "\n",
        "        # Loss, Optimizer and LRScheduler\n",
        "        self.criterion = getattr(loss_functions, self.config.loss_fn)(self.config)\n",
        "        self.optimizer = torch.optim.RMSprop(self.model.parameters(), lr=self.config.learning_rate, alpha=0.95)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5,\n",
        "                                                                    patience=4, verbose=True)\n",
        "        self.early_stopping = EarlyStopping(patience=10)\n",
        "        self.agg_sum = self.config.loss_fn[:3] == 'SPL'\n",
        "        self.loss_agg = np.sum if self.agg_sum else np.mean\n",
        "\n",
        "        # Metric\n",
        "        self.metric = getattr(metrics, config.metric)()\n",
        "        self.metric_2 = getattr(metrics, config.secondary_metric)()\n",
        "\n",
        "        print(f' Loading Data '.center(self.terminal_width, '*'))\n",
        "        data_loader = DataLoader(self.config)\n",
        "        self.ids = data_loader.ids\n",
        "\n",
        "        self.train_loader = data_loader.create_train_loader()\n",
        "        self.val_loader = data_loader.create_val_loader()\n",
        "        self.n_windows = data_loader.n_windows\n",
        "\n",
        "        self.start_epoch, self.min_val_error = 1, None\n",
        "        # Load checkpoint if training is to be resumed\n",
        "        self.model_checkpoint = ModelCheckpoint(config=self.config)\n",
        "        if config.resume_training:\n",
        "            self.model, self.optimizer, self.scheduler, [self.start_epoch, self.min_val_error, num_bad_epochs] = \\\n",
        "                self.model_checkpoint.load(self.model, self.optimizer, self.scheduler)\n",
        "            self.early_stopping.best = self.min_val_error\n",
        "            self.early_stopping.num_bad_epochs = num_bad_epochs\n",
        "            print(f'Resuming model training from epoch {self.start_epoch}')\n",
        "        else:\n",
        "            # remove previous logs, if any\n",
        "            if self.config.fold is None:\n",
        "                logs = glob.glob('./logs/.*') + glob.glob('./logs/*')\n",
        "                for f in logs:\n",
        "                    try:\n",
        "                        os.remove(f)\n",
        "                    except IsADirectoryError:\n",
        "                        shutil.rmtree(f)\n",
        "            else:\n",
        "                logs = glob.glob(f'./logs/fold_{self.config.fold}/.*') + glob.glob(f'./logs/fold_{self.config.fold}/*')\n",
        "                for f in logs:\n",
        "                    os.remove(f)\n",
        "\n",
        "        # logging\n",
        "        self.writer = SummaryWriter(f'logs') if self.config.fold is None \\\n",
        "            else SummaryWriter(f'logs/fold_{self.config.fold}')\n",
        "\n",
        "    def _get_val_loss_and_err(self):\n",
        "        self.model.eval()\n",
        "        progbar = tqdm(self.val_loader)\n",
        "        progbar.set_description(\"             \")\n",
        "        losses, epoch_preds, epoch_ys, epoch_ws, epoch_scales = [], [], [], [], []\n",
        "        for i, [x, y, norm_factor, ids_idx, loss_input, _] in enumerate(progbar):\n",
        "            epoch_ys.append(y.data.numpy())\n",
        "            epoch_scales.append(loss_input[0].data.numpy())\n",
        "            epoch_ws.append(loss_input[1].data.numpy())\n",
        "\n",
        "            x = [inp.to(self.config.device) for inp in x]\n",
        "            y = y.to(self.config.device)\n",
        "            norm_factor = norm_factor.to(self.config.device)\n",
        "            loss_input = [inp.to(self.config.device) for inp in loss_input]\n",
        "\n",
        "            preds = self.model(*x) * norm_factor[:, None, None]\n",
        "            epoch_preds.append(preds.data.cpu().numpy())\n",
        "            loss = self.criterion(preds, y, *loss_input)\n",
        "            losses.append(loss.data.cpu().numpy())\n",
        "\n",
        "        epoch_preds, epoch_ys = np.concatenate(epoch_preds, axis=0), np.concatenate(epoch_ys, axis=0)\n",
        "        epoch_ws, epoch_scales = np.concatenate(epoch_ws, axis=0), np.concatenate(epoch_scales, axis=0)\n",
        "\n",
        "        val_error = self.metric.get_error(epoch_preds, epoch_ys, epoch_scales, epoch_ws)\n",
        "        val_error_2 = self.metric_2.get_error(epoch_preds[:, :, 4], epoch_ys, epoch_scales, epoch_ws)\n",
        "\n",
        "        return self.loss_agg(losses), val_error, val_error_2\n",
        "\n",
        "    def train(self):\n",
        "        print(f' Training '.center(self.terminal_width, '*'), end='\\n\\n')\n",
        "\n",
        "        for epoch in range(self.start_epoch, self.config.num_epochs + 1):\n",
        "            print(f' Epoch [{epoch}/{self.config.num_epochs}] '.center(self.terminal_width, 'x'))\n",
        "            self.model.train()\n",
        "            progbar = tqdm(self.train_loader)\n",
        "            losses, epoch_preds, epoch_ys, epoch_ws, epoch_scales = [], [], [], [], []\n",
        "            for i, [x, y, norm_factor, ids_idx, loss_input, window_id] in enumerate(progbar):\n",
        "                x = [inp.to(self.config.device) for inp in x]\n",
        "                y = y.to(self.config.device)\n",
        "                norm_factor = norm_factor.to(self.config.device)\n",
        "                loss_input = [inp.to(self.config.device) for inp in loss_input]\n",
        "\n",
        "                # Forward + Backward + Optimize\n",
        "                self.optimizer.zero_grad()\n",
        "                preds = self.model(*x) * norm_factor[:, None, None]\n",
        "\n",
        "                if self.config.sliding_window:\n",
        "                    if torch.sum(window_id == self.n_windows - 1) > 0:\n",
        "                        epoch_ys.append(y[window_id == self.n_windows - 1].data.cpu().numpy().reshape(-1, 28))\n",
        "                        epoch_scales.append(loss_input[0][window_id == self.n_windows - 1]\n",
        "                                            .data.cpu().numpy().reshape(-1))\n",
        "                        epoch_ws.append(loss_input[1][window_id == self.n_windows - 1]\n",
        "                                        .data.cpu().numpy().reshape(-1))\n",
        "                        epoch_preds.append(preds[window_id == self.n_windows - 1].data.cpu().numpy().reshape(-1, 28, 9))\n",
        "                else:\n",
        "                    epoch_ys.append(y.data.cpu().numpy())\n",
        "                    epoch_scales.append(loss_input[0].data.cpu().numpy())\n",
        "                    epoch_ws.append(loss_input[1].data.cpu().numpy())\n",
        "                    epoch_preds.append(preds.data.cpu().cpu().numpy())\n",
        "\n",
        "                loss = self.criterion(preds, y, *loss_input)\n",
        "                losses.append(loss.data.cpu().numpy())\n",
        "\n",
        "                if self.agg_sum:\n",
        "                    progbar.set_description(\"loss = %0.3f \" % np.round(\n",
        "                        (len(self.train_loader) / (i + 1)) * self.loss_agg(losses) / self.n_windows, 3))\n",
        "                else:\n",
        "                    progbar.set_description(\"loss = %0.3f \" % np.round(self.loss_agg(losses), 3))\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # Get training and validation loss and error\n",
        "            epoch_preds, epoch_ys = np.concatenate(epoch_preds, axis=0), np.concatenate(epoch_ys, axis=0)\n",
        "            epoch_ws, epoch_scales = np.concatenate(epoch_ws, axis=0), np.concatenate(epoch_scales, axis=0)\n",
        "\n",
        "            if self.agg_sum:\n",
        "                train_loss = self.loss_agg(losses) / self.n_windows\n",
        "            else:\n",
        "                train_loss = self.loss_agg(losses)\n",
        "\n",
        "            train_error = self.metric.get_error(epoch_preds, epoch_ys, epoch_scales, epoch_ws)\n",
        "            train_error_2 = self.metric_2.get_error(epoch_preds[:, :, 4], epoch_ys, epoch_scales, epoch_ws)\n",
        "\n",
        "            val_loss, val_error, val_error_2 = self._get_val_loss_and_err()\n",
        "\n",
        "            print(f'Training Loss: {train_loss:.4f}, Training Error: {train_error:.4f}, '\n",
        "                  f'Training Secondary Error: {train_error_2:.4f}\\n'\n",
        "                  f'Validation Loss: {val_loss:.4f}, Validation Error: {val_error:.4f}, '\n",
        "                  f'Validation Secondary Error: {val_error_2:.4f}')\n",
        "\n",
        "            # Change learning rate according to scheduler\n",
        "            self.scheduler.step(val_error)\n",
        "\n",
        "            # save checkpoint and best model\n",
        "            if self.min_val_error is None:\n",
        "                self.min_val_error = val_error\n",
        "                is_best = True\n",
        "                print(f'Best model obtained at the end of epoch {epoch}')\n",
        "            else:\n",
        "                if val_error < self.min_val_error:\n",
        "                    self.min_val_error = val_error\n",
        "                    is_best = True\n",
        "                    print(f'Best model obtained at the end of epoch {epoch}')\n",
        "                else:\n",
        "                    is_best = False\n",
        "            self.model_checkpoint.save(is_best, self.min_val_error, self.early_stopping.num_bad_epochs,\n",
        "                                       epoch, self.model, self.optimizer, self.scheduler)\n",
        "\n",
        "            # write logs\n",
        "            self.writer.add_scalar(f'{self.config.loss_fn}/train', train_loss, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.loss_fn}/val', val_loss, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.metric}/train', train_error, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.metric}/val', val_error, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.secondary_metric}/train', train_error_2, epoch * i)\n",
        "            self.writer.add_scalar(f'{self.config.secondary_metric}/val', val_error_2, epoch * i)\n",
        "\n",
        "            # Early Stopping\n",
        "            if self.early_stopping.step(val_error):\n",
        "                print(f' Training Stopped'.center(self.terminal_width, '*'))\n",
        "                print(f'Early stopping triggered after epoch {epoch}')\n",
        "                break\n",
        "\n",
        "        self.writer.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # sys.stdout = open('train.log', 'w')\n",
        "    # sys.stderr = sys.stdout\n",
        "    config = Config\n",
        "    terminal_width = shutil.get_terminal_size((80, 20)).columns\n",
        "    # Check if k-fold training is enabled\n",
        "    if config.k_fold:\n",
        "        print(f' K-fold Training '.center(terminal_width, '*'))\n",
        "\n",
        "        # If resuming model training, start training from specified fold\n",
        "        start_fold = config.resume_from_fold - 1 if config.resume_training else 0\n",
        "\n",
        "        # Loop over all folds and train model using the corresponding fold config\n",
        "        for fold, [fold_train_ts, fold_val_ts] in enumerate(config.k_fold_splits):\n",
        "            if fold < start_fold:\n",
        "                continue\n",
        "            config.fold = fold + 1\n",
        "            print()\n",
        "            print(f' Fold [{config.fold}/{len(config.k_fold_splits)}] '.center(terminal_width, '*'))\n",
        "            config.training_ts, config.validation_ts = fold_train_ts, fold_val_ts\n",
        "\n",
        "            trainer = Trainer(config)\n",
        "            trainer.train()\n",
        "            config.resume_training = False  # Train future folds from the beginning\n",
        "    else:\n",
        "        config.fold = None\n",
        "        trainer = Trainer(config)\n",
        "        trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "******************************* K-fold Training ********************************\n\n********************************** Fold [1/3] **********************************\n************************* Model: seq2seq_w_attn_on_hid *************************\nSeq2Seq(\n  (encoder): Encoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3050, 50)\n      (1): Embedding(8, 4)\n      (2): Embedding(4, 2)\n      (3): Embedding(11, 5)\n      (4): Embedding(4, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (rnns): ModuleList(\n      (0): LSTM(132, 128, bidirectional=True)\n      (1): LSTM(256, 128, bidirectional=True)\n    )\n    (rnn_dropouts): ModuleList(\n      (0): Dropout(p=0.2, inplace=False)\n    )\n  )\n  (decoder): AttnDecoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3050, 50)\n      (1): Embedding(8, 4)\n      (2): Embedding(4, 2)\n      (3): Embedding(11, 5)\n      (4): Embedding(4, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (attns): ModuleList(\n      (0): Linear(in_features=644, out_features=364, bias=True)\n      (1): Linear(in_features=644, out_features=364, bias=True)\n    )\n    (attn_combine): ModuleList(\n      (0): Linear(in_features=256, out_features=128, bias=True)\n      (1): Linear(in_features=256, out_features=128, bias=True)\n    )\n    (rnns): ModuleList(\n      (0): LSTM(132, 128, bidirectional=True)\n      (1): LSTM(256, 128, bidirectional=True)\n    )\n    (rnn_dropouts): ModuleList(\n      (0): Dropout(p=0.0, inplace=False)\n    )\n    (pred): Linear(in_features=256, out_features=9, bias=True)\n  )\n)\n\n********************************* Loading Data *********************************\n*********************************** Training ***********************************\n\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [1/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.3540, Training Error: 0.3843, Training Secondary Error: 36.1862\nValidation Loss: 0.3366, Validation Error: 0.3366, Validation Secondary Error: 28.8360\nBest model obtained at the end of epoch 1\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [2/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.2786, Training Error: 0.3036, Training Secondary Error: 23.5565\nValidation Loss: 0.1954, Validation Error: 0.1954, Validation Secondary Error: 19.7424\nBest model obtained at the end of epoch 2\n\n********************************** Fold [2/3] **********************************\n************************* Model: seq2seq_w_attn_on_hid *************************\nSeq2Seq(\n  (encoder): Encoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3050, 50)\n      (1): Embedding(8, 4)\n      (2): Embedding(4, 2)\n      (3): Embedding(11, 5)\n      (4): Embedding(4, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (rnns): ModuleList(\n      (0): LSTM(132, 128, bidirectional=True)\n      (1): LSTM(256, 128, bidirectional=True)\n    )\n    (rnn_dropouts): ModuleList(\n      (0): Dropout(p=0.2, inplace=False)\n    )\n  )\n  (decoder): AttnDecoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3050, 50)\n      (1): Embedding(8, 4)\n      (2): Embedding(4, 2)\n      (3): Embedding(11, 5)\n      (4): Embedding(4, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (attns): ModuleList(\n      (0): Linear(in_features=644, out_features=364, bias=True)\n      (1): Linear(in_features=644, out_features=364, bias=True)\n    )\n    (attn_combine): ModuleList(\n      (0): Linear(in_features=256, out_features=128, bias=True)\n      (1): Linear(in_features=256, out_features=128, bias=True)\n    )\n    (rnns): ModuleList(\n      (0): LSTM(132, 128, bidirectional=True)\n      (1): LSTM(256, 128, bidirectional=True)\n    )\n    (rnn_dropouts): ModuleList(\n      (0): Dropout(p=0.0, inplace=False)\n    )\n    (pred): Linear(in_features=256, out_features=9, bias=True)\n  )\n)\n\n********************************* Loading Data *********************************\n*********************************** Training ***********************************\n\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [1/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.2482, Training Error: 0.2530, Training Secondary Error: 25.5306\nValidation Loss: 0.3109, Validation Error: 0.3109, Validation Secondary Error: 44.4293\nBest model obtained at the end of epoch 1\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [2/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.2046, Training Error: 0.2195, Training Secondary Error: 26.5704\nValidation Loss: 0.1806, Validation Error: 0.1806, Validation Secondary Error: 16.4910\nBest model obtained at the end of epoch 2\n\n********************************** Fold [3/3] **********************************\n************************* Model: seq2seq_w_attn_on_hid *************************\nSeq2Seq(\n  (encoder): Encoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3050, 50)\n      (1): Embedding(8, 4)\n      (2): Embedding(4, 2)\n      (3): Embedding(11, 5)\n      (4): Embedding(4, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (rnns): ModuleList(\n      (0): LSTM(132, 128, bidirectional=True)\n      (1): LSTM(256, 128, bidirectional=True)\n    )\n    (rnn_dropouts): ModuleList(\n      (0): Dropout(p=0.2, inplace=False)\n    )\n  )\n  (decoder): AttnDecoder(\n    (embeddings): ModuleList(\n      (0): Embedding(3050, 50)\n      (1): Embedding(8, 4)\n      (2): Embedding(4, 2)\n      (3): Embedding(11, 5)\n      (4): Embedding(4, 2)\n    )\n    (cal_embedding): Embedding(31, 16)\n    (attns): ModuleList(\n      (0): Linear(in_features=644, out_features=364, bias=True)\n      (1): Linear(in_features=644, out_features=364, bias=True)\n    )\n    (attn_combine): ModuleList(\n      (0): Linear(in_features=256, out_features=128, bias=True)\n      (1): Linear(in_features=256, out_features=128, bias=True)\n    )\n    (rnns): ModuleList(\n      (0): LSTM(132, 128, bidirectional=True)\n      (1): LSTM(256, 128, bidirectional=True)\n    )\n    (rnn_dropouts): ModuleList(\n      (0): Dropout(p=0.0, inplace=False)\n    )\n    (pred): Linear(in_features=256, out_features=9, bias=True)\n  )\n)\n\n********************************* Loading Data *********************************\n*********************************** Training ***********************************\n\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [1/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.2595, Training Error: 0.2522, Training Secondary Error: 26.3637\nValidation Loss: 0.2953, Validation Error: 0.2953, Validation Secondary Error: 31.0250\nBest model obtained at the end of epoch 1\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [2/2] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nTraining Loss: 0.2100, Training Error: 0.2026, Training Secondary Error: 18.0921\nValidation Loss: 0.2801, Validation Error: 0.2801, Validation Secondary Error: 34.4033\nBest model obtained at the end of epoch 2\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "loss = 0.354 : 100%|| 3749/3749 [31:45<00:00,  1.97it/s]\n             : 100%|| 268/268 [00:37<00:00,  7.21it/s]\nloss = 0.279 : 100%|| 3749/3749 [31:47<00:00,  1.97it/s]\n             : 100%|| 268/268 [00:37<00:00,  7.17it/s]\nloss = 0.248 : 100%|| 3749/3749 [31:45<00:00,  1.97it/s]\n             : 100%|| 268/268 [00:37<00:00,  7.20it/s]\nloss = 0.205 : 100%|| 3749/3749 [31:45<00:00,  1.97it/s]\n             : 100%|| 268/268 [00:37<00:00,  7.17it/s]\nloss = 0.259 : 100%|| 3749/3749 [31:43<00:00,  1.97it/s]\n             : 100%|| 268/268 [00:37<00:00,  7.16it/s]\nloss = 0.210 : 100%|| 3749/3749 [31:44<00:00,  1.97it/s]\n             : 100%|| 268/268 [00:36<00:00,  7.26it/s]\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vzNppuVlr4i",
        "outputId": "0415258f-dc4e-48c0-923f-53acaa258a15",
        "gather": {
          "logged": 1651342082142
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Uncertainty_stream",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}