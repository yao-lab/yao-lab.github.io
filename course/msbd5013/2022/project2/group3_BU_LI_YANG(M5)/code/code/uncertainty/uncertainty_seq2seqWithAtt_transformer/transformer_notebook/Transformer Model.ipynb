{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "from tqdm import notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pickle as pkl\n",
    "import shutil\n",
    "from tqdm import tqdm, notebook\n",
    "import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../accuracy_stream/data/data.pickle', 'rb') as f:\n",
    "    data_dict = pkl.load(f)\n",
    "\n",
    "ids = data_dict['sales_data_ids']\n",
    "# calendar_index = data_dict['calendar_index']\n",
    "X_prev_day_sales = data_dict['X_prev_day_sales']\n",
    "X_enc_only_feats = data_dict['X_enc_only_feats']\n",
    "X_enc_dec_feats = data_dict['X_enc_dec_feats']\n",
    "X_calendar = data_dict['X_calendar']\n",
    "enc_dec_feat_names = data_dict['enc_dec_feat_names']\n",
    "Y = data_dict['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.extend(['..'])\n",
    "from data_loader.data_generator import DataLoader\n",
    "from utils.data_utils import *\n",
    "from utils.training_utils import ModelCheckpoint, EarlyStopping\n",
    "from losses_and_metrics import loss_functions, metrics\n",
    "from config import Config\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Config:\n",
    "\n",
    "    resume_training = False\n",
    "\n",
    "    loss_fn = 'SPLLoss'\n",
    "    metric = 'SPLMetric'\n",
    "    secondary_metric = 'WRMSSEMetric'\n",
    "    architecture = 'seq2seq'\n",
    "\n",
    "    # Running a sliding window training will help increase the training data\n",
    "    sliding_window = False  # Note: sliding window has not been tested with WRMSSELoss\n",
    "    window_length = 28 * 13\n",
    "\n",
    "    # hidden dimension and no. of layers will be the same for both encoder and decoder\n",
    "    rnn_num_hidden = 2\n",
    "    rnn_num_layers = 1\n",
    "    bidirectional = False\n",
    "    enc_rnn_dropout = 0.2\n",
    "    dec_rnn_dropout = 0.0\n",
    "\n",
    "    num_epochs = 1\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # training, validation and test periods\n",
    "    training_ts = {'data_start_t': 1969 - 1 - (28 * 7), 'horizon_start_t': 1969 - 1 - (28 * 4),\n",
    "                   'horizon_end_t': 1969 - 1 - (28 * 3)}\n",
    "    validation_ts = {'data_start_t': 1969 - 1 - (28 * 6), 'horizon_start_t': 1969 - 1 - (28 * 3),\n",
    "                     'horizon_end_t': 1969 - 1 - (28 * 2)}\n",
    "    test_ts = {'data_start_t': 1969 - 1 - (28 * 15), 'horizon_start_t': 1969 - 1 - (28 * 2),\n",
    "               'horizon_end_t': 1969 - 1 - (28 * 1)}\n",
    "\n",
    "    data_file = '../../accuracy_stream/data/data.pickle'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config, model):\n",
    "        self.config = config\n",
    "        self.terminal_width = shutil.get_terminal_size((80, 20)).columns\n",
    "\n",
    "        # Model\n",
    "        print(f' Model: {self.config.architecture} '.center(self.terminal_width, '*'))\n",
    "        self.model = model\n",
    "\n",
    "        # Loss, Optimizer and LRScheduler\n",
    "        self.criterion = getattr(loss_functions, config.loss_fn)(self.config)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5,\n",
    "                                                                    patience=5, verbose=True)\n",
    "        self.early_stopping = EarlyStopping(patience=10)\n",
    "        self.agg_sum = self.config.loss_fn[:3] == 'SPL'\n",
    "        self.loss_agg = np.sum if self.agg_sum else np.mean\n",
    "\n",
    "        # Metric\n",
    "        self.metric = getattr(metrics, config.metric)()\n",
    "        self.metric_2 = getattr(metrics, config.secondary_metric)()\n",
    "\n",
    "        print(f' Loading data '.center(self.terminal_width, '*'))\n",
    "        data_loader = DataLoader(self.config)\n",
    "        self.ids = data_loader.ids\n",
    "        self.train_loader = data_loader.create_train_loader()\n",
    "        self.val_loader = data_loader.create_val_loader()\n",
    "        self.n_windows = data_loader.n_windows\n",
    "        self.rnn_input = []\n",
    "\n",
    "\n",
    "        self.start_epoch, self.min_val_error = 1, None\n",
    "\n",
    "    def _get_val_loss_and_err(self):\n",
    "        self.model.eval()\n",
    "        progbar = tqdm(self.val_loader)\n",
    "        progbar.set_description(\"             \")\n",
    "        losses, epoch_preds, epoch_ys, epoch_ws, epoch_scales = [], [], [], [], []\n",
    "        for i, [x, y, norm_factor, ids_idx, loss_input, _] in enumerate(progbar):\n",
    "            epoch_ys.append(y.data.numpy())\n",
    "            epoch_scales.append(loss_input[0].data.numpy())\n",
    "            epoch_ws.append(loss_input[1].data.numpy())\n",
    "\n",
    "            x = [inp.to(self.config.device) for inp in x]\n",
    "            y = y.to(self.config.device)\n",
    "            norm_factor = norm_factor.to(self.config.device)\n",
    "            loss_input = [inp.to(self.config.device) for inp in loss_input]\n",
    "\n",
    "            preds = self.model(*x) * norm_factor[:, None, None]\n",
    "            epoch_preds.append(preds.data.cpu().numpy())\n",
    "            loss = self.criterion(preds, y, *loss_input)\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "        epoch_preds, epoch_ys = np.concatenate(epoch_preds, axis=0), np.concatenate(epoch_ys, axis=0)\n",
    "        epoch_ws, epoch_scales = np.concatenate(epoch_ws, axis=0), np.concatenate(epoch_scales, axis=0)\n",
    "\n",
    "        val_error = self.metric.get_error(epoch_preds, epoch_ys, epoch_scales, epoch_ws)\n",
    "        val_error_2 = self.metric_2.get_error(epoch_preds[:, :, 4], epoch_ys, epoch_scales, epoch_ws)\n",
    "\n",
    "        return self.loss_agg(losses), val_error, val_error_2\n",
    "\n",
    "    def train(self):\n",
    "        print(f' Training '.center(self.terminal_width, '*'), end='\\n\\n')\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.config.num_epochs + 1):\n",
    "            print(f' Epoch [{epoch}/{self.config.num_epochs}] '.center(self.terminal_width, 'x'))\n",
    "            self.model.train()\n",
    "            progbar = notebook.tqdm(self.train_loader)\n",
    "            losses, epoch_preds, epoch_ys, epoch_ws, epoch_scales = [], [], [], [], []\n",
    "            for i, [x, y, norm_factor, ids_idx, loss_input, window_id] in enumerate(progbar):\n",
    "                x = [inp.to(self.config.device) for inp in x]\n",
    "#                 self.rnn_input.append(x[3].data.cpu().numpy())\n",
    "                y = y.to(self.config.device)\n",
    "                norm_factor = norm_factor.to(self.config.device)\n",
    "                loss_input = [inp.to(self.config.device) for inp in loss_input]\n",
    "\n",
    "                # Forward + Backward + Optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.model(*x) * norm_factor[:, None, None]\n",
    "\n",
    "                if self.config.sliding_window:\n",
    "                    if torch.sum(window_id == self.n_windows - 1) > 0:\n",
    "                        epoch_ys.append(y[window_id == self.n_windows - 1].data.cpu().numpy().reshape(-1, 28))\n",
    "                        epoch_scales.append(loss_input[0][window_id == self.n_windows - 1]\n",
    "                                            .data.cpu().numpy().reshape(-1))\n",
    "                        epoch_ws.append(loss_input[1][window_id == self.n_windows - 1]\n",
    "                                        .data.cpu().numpy().reshape(-1))\n",
    "                        epoch_preds.append(preds[window_id == self.n_windows - 1].data.cpu().numpy().reshape(-1, 28, 9))\n",
    "                else:\n",
    "                    epoch_ys.append(y.data.cpu().numpy())\n",
    "                    epoch_scales.append(loss_input[0].data.cpu().numpy())\n",
    "                    epoch_ws.append(loss_input[1].data.cpu().numpy())\n",
    "                    epoch_preds.append(preds.data.cpu().cpu().numpy())\n",
    "\n",
    "                loss = self.criterion(preds, y, *loss_input)\n",
    "                losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "                if self.agg_sum:\n",
    "                    progbar.set_description(\"loss = %0.3f \" % np.round(\n",
    "                        (len(self.train_loader) / (i + 1)) * self.loss_agg(losses) / self.n_windows, 3))\n",
    "                else:\n",
    "                    progbar.set_description(\"loss = %0.3f \" % np.round(self.loss_agg(losses), 3))\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Get training and validation loss and error\n",
    "            epoch_preds, epoch_ys = np.concatenate(epoch_preds, axis=0), np.concatenate(epoch_ys, axis=0)\n",
    "            epoch_ws, epoch_scales = np.concatenate(epoch_ws, axis=0), np.concatenate(epoch_scales, axis=0)\n",
    "\n",
    "            if self.agg_sum:\n",
    "                train_loss = self.loss_agg(losses) / self.n_windows\n",
    "            else:\n",
    "                train_loss = self.loss_agg(losses)\n",
    "\n",
    "            train_error = self.metric.get_error(epoch_preds, epoch_ys, epoch_scales, epoch_ws)\n",
    "            train_error_2 = self.metric_2.get_error(epoch_preds[:, :, 4], epoch_ys, epoch_scales, epoch_ws)\n",
    "\n",
    "            val_loss, val_error, val_error_2 = self._get_val_loss_and_err()\n",
    "\n",
    "            print(f'Training Loss: {train_loss:.4f}, Training Error: {train_error:.4f}, '\n",
    "                  f'Training Secondary Error: {train_error_2:.4f}\\n'\n",
    "                  f'Validation Loss: {val_loss:.4f}, Validation Error: {val_error:.4f}, '\n",
    "                  f'Validation Secondary Error: {val_error_2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "# Build a seq2seq model\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_sizes, cal_embedding_sizes, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.input_size = input_size\n",
    "        self.max_length = config.window_length if config.sliding_window \\\n",
    "            else config.training_ts['horizon_start_t'] - config.training_ts['data_start_t']\n",
    "\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(classes, hidden_size)\n",
    "                                         for classes, hidden_size in embedding_sizes])\n",
    "        self.cal_embedding = nn.Embedding(cal_embedding_sizes[0], cal_embedding_sizes[1])\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(self.max_length, self.input_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.input_size, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.input_size])).to(self.config.device)\n",
    "\n",
    "    def forward(self, x, x_emb, x_cal_emb):\n",
    "        x, x_emb, x_cal_emb = x.permute(1, 0, 2), x_emb.permute(1, 0, 2), x_cal_emb.permute(1, 0, 2)  # make time-major\n",
    "        output_emb = [emb(x_emb[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        output_emb = torch.cat(output_emb, 2)\n",
    "\n",
    "        # share embedding layer for both the calendar events\n",
    "        output_emb_cal = [self.cal_embedding(x_cal_emb[:, :, 0]), self.cal_embedding(x_cal_emb[:, :, 1])]\n",
    "        output_emb_cal = torch.cat(output_emb_cal, 2)\n",
    "\n",
    "        x = torch.cat([x, output_emb, output_emb_cal], 2).permute(1, 0, 2)\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        x_len = x.shape[1]\n",
    "        \n",
    "        # Positional Encoding\n",
    "        pos = torch.arange(0, x_len).unsqueeze(0).repeat(batch_size, 1).to(self.config.device)\n",
    "        x = self.dropout((x * self.scale) + self.pos_embedding(pos))\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Transformer\n",
    "        output = self.transformer_encoder(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_sizes, cal_embedding_sizes, output_size, config):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.config = config\n",
    "        self.input_size = input_size\n",
    "        self.max_length = 28\n",
    "\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(classes, hidden_size)\n",
    "                                         for classes, hidden_size in embedding_sizes])\n",
    "        self.cal_embedding = nn.Embedding(cal_embedding_sizes[0], cal_embedding_sizes[1])\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(self.max_length, self.input_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=self.input_size, nhead=4)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.input_size])).to(self.config.device)\n",
    "        self.pred = nn.Linear(self.input_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, enc_out, y_mask, x, x_emb, x_cal_emb):\n",
    "        x, x_emb, x_cal_emb = x.permute(1, 0, 2), x_emb.permute(1, 0, 2), x_cal_emb.permute(1, 0, 2)  # make time-major\n",
    "        output_emb = [emb(x_emb[:, :, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        output_emb = torch.cat(output_emb, 2)\n",
    "\n",
    "        # share embedding layer for both the calendar events\n",
    "        output_emb_cal = [self.cal_embedding(x_cal_emb[:, :, 0]), self.cal_embedding(x_cal_emb[:, :, 1])]\n",
    "        output_emb_cal = torch.cat(output_emb_cal, 2)\n",
    "\n",
    "        x = torch.cat([x, output_emb, output_emb_cal], 2).permute(1, 0, 2)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x_len = x.shape[1]\n",
    "        \n",
    "        # Positional Encoding\n",
    "        pos = torch.arange(0, x_len).unsqueeze(0).repeat(batch_size, 1).to(self.config.device)\n",
    "        x = self.dropout((x * self.scale) + self.pos_embedding(pos))\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Transformer\n",
    "        output = self.transformer_decoder(x, enc_out, y_mask)\n",
    "        \n",
    "        output = self.pred(output.permute(1, 0, 2))\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.config = config\n",
    "        self.pred_len = 28\n",
    "        \n",
    "    def forward(self, x_enc, x_enc_emb, x_cal_enc_emb, x_dec, x_dec_emb, x_cal_dec_emb, x_prev_day_sales_dec):\n",
    "        batch_size = x_dec.shape[0]\n",
    "\n",
    "        encoder_output = self.encoder(x_enc, x_enc_emb, x_cal_enc_emb)\n",
    "        \n",
    "        # If running in eval mode, run through the decoder in single steps and use predicted x_prev_sales\n",
    "        # as actual x_prev_sales is not available\n",
    "        if self.training:\n",
    "            x_dec = torch.cat([x_dec, x_prev_day_sales_dec], dim=2)\n",
    "            y_mask = torch.tril(torch.ones((self.pred_len, self.pred_len))).bool()\n",
    "            predictions = self.decoder(encoder_output, y_mask, x_dec, x_dec_emb, x_cal_dec_emb)\n",
    "        else:\n",
    "            \n",
    "            # create a tensor to store the outputs\n",
    "            predictions = torch.zeros(batch_size, self.pred_len, 9).to(self.config.device)\n",
    "            \n",
    "            for timestep in range(self.pred_len):\n",
    "\n",
    "                if timestep == 0:\n",
    "                    # for the first timestep of decoder, use previous steps' sales\n",
    "                    dec_input = torch.cat([x_dec[:, 0, :], x_prev_day_sales_dec[:, 0]], dim=1).unsqueeze(1)\n",
    "                else:\n",
    "                    # for next timestep, current timestep's output will serve as the input along with other features\n",
    "                    dec_input = torch.cat([x_dec[:, timestep, :], decoder_output[:, 4].unsqueeze(1)], dim=1).unsqueeze(1)\n",
    "                \n",
    "                y_mask = torch.tril(torch.ones((1, 1))).bool()\n",
    "                decoder_output = self.decoder(encoder_output, y_mask, dec_input,\n",
    "                                              x_dec_emb[:, timestep, :].unsqueeze(1),\n",
    "                                              x_cal_dec_emb[:, timestep, :].unsqueeze(1))[:, 0, :]\n",
    "\n",
    "                # add predictions to predictions tensor\n",
    "                predictions[:, timestep] = decoder_output\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): Encoder(\n",
       "    (embeddings): ModuleList(\n",
       "      (0): Embedding(3050, 2)\n",
       "      (1): Embedding(8, 1)\n",
       "      (2): Embedding(4, 1)\n",
       "      (3): Embedding(11, 1)\n",
       "      (4): Embedding(4, 1)\n",
       "    )\n",
       "    (cal_embedding): Embedding(31, 1)\n",
       "    (pos_embedding): Embedding(84, 20)\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=20, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=20, bias=True)\n",
       "          (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=20, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=20, bias=True)\n",
       "          (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embeddings): ModuleList(\n",
       "      (0): Embedding(3050, 2)\n",
       "      (1): Embedding(8, 1)\n",
       "      (2): Embedding(4, 1)\n",
       "      (3): Embedding(11, 1)\n",
       "      (4): Embedding(4, 1)\n",
       "    )\n",
       "    (cal_embedding): Embedding(31, 1)\n",
       "    (pos_embedding): Embedding(28, 20)\n",
       "    (transformer_decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=20, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=20, bias=True)\n",
       "          (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=20, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=20, bias=True)\n",
       "          (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (pred): Linear(in_features=20, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for item_id, dept_id, cat_id, store_id, state_id respectively\n",
    "embedding_sizes = [(3049 + 1, 2), (7 + 1, 1), (3 + 1, 1), (10 + 1, 1), (3 + 1, 1)]\n",
    "cal_embedding_sizes = (31, 1)\n",
    "num_features_enc = 12 + sum([j for i, j in embedding_sizes]) + cal_embedding_sizes[1] * 2\n",
    "num_features_dec = 12 + sum([j for i, j in embedding_sizes]) + cal_embedding_sizes[1] * 2\n",
    "enc = Encoder(num_features_enc, embedding_sizes, cal_embedding_sizes, config)\n",
    "dec = Decoder(num_features_dec, embedding_sizes, cal_embedding_sizes, 9, config)\n",
    "model = TransformerModel(enc, dec, config)\n",
    "model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************** Model: seq2seq ********************************\n",
      "********************************* Loading data *********************************\n",
      "*********************************** Training ***********************************\n",
      "\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Epoch [1/1] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c7226c69d44939afe15ef525760154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=335.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "             : 100%|██████████| 335/335 [01:37<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4396, Training Error: 0.4396, Training Secondary Error: 49.2589\n",
      "Validation Loss: 0.3600, Validation Error: 0.3600, Validation Secondary Error: 39.3912\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config, model)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
