{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Contest : Home Credit Default Risk\n",
    "MAFS 6010Z Project 1 \n",
    "Wong Hoi Ming(20641276)\n",
    "Wong Sik Tsun(20038819)\n",
    "\n",
    "https://www.kaggle.com/c/home-credit-default-risk/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are going to apply gradient boosting trees to predict whether a client will have payment difficulties.\n",
    "\n",
    "We will go straight into the implementation part, followed by results analysis, as a clear description of the data set can be find in the following link.\n",
    "https://www.kaggle.com/c/home-credit-default-risk/data\n",
    "\n",
    "##Data Processing\n",
    "\n",
    "We first import our data sets. The data of current applications, previous applications and clients' history of required installmet and actual payment will be explored and used for model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data and install the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "app_train = pd.read_csv(\"application_train.csv\",sep = \",\")\n",
    "app_test = pd.read_csv(\"application_test.csv\",sep = \",\")\n",
    "prev_app = pd.read_csv(\"previous_application.csv\",sep = \",\")\n",
    "instl_pmt = pd.read_csv(\"installments_payments.csv\",sep = \",\")\n",
    "\n",
    "#print(app_train.shape)\n",
    "    \n",
    "# We define 3 functions for the exploratory analysis on the features\n",
    "# This function is written for checking the situation of missing values in each column of a dataframe\n",
    "\n",
    "def missing_val_table(dataframe):\n",
    "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
    "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
    "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
    "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
    "    return missing_df\n",
    "\n",
    "# We define two functions for a brief summary of our numerical and categorical variables\n",
    "def cat_summary(dataframe, col_name, plot=False):\n",
    "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
    "                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))  \n",
    "def num_summary(dataframe, numerical_col, plot=False):\n",
    "    quantiles = [0.01, 0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n",
    "    print(dataframe[numerical_col].describe(quantiles).T)    \n",
    "    \n",
    "##########################################################################################\n",
    "############ Data Cleansing/Feature Engineering in data set application_train ############\n",
    "##########################################################################################\n",
    "\n",
    "# We replace illogical data point in days_employed and replace it with na\n",
    "# since one should not have worked for 1000 years. The 365243 data point is not valid\n",
    "app_train['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "# We skim through the categorical variables and check the no. of distinct values in each of them\n",
    "# We apply domain knowledge to group different values of occupation & organization type\n",
    "# so that later we will have a lower dimensionality when we do one-hot encoding for the categorical variables\n",
    "app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)\n",
    "np.unique(app_train['ORGANIZATION_TYPE'])\n",
    "np.unique(str(app_train['OCCUPATION_TYPE']))\n",
    "\n",
    "## We define a function that will process app_train and app_test in the same manner\n",
    "def process_applications(df):\n",
    "# Organization - We regroup the organization type so that the resultant categorization will leave \n",
    "# fewer distinct values\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Business Entity\"), \n",
    "                                       \"Business_Entity\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Industry\"), \n",
    "                                       \"Industry\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Trade\"),\"Trade\",\n",
    "                                   df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Emergency\",\"Police\", \n",
    "                                                                 \"Government\", \"Postal\", \n",
    "                                                                \"Military\", \"Security Ministries\"]), \n",
    "                                       \"Official\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Transport\"),\n",
    "                                       \"Transport\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"School\", \"Kindergarten\", \"University\"]),\n",
    "                                       \"Education\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Realtor\", \"Housing\"]), \"Property\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Hotel\", \"Restaurant\",\"Services\",\"Advertising\"]), \n",
    "                                   \"F&B_Hospitality_Ads\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Bank\", \"Insurance\"]),\n",
    "                                       \"Financial\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Cleaning\",\"Electricity\", \"Telecom\", \"Mobile\", \"Security\"]), \"Utilities\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Medicine\", \"Legal Services\"]), \"Professional\", df[\"ORGANIZATION_TYPE\"])\n",
    "    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Religion\", \"Culture\"]), \"Other\", df[\"ORGANIZATION_TYPE\"])\n",
    "\n",
    "# OCCUPATION_TYPE - We do similar the similar thing for occupation\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Low-skill Laborers\", \"Cooking staff\", \"Security staff\",\n",
    "                                                                 \"Cleaning staff\", \"Waiters/barmen staff\", \"Laborers\"]), \n",
    "                                     \"Laborers\", df[\"OCCUPATION_TYPE\"])\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"IT staff\", \"High skill tech staff\",\"Accountants\"]), \n",
    "                                     \"High_skill_staff\", df[\"OCCUPATION_TYPE\"])\n",
    "    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Secretaries\", \"HR staff\",\"Realty agents\", \n",
    "                                                                 \"Private service staff\"]), \n",
    "                                     \"Others\", df[\"OCCUPATION_TYPE\"])\n",
    "\n",
    "# We group all the flags of address mismatch into 1 variable by summing them to reduce dimension\n",
    "# This new variable can be interpreted as degree of mismatch, ranging from 0-6\n",
    "# ADDRESS_MISMATCH\n",
    "    cols = [\"REG_REGION_NOT_LIVE_REGION\",\"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \n",
    "                \"REG_CITY_NOT_LIVE_CITY\",\"REG_CITY_NOT_WORK_CITY\",\"LIVE_CITY_NOT_WORK_CITY\"]\n",
    "    df[\"ADDRESS_MISMATCH\"] = df[cols].sum(axis = 1)\n",
    "    df.drop(cols, axis = 1, inplace = True)\n",
    "\n",
    "    #### Adding new variables####\n",
    "\n",
    "    # 1 DAYS_EMPLOYED_RATIO\n",
    "    # We believe it is more reasonable to consider one's employment history relative to his/her age\n",
    "    # So we add a variable to calculate the ratio.\n",
    "    # The more time he/she spend in working in his/her whole life, he/she may be more responsible or capable,\n",
    "    # and thus having more ability to repay\n",
    "    df['DAYS_EMPLOYED_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "\n",
    "    # The next two transform the credit score from external sources\n",
    "    # 2 Simple average of EXT_SOURCE_1 to EXT_SOURCE_3\n",
    "    df[\"EXTSOURCE_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "\n",
    "    # 3 Geometric mean of EXT_SOURCE_1 to EXT_SOURCE_3, in case one client scores very high in one, but low in the other two\n",
    "    df['EXTSOURCES_GM'] = pow(df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3'], 1/3)\n",
    "\n",
    "    # The next three calculate ratios of annuity, income and credit amount. We postulate that higher income level relative to\n",
    "    # loan amount should imply better ability to repay\n",
    "    \n",
    "    # 4 Ratio of loan annuity to the credit amount of the loan\n",
    "    df['ANNUITY_CREDIT_RATIO'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "\n",
    "    # 5 Ratio of loan annuity to the income level of the loan\n",
    "    df['ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "\n",
    "    # 6 Ratio of income level of client to credit amount\n",
    "    df['INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "\n",
    "    # The next two added variable consider the credit amount relative to the customer consumption\n",
    "    # 7 Ratio of credit amount to value of goods purchased\n",
    "    df[\"CREDIT_GOODS_RATIO\"] = df[\"AMT_CREDIT\"]/df[\"AMT_GOODS_PRICE\"]\n",
    "    # 8 Diff btw credit amount and value of goods purchased\n",
    "    df[\"CREDIT_GOODS_DIFF\"] = df[\"AMT_CREDIT\"] - df[\"AMT_GOODS_PRICE\"]\n",
    "\n",
    "process_applications(app_train)\n",
    "process_applications(app_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################################################\n",
    "#### Data Cleansing/Aggregation/Feature Engineering in data set previous application #####\n",
    "##########################################################################################\n",
    "\n",
    "df = prev_app.copy()\n",
    "# We note that the below five columns contains illogical inputs \n",
    "#for the days in prev_app relative to the current application\n",
    "# We consider it to be missing values\n",
    "df['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "\n",
    "prev_app.select_dtypes('object').apply(pd.Series.nunique, axis=0)\n",
    "### Regroup Goods Category to reduce # of one-hot variables to be added in the modelling step\n",
    "a = ['Auto Accessories','Vehicles']\n",
    "h = ['Construction Materials','Furniture','Construction Materials','Homewares','House Construction','Gardening']\n",
    "e = ['Audio/Video', 'Computers', 'Consumer Electronics', 'Photo / Cinema Equipment','Mobile']\n",
    "m = ['Medical Supplies','Medicine']\n",
    "c = ['Jewelry', 'Clothing and Accessories']\n",
    "l = ['Sport and Leisure', 'Tourism','Fitness']\n",
    "o = ['Additional Service','Weapon','Office Appliances','Insurance','Direct Sales','Animals']\n",
    "df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(a, 'Automobile-related')\n",
    "df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(h, 'Home_related')\n",
    "df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(e, 'Electronics')\n",
    "df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(m, 'Medical_related')\n",
    "df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(c, 'Fashion&Jewelry')\n",
    "df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(l, 'Leisure')\n",
    "df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(o, 'Other')\n",
    "\n",
    "### Regroup seller industry to reduce the number of distinct values\n",
    "df['NAME_SELLER_INDUSTRY'] = df['NAME_SELLER_INDUSTRY'].replace(['Clothing','Jewelry'],'Fashion&Jewelry')\n",
    "df['NAME_SELLER_INDUSTRY'] = df['NAME_SELLER_INDUSTRY'].replace(['Construction','Furniture'],'Building-related')\n",
    "\n",
    "## Reclassify type of suite to reduce the number of distinct values\n",
    "df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace('Unaccompanied', 'single')\n",
    "m = ['Children', 'Other_B', 'Other_A','Family', 'Spouse, partner', 'Group of people']\n",
    "df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace(m, 'multiple')\n",
    "\n",
    "del m, a, h, e, c, l, o\n",
    "\n",
    "# Since the same SK_ID_CURR have multiple records in previous applications\n",
    "# We need to aggregate them separately for the numerical and categorical features\n",
    "cat_cols = [col for col in df.columns if df[col].dtypes == \"O\"]\n",
    "col_list = df.columns.tolist()\n",
    "id_list = [\"SK_ID_CURR\",\"SK_ID_PREV\"]\n",
    "# Numerical cols are defined as those that are not categorical nor the ID column\n",
    "num_cols = [col for col in col_list if col not in cat_cols + id_list]\n",
    "df = pd.get_dummies(df, prefix=cat_cols)\n",
    "# Update the cat_cols after we add dummies\n",
    "cat_cols = [col for col in df.columns if col not in num_cols + id_list]\n",
    "# Previous applications numeric features\n",
    "num_to_agg = {}\n",
    "for col in num_cols:\n",
    "    num_to_agg[col] = ['min', 'max', 'mean']\n",
    "        \n",
    "# Previous applications categorical features\n",
    "cat_to_agg = {}\n",
    "for col in cat_cols:\n",
    "    cat_to_agg[col] = ['mean']\n",
    "        \n",
    "prev_app_agg = df.groupby('SK_ID_CURR').agg({**num_to_agg, **cat_to_agg})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "#### Data Cleansing/Aggregation/Feature Engineering in data set installment #####\n",
    "##########################################################################################\n",
    "\n",
    "df = instl_pmt.copy()\n",
    "\n",
    "# Feature Engineering\n",
    "# \"DAYS_ENTRY_PAYMENT - DAYS_INSTALMENT\" represents the number of days of delayed installment payment \n",
    "df['DAYS_DELAY_PAYMENT'] = df['DAYS_ENTRY_PAYMENT']-df['DAYS_INSTALMENT']\n",
    "# \"AMT_INSTALMENT - AMT_PAYMENT\" represents the overdue amount of each installment payment \n",
    "df['OVERDUE_AMT_PAYMENT'] = df['AMT_INSTALMENT']-df['AMT_PAYMENT']\n",
    "# The variable we created above may bias to large payment amount, therefore we also add a relative measure here\n",
    "df['OVERDUE_AMT_PAYMENT_RELATIVE'] = 0\n",
    "df.loc[df['AMT_INSTALMENT'] != 0, 'OVERDUE_AMT_PAYMENT_RELATIVE'] = df['AMT_PAYMENT']/df['AMT_INSTALMENT']\n",
    "\n",
    "# We only use part of the data fields which look more relevant and then group them by the ID of current loan for merging\n",
    "# All those data fields are with numerical datatype\n",
    "instl_cols = ['DAYS_INSTALMENT','DAYS_ENTRY_PAYMENT','AMT_INSTALMENT','AMT_PAYMENT','DAYS_DELAY_PAYMENT'\n",
    "                  ,'OVERDUE_AMT_PAYMENT','OVERDUE_AMT_PAYMENT_RELATIVE']\n",
    "instl_col_name_agg = {}\n",
    "for col in instl_cols:\n",
    "    instl_col_name_agg[col] = ['min', 'max', 'mean','sum']        \n",
    "     \n",
    "instl_agg = df.groupby('SK_ID_CURR').agg({**instl_col_name_agg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:522: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[200]\ttrain's auc: 0.822014\ttrain's binary_logloss: 0.522323\tvalid's auc: 0.787875\tvalid's binary_logloss: 0.537871\n",
      "Early stopping, best iteration is:\n",
      "[324]\ttrain's auc: 0.843292\ttrain's binary_logloss: 0.499226\tvalid's auc: 0.789099\tvalid's binary_logloss: 0.523107\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[200]\ttrain's auc: 0.822653\ttrain's binary_logloss: 0.52112\tvalid's auc: 0.776544\tvalid's binary_logloss: 0.540029\n",
      "[400]\ttrain's auc: 0.855521\ttrain's binary_logloss: 0.485119\tvalid's auc: 0.77799\tvalid's binary_logloss: 0.517723\n",
      "Early stopping, best iteration is:\n",
      "[384]\ttrain's auc: 0.85319\ttrain's binary_logloss: 0.487636\tvalid's auc: 0.778253\tvalid's binary_logloss: 0.519303\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[200]\ttrain's auc: 0.822551\ttrain's binary_logloss: 0.521648\tvalid's auc: 0.781403\tvalid's binary_logloss: 0.537482\n",
      "Early stopping, best iteration is:\n",
      "[305]\ttrain's auc: 0.841214\ttrain's binary_logloss: 0.501261\tvalid's auc: 0.782845\tvalid's binary_logloss: 0.524406\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[200]\ttrain's auc: 0.822553\ttrain's binary_logloss: 0.521551\tvalid's auc: 0.780405\tvalid's binary_logloss: 0.542109\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttrain's auc: 0.830075\ttrain's binary_logloss: 0.513333\tvalid's auc: 0.78104\tvalid's binary_logloss: 0.536978\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[200]\ttrain's auc: 0.822544\ttrain's binary_logloss: 0.521616\tvalid's auc: 0.783418\tvalid's binary_logloss: 0.540486\n",
      "[400]\ttrain's auc: 0.855172\ttrain's binary_logloss: 0.485717\tvalid's auc: 0.785133\tvalid's binary_logloss: 0.518032\n",
      "Early stopping, best iteration is:\n",
      "[421]\ttrain's auc: 0.858205\ttrain's binary_logloss: 0.482331\tvalid's auc: 0.785352\tvalid's binary_logloss: 0.515876\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "############ Model Fitting - Gradient Boosting ############\n",
    "################################################################\n",
    "\n",
    "train_data = app_train.copy()\n",
    "test_data = app_test.copy()\n",
    "\n",
    "# Preset the cross-validation fold to be 10\n",
    "n_folds = 10\n",
    "\n",
    "#Store loan ID & labels\n",
    "labels = train_data['TARGET']\n",
    "train_ids = train_data['SK_ID_CURR']\n",
    "test_ids = test_data['SK_ID_CURR']\n",
    "\n",
    "######### Inner join the applications data set with the aggregate previous application\n",
    "train_data = train_data.join(prev_app_agg,how='left',on='SK_ID_CURR')\n",
    "test_data = test_data.join(prev_app_agg,how='left',on='SK_ID_CURR')\n",
    "\n",
    "######### Inner join the two datasets with the installment data\n",
    "train_data = train_data.join(instl_agg,how='left',on='SK_ID_CURR')\n",
    "test_data = test_data.join(instl_agg,how='left',on='SK_ID_CURR')\n",
    "\n",
    "# Take out the ID & Target first before we align the feature sets by their common features\n",
    "train_data = train_data.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "test_data = test_data.drop(columns = ['SK_ID_CURR'])\n",
    "    \n",
    "# One-hot encoding for categorical variables\n",
    "train_data = pd.get_dummies(train_data)\n",
    "test_data = pd.get_dummies(test_data)\n",
    "\n",
    "# Align the dataframes by the common columns\n",
    "train_data, test_data = train_data.align(test_data, join = 'inner', axis = 1)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = list(train_data.columns)\n",
    "    \n",
    "# Convert train_data to matrices\n",
    "features = np.array(train_data)\n",
    "test_features = np.array(test_data)\n",
    "n_trn = features.shape[0]\n",
    "n_tst = test_features.shape[0]\n",
    "    \n",
    "# Create the kfold object \n",
    "k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "\n",
    "# Create empty array for feature importances, test & out-of-fold predictions\n",
    "feat_importance = np.zeros(len(feature_names))\n",
    "test_pred = np.zeros(n_tst)\n",
    "out_of_fold = np.zeros(n_trn)\n",
    "# Lists for recording validation and training scores\n",
    "valid_scores = []\n",
    "train_scores = []\n",
    "\n",
    "kf = k_fold.split(features)\n",
    "for train_index, valid_index in kf:\n",
    "    train_features, train_labels = features[train_index], labels[train_index]\n",
    "    # Validation data for the fold\n",
    "    valid_features, valid_labels = features[valid_index], labels[valid_index]    \n",
    "    model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "    # Train the model\n",
    "    model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "              eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "              eval_names = ['valid', 'train'], categorical_feature = 'auto',\n",
    "              early_stopping_rounds = 50, verbose = 200)\n",
    "    \n",
    "    best_iteration = model.best_iteration_\n",
    "    out_of_fold[valid_index] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "    \n",
    "    # Store the feature importance\n",
    "    feat_importance += model.feature_importances_ / n_folds\n",
    "    \n",
    "    # Also use the model fitted in this fold for prediction in the real test data (without target)\n",
    "    test_pred += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / n_folds\n",
    "    \n",
    "    valid_score = model.best_score_['valid']['auc']\n",
    "    train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "    valid_scores.append(valid_score)\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "valid_scores.append(valid_auc)\n",
    "train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "# Table for validation scores\n",
    "fold_names = list(range(n_folds))\n",
    "fold_names.append('overall')\n",
    "    \n",
    "# Dataframe of validation scores\n",
    "metrics = pd.DataFrame({'fold': fold_names,\n",
    "                        'train': train_scores,\n",
    "                        'valid': valid_scores})\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "######## Feature Importance Plots ###############################\n",
    "#################################################################\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a dataframe to store the feature importances & Sort the importacne in descending order\n",
    "df_ft_imp = pd.DataFrame({'feature': feature_names, 'importance': feat_importance})\n",
    "df_ft_imp = df_ft_imp.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "# Normalize the feature importances\n",
    "df_ft_imp['importance_normalized'] = df_ft_imp['importance'] / df_ft_imp['importance'].sum()\n",
    "\n",
    "# Plot the feature importances in horizontal bar charts\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (20, 12))\n",
    "ax = plt.subplot()\n",
    "ax.barh(list(reversed(list(df.index[:30]))), \n",
    "    df_ft_imp['importance_normalized'].head(30), \n",
    "    align = 'center', color = 'cyan')\n",
    "    \n",
    "# Set the yticks and labels\n",
    "ax.set_yticks(list(reversed(list(df.index[:30]))))\n",
    "ax.set_yticklabels(df_ft_imp['feature'].head(30))\n",
    "    \n",
    "# Plot labeling\n",
    "plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_pred})\n",
    "#submission.head()\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
