{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 导入必要的包\nimport numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-18T09:20:49.233548Z","iopub.execute_input":"2021-09-18T09:20:49.233911Z","iopub.status.idle":"2021-09-18T09:20:49.258743Z","shell.execute_reply.started":"2021-09-18T09:20:49.233806Z","shell.execute_reply":"2021-09-18T09:20:49.257765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导入必要的包\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\n\nfrom sklearn import datasets\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport sklearn.linear_model as skl_lm\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, roc_curve, auc\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npd.set_option('display.notebook_repr_html', False)\n\n%matplotlib inline\nplt.style.use('seaborn-white')","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:20:49.260727Z","iopub.execute_input":"2021-09-18T09:20:49.261955Z","iopub.status.idle":"2021-09-18T09:20:51.380108Z","shell.execute_reply.started":"2021-09-18T09:20:49.2619Z","shell.execute_reply":"2021-09-18T09:20:51.379255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#导入数据\napp_train = pd.read_csv('../input/home-credit-default-risk/application_train.csv')\napp_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:20:51.381495Z","iopub.execute_input":"2021-09-18T09:20:51.381769Z","iopub.status.idle":"2021-09-18T09:20:58.922378Z","shell.execute_reply.started":"2021-09-18T09:20:51.38174Z","shell.execute_reply":"2021-09-18T09:20:58.921249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 检查train data中的数据类型\napp_train.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:20:58.924585Z","iopub.execute_input":"2021-09-18T09:20:58.924886Z","iopub.status.idle":"2021-09-18T09:20:58.942296Z","shell.execute_reply.started":"2021-09-18T09:20:58.924855Z","shell.execute_reply":"2021-09-18T09:20:58.941086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 编码种类不超过2的分类变量\nle = LabelEncoder()\nle_count = 0\n \n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:20:58.944178Z","iopub.execute_input":"2021-09-18T09:20:58.944506Z","iopub.status.idle":"2021-09-18T09:20:59.88525Z","shell.execute_reply.started":"2021-09-18T09:20:58.944468Z","shell.execute_reply":"2021-09-18T09:20:59.884136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#处理性别变量\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n \nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:20:59.886825Z","iopub.execute_input":"2021-09-18T09:20:59.887062Z","iopub.status.idle":"2021-09-18T09:21:00.860712Z","shell.execute_reply.started":"2021-09-18T09:20:59.887037Z","shell.execute_reply":"2021-09-18T09:21:00.859518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targ = app_train['TARGET'] #save the column \"TARGET\"\n \n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n \n# Add the target back to the train data\napp_train['TARGET'] = targ\n \nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:00.863701Z","iopub.execute_input":"2021-09-18T09:21:00.863949Z","iopub.status.idle":"2021-09-18T09:21:01.227518Z","shell.execute_reply.started":"2021-09-18T09:21:00.863923Z","shell.execute_reply":"2021-09-18T09:21:01.226522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Even though we found some null data in the train data, we can still use correlations to select appropriate variables\n# Find correlations beween variables with the target \n# and rank them in order to pick up variables relatively highly related to the response\n\ncorrelations = app_train.corr()['TARGET'].sort_values()\n \n# print those correlation coefficient\nprint('Variables with TOP 15 Positive Correlation coefficient:\\n', correlations.tail(15))\nprint('\\nVariables with TOP 15 Negative Correlation coefficient:\\n', correlations.head(15))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:01.2292Z","iopub.execute_input":"2021-09-18T09:21:01.229561Z","iopub.status.idle":"2021-09-18T09:21:52.705254Z","shell.execute_reply.started":"2021-09-18T09:21:01.229521Z","shell.execute_reply":"2021-09-18T09:21:52.704095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a new dataframe for variables selected before, i.e. the polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH','TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n \n# To avoid those null data affect the regression model, we need to replace them with appropriate values    \n# We adopt the function ---- imputer to do that\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = 'median')\n \n# Similarly, we leave the column \"TARGET\" unaffected    \npoly_target = poly_features['TARGET']\npoly_features_new = poly_features.drop(columns = ['TARGET'])\n \n# impute missing values in both train and test data\npoly_features_new = imputer.fit_transform(poly_features_new)\npoly_features_test_new = imputer.transform(poly_features_test)\n \nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:52.707121Z","iopub.execute_input":"2021-09-18T09:21:52.707444Z","iopub.status.idle":"2021-09-18T09:21:53.021496Z","shell.execute_reply.started":"2021-09-18T09:21:52.707405Z","shell.execute_reply":"2021-09-18T09:21:53.020651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the significance of poly features and check whether each of them should be in the regression\nVersion_1 = sm.add_constant(poly_features_new)\nest = sm.Logit(poly_target , Version_1).fit()\nest.summary().tables[1]","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:53.025912Z","iopub.execute_input":"2021-09-18T09:21:53.026266Z","iopub.status.idle":"2021-09-18T09:21:55.625482Z","shell.execute_reply.started":"2021-09-18T09:21:53.026214Z","shell.execute_reply":"2021-09-18T09:21:55.624582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now that all of variables mentioned above are appropriate, we try to build a regression model\nregr = skl_lm.LogisticRegression()\nregr.fit(poly_features_new, poly_target)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:55.640441Z","iopub.execute_input":"2021-09-18T09:21:55.640938Z","iopub.status.idle":"2021-09-18T09:21:56.329132Z","shell.execute_reply.started":"2021-09-18T09:21:55.640897Z","shell.execute_reply":"2021-09-18T09:21:56.328024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = regr.predict(poly_features_new)\ncm_log = confusion_matrix(poly_target, pred)\ncm_df = pd.DataFrame(cm_log.T, index=regr.classes_,\n                     columns=regr.classes_)\ncm_df.index.name = 'Predicted'\ncm_df.columns.name = 'True'\nprint(cm_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:56.330698Z","iopub.execute_input":"2021-09-18T09:21:56.340886Z","iopub.status.idle":"2021-09-18T09:21:56.858515Z","shell.execute_reply.started":"2021-09-18T09:21:56.340808Z","shell.execute_reply":"2021-09-18T09:21:56.857605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_p = regr.predict_proba(poly_features_new)\ncm_df = pd.DataFrame({'True': poly_target, 'Pred': pred_p[:,1] > .2})\ncm_df.Pred.replace(to_replace={True:'Yes', False:'No'}, inplace=True)\nprint(cm_df.groupby(['True', 'Pred']).size().unstack('True').T)\nprint(classification_report(poly_target, pred_p[:,1] > .2))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:22:23.17092Z","iopub.execute_input":"2021-09-18T09:22:23.171237Z","iopub.status.idle":"2021-09-18T09:22:23.745905Z","shell.execute_reply.started":"2021-09-18T09:22:23.171206Z","shell.execute_reply":"2021-09-18T09:22:23.744435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features_new)\n \n# Transform the features\npoly_features_trans = poly_transformer.transform(poly_features_new)\npoly_features_test_trans = poly_transformer.transform(poly_features_test_new)\nprint('Polynomial Features shape: ', poly_features_trans.shape)\npoly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:57.487749Z","iopub.execute_input":"2021-09-18T09:21:57.487959Z","iopub.status.idle":"2021-09-18T09:21:57.636455Z","shell.execute_reply.started":"2021-09-18T09:21:57.487936Z","shell.execute_reply":"2021-09-18T09:21:57.635376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe of the features \npoly_features_DF = pd.DataFrame(poly_features_trans, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\npoly_features_test_DF = pd.DataFrame(poly_features_test_trans, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n \n# Add the target into the data\npoly_features_DF['TARGET'] = poly_target\n \n# Find the correlations with the target and print them\ncorrelations_2 = poly_features_DF.corr()['TARGET'].sort_values()\n \nprint(correlations_2.head(15))\nprint(correlations_2.tail(15))\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:57.637858Z","iopub.execute_input":"2021-09-18T09:21:57.638104Z","iopub.status.idle":"2021-09-18T09:21:58.878624Z","shell.execute_reply.started":"2021-09-18T09:21:57.638076Z","shell.execute_reply":"2021-09-18T09:21:58.87764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only choose variables with a correlation coefficient more than 0.150000\ntry_set = correlations_2.head(10)._stat_axis.values.tolist() + correlations_2.tail(5)._stat_axis.values.tolist()\ntry_set","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:58.879942Z","iopub.execute_input":"2021-09-18T09:21:58.880156Z","iopub.status.idle":"2021-09-18T09:21:58.889361Z","shell.execute_reply.started":"2021-09-18T09:21:58.880132Z","shell.execute_reply":"2021-09-18T09:21:58.888284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try_set2=try_set[:-2]\ntry_set2","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:58.890819Z","iopub.execute_input":"2021-09-18T09:21:58.891102Z","iopub.status.idle":"2021-09-18T09:21:58.901216Z","shell.execute_reply.started":"2021-09-18T09:21:58.891074Z","shell.execute_reply":"2021-09-18T09:21:58.900504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculate the significance of candidates of new features\nVersion_2 = sm.add_constant(poly_features_DF[try_set2])\nest2 = sm.Logit(poly_target , Version_2).fit()\nest2.summary().tables[1]","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:21:58.902385Z","iopub.execute_input":"2021-09-18T09:21:58.902962Z","iopub.status.idle":"2021-09-18T09:22:01.903684Z","shell.execute_reply.started":"2021-09-18T09:21:58.902928Z","shell.execute_reply":"2021-09-18T09:22:01.902781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#挑选出有用的参数并再计算验证\nVersion_3=poly_features_DF[['EXT_SOURCE_2 EXT_SOURCE_3','EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3',\n                               'EXT_SOURCE_2 EXT_SOURCE_3^2','EXT_SOURCE_1 EXT_SOURCE_3',\n                               'EXT_SOURCE_2','EXT_SOURCE_3','EXT_SOURCE_1 EXT_SOURCE_2 DAYS_BIRTH',\n                               'EXT_SOURCE_2 DAYS_BIRTH','EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH']]\nest3 = sm.Logit(poly_target , Version_3).fit()\nest3.summary().tables[1]","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:22:01.905113Z","iopub.execute_input":"2021-09-18T09:22:01.905767Z","iopub.status.idle":"2021-09-18T09:22:04.933884Z","shell.execute_reply.started":"2021-09-18T09:22:01.905721Z","shell.execute_reply":"2021-09-18T09:22:04.933013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regr2 = skl_lm.LogisticRegression()\nregr2.fit(Version_3, poly_target)\n\npred2 = regr2.predict(Version_3)\ncm_log2 = confusion_matrix(poly_target, pred2)\ncm_df2 = pd.DataFrame(cm_log2.T, index=regr2.classes_,\n                     columns=regr2.classes_)\ncm_df2.index.name = 'Predicted'\ncm_df2.columns.name = 'True'\nprint(cm_df2)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:22:04.935271Z","iopub.execute_input":"2021-09-18T09:22:04.935799Z","iopub.status.idle":"2021-09-18T09:22:07.624791Z","shell.execute_reply.started":"2021-09-18T09:22:04.935758Z","shell.execute_reply":"2021-09-18T09:22:07.623624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set 0.2 as a standard to classify the prediction\npred_p2 = regr2.predict_proba(Version_3)\ncm_df2 = pd.DataFrame({'True': poly_target, 'Pred': pred_p2[:,1] > .2})\ncm_df2.Pred.replace(to_replace={True:'Yes', False:'No'}, inplace=True)\nprint(cm_df2.groupby(['True', 'Pred']).size().unstack('True').T)\nprint(classification_report(poly_target, pred_p2[:,1] > .2))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:22:07.626417Z","iopub.execute_input":"2021-09-18T09:22:07.626737Z","iopub.status.idle":"2021-09-18T09:22:08.176658Z","shell.execute_reply.started":"2021-09-18T09:22:07.626699Z","shell.execute_reply":"2021-09-18T09:22:08.175487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ROC curve of Logistic Regression1\nfalse_pos_rate1, true_pos_rate1, _ = roc_curve(poly_target, pred_p[:,1])\nroc_auc1 = auc(false_pos_rate1, true_pos_rate1)\n\n# ROC curve of Logistic Regression2\nfalse_pos_rate2, true_pos_rate2, _ = roc_curve(poly_target, pred_p2[:,1])\nroc_auc2 = auc(false_pos_rate2, true_pos_rate2)\n\n\nfig, (ax1) = plt.subplots(1,  figsize=(10,10))\nax1.plot(false_pos_rate1, true_pos_rate1, label='Logistic Regression1(area = %0.2f)' % roc_auc1, color='b')\nax1.plot(false_pos_rate2, true_pos_rate2, label='Logistic Regression2(area = %0.2f)' % roc_auc2, color='r')\nax1.set_title('Dataset')\n\n\n\nfor ax in fig.axes:\n    ax.plot([0, 1], [0, 1], 'k--')\n    ax.set_xlim([-0.05, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend(loc=\"lower right\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:22:08.17843Z","iopub.execute_input":"2021-09-18T09:22:08.178763Z","iopub.status.idle":"2021-09-18T09:22:08.630551Z","shell.execute_reply.started":"2021-09-18T09:22:08.178726Z","shell.execute_reply":"2021-09-18T09:22:08.629851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain the final result\npred3 =regr2.predict_proba(poly_features_test_DF[['EXT_SOURCE_2 EXT_SOURCE_3',\n                                         'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3',\n                                         'EXT_SOURCE_2 EXT_SOURCE_3^2',\n                                         'EXT_SOURCE_1 EXT_SOURCE_3',\n                                         'EXT_SOURCE_2','EXT_SOURCE_3',\n                                         'EXT_SOURCE_1 EXT_SOURCE_2 DAYS_BIRTH',\n                                         'EXT_SOURCE_2 DAYS_BIRTH'\n                                         ,'EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH']])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:22:08.633702Z","iopub.execute_input":"2021-09-18T09:22:08.634457Z","iopub.status.idle":"2021-09-18T09:22:08.650093Z","shell.execute_reply.started":"2021-09-18T09:22:08.634419Z","shell.execute_reply":"2021-09-18T09:22:08.649089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nresult = pd.DataFrame({'SK_ID_CURR': app_test['SK_ID_CURR'], 'TARGET': pred3[:,1]})\n\nresult","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:22:08.656043Z","iopub.execute_input":"2021-09-18T09:22:08.656834Z","iopub.status.idle":"2021-09-18T09:22:08.672303Z","shell.execute_reply.started":"2021-09-18T09:22:08.656778Z","shell.execute_reply":"2021-09-18T09:22:08.671236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the data for submission\nresult.to_csv('result.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T09:22:08.673826Z","iopub.execute_input":"2021-09-18T09:22:08.675022Z","iopub.status.idle":"2021-09-18T09:22:08.921943Z","shell.execute_reply.started":"2021-09-18T09:22:08.674976Z","shell.execute_reply":"2021-09-18T09:22:08.921045Z"},"trusted":true},"execution_count":null,"outputs":[]}]}