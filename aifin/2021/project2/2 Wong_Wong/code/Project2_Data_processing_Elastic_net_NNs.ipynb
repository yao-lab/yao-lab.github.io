{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPDGj7u5Z3yM"
   },
   "outputs": [],
   "source": [
    "############ If you are not running the codes in Colab but in your own device, you can skip the first three cells\n",
    "# Code to read csv file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yeuKQpWto1d"
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92\n",
    "## We have two datasets and we download them into Colab\n",
    "\n",
    "id = '16NQGuP1S5NVZdCzy8BAdzsrdHzvJucOD'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('GKX_20201231.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uP8pMm3kacf"
   },
   "outputs": [],
   "source": [
    "id2 = '1YmTrkE_CSAe4D7StdupnTdrhBBUHCtIp'\n",
    "downloaded = drive.CreateFile({'id':id2}) \n",
    "downloaded.GetContentFile('PredictorData2020.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8pMdt-VEoYgb"
   },
   "outputs": [],
   "source": [
    "## Restart runtime in Colab at this point to release memory before running the below codes\n",
    "## Make sure the two data files have been saved in Colab or in the directory\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "import math\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import warnings\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDJ37SgCtoM-"
   },
   "outputs": [],
   "source": [
    "# Import the data set, either from local directory or Colab space\n",
    "df = pd.read_csv('GKX_20201231.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17349,
     "status": "ok",
     "timestamp": 1635930443142,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "S1eDXQjmhQin",
    "outputId": "a63908a5-8a2e-4550-cf84-ada5b1b07268"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "####################### 1. Data Processing #############################\n",
    "########################################################################\n",
    "# The dimension & column names\n",
    "print(df.shape)\n",
    "df.columns\n",
    "\n",
    "# All columns are either numeric or dummy variables for different categories\n",
    "print(df.select_dtypes('object').shape[1])\n",
    "\n",
    "# We note the data actually start from 1926\n",
    "# while the paper used data from Mar-1957\n",
    "\n",
    "# We filter out data earlier than Feb-1957 and later than Dec-2016\n",
    "# Feb-1957 is kept for first return calculation\n",
    "df = df[(df['DATE'] > 19570228)].reset_index().iloc[:,1:]\n",
    "df = df[(df['DATE'] // 10000 < 2017)].reset_index().iloc[:,1:]\n",
    "\n",
    "# Transform the date format of the stock data set into yyyymm\n",
    "df['DATE'] = df['DATE'] // 100\n",
    "df = df.rename(columns={'DATE': 'yyyymm'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dk7K3StUilAI"
   },
   "outputs": [],
   "source": [
    "# We know that among the 101 columns, 94 are stock level characteristics\n",
    "# The sic2 column corresponds to the industry variable\n",
    "# The other six columns are not used for prediction, according to the appendix of the paper\n",
    "for i in range(0,len(df.columns)):\n",
    "    if 10*i < len(df.columns):\n",
    "        print(df.columns.sort_values()[(10*i):(min(len(df.columns),10*(i+1)))])\n",
    "\n",
    "## The six columns are ['DATE','RET','SHROUT','mve0','prc','permno']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1635930460603,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "W88OKul4jekg",
    "outputId": "83a658ce-c254-4ac1-ae21-3fe92ab11caa"
   },
   "outputs": [],
   "source": [
    "############## Import the other dataset mentioned by the author from Welch and Goyal(2008)\n",
    "\n",
    "# This contains the macroeconomic factors he constructed\n",
    "# https://sites.google.com/view/agoyal145\n",
    "# https://drive.google.com/file/d/1ACbhdnIy0VbCWgsnXkjcddiV8HF4feWv/view\n",
    "df2 = pd.read_csv(\"PredictorData2020.csv\",sep=\",\")\n",
    "df2 = df2[df2['yyyymm']>=195703].reset_index().iloc[:,1:]\n",
    "\n",
    "# Note the index was stored as object\n",
    "df2.dtypes\n",
    "# First remove the \",\" in the index level stored as string\n",
    "for i in range(0,len(df2['Index'])):\n",
    "    df2['Index'][i] = df2['Index'][i].replace(',','')\n",
    "# Then change the index level back to float\n",
    "df2['Index'] = df2['Index'].apply(float)\n",
    "df2.dtypes\n",
    "    \n",
    "# Compute dividend to price ratio\n",
    "df2['dp_SnP'] = df2['D12'].apply(math.log) - df2['Index'].apply(math.log)\n",
    "# Compute earnings to price ratio\n",
    "df2['ep_SnP'] = df2['E12'].apply(math.log) - df2['Index'].apply(math.log)\n",
    "# Compute the default yield spread\n",
    "df2['dfy'] = df2['BAA'] - df2['AAA']\n",
    "# Term spread = Long term yield - t-bill rate\n",
    "df2['tms'] = df2['lty'] - df2['tbl']\n",
    "\n",
    "# Rename the book-to-market ratio since there is another variable of the exact same name in the core dataset\n",
    "df2 = df2.rename(columns={'b/m': 'bm_DJI'})\n",
    "\n",
    "# Keep only the eight variables needed\n",
    "lst = ['yyyymm','dp_SnP','ep_SnP','bm_DJI','ntis','tbl','tms','dfy','svar']\n",
    "df2 = df2[lst]\n",
    "# Force the date to be integer values\n",
    "df2['yyyymm'] = df2['yyyymm'].astype('int')\n",
    "#df2.head()\n",
    "#df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37RkvvFzmFJf"
   },
   "outputs": [],
   "source": [
    "## We join the two datasets by the date column.ie yyyymm\n",
    "\n",
    "# Create a column to store the values\n",
    "df['tbl'] = np.zeros(df.shape[0])\n",
    "df['dp_SnP'] = np.zeros(df.shape[0])\n",
    "df['ep_SnP'] = np.zeros(df.shape[0])\n",
    "df['bm_DJI'] = np.zeros(df.shape[0])\n",
    "df['ntis'] = np.zeros(df.shape[0])\n",
    "df['tms'] = np.zeros(df.shape[0])\n",
    "df['dfy'] = np.zeros(df.shape[0])\n",
    "df['svar'] = np.zeros(df.shape[0])\n",
    "\n",
    "# Store the number of months\n",
    "period_list = np.unique(df['yyyymm'])\n",
    "num_months = len(period_list)\n",
    "\n",
    "current_row = 0\n",
    "for period in period_list:\n",
    "    #Store the number stocks with data in the period\n",
    "    n_period = df[df['yyyymm']==period].shape[0]\n",
    "    #Record the position of the macro variable in the period\n",
    "    tmp = df2[df2['yyyymm']==period].index[0]\n",
    "\n",
    "    # fill in the macro variables for the stock at that period\n",
    "    # Since the data have been sorted, we can assign values by multiple rows at a time\n",
    "    df.loc[current_row:(current_row+n_period-1),'tbl'] = float(df2.loc[tmp,'tbl'])\n",
    "    df.loc[current_row:(current_row+n_period-1),'dp_SnP'] = float(df2.loc[tmp,'dp_SnP'])\n",
    "    df.loc[current_row:(current_row+n_period-1),'ep_SnP'] = float(df2.loc[tmp,'ep_SnP'])\n",
    "    df.loc[current_row:(current_row+n_period-1),'bm_DJI'] = float(df2.loc[tmp,'bm_DJI'])\n",
    "    df.loc[current_row:(current_row+n_period-1),'ntis'] = float(df2.loc[tmp,'ntis'])\n",
    "    df.loc[current_row:(current_row+n_period-1),'tms'] = float(df2.loc[tmp,'tms'])\n",
    "    df.loc[current_row:(current_row+n_period-1),'dfy'] = float(df2.loc[tmp,'dfy'])\n",
    "    df.loc[current_row:(current_row+n_period-1),'svar'] = float(df2.loc[tmp,'svar'])\n",
    "    \n",
    "    current_row = current_row + n_period\n",
    "  \n",
    "\n",
    "del(df2)\n",
    "gc.collect()\n",
    "\n",
    "# After merging compute the excess return\n",
    "df['excess_return'] = df['RET'] - df['tbl']       \n",
    "\n",
    "#print(df[['permno','yyyymm','tbl','excess_return','svar']].head())\n",
    "#df[['permno','yyyymm','tbl','excess_return','svar']].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1635930675438,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "L5bGZpxlnUJt",
    "outputId": "1d942211-dd7b-4de0-e3bd-0e4bc61ba082"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "# Note the df has 107 float variables and 3 integer variable\n",
    "print(df.dtypes[df.dtypes=='int64'])\n",
    "len(df.dtypes[df.dtypes=='float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z73x81XophYN",
    "outputId": "1430af51-c4b0-42e1-a7f1-1b2207d5b059"
   },
   "outputs": [],
   "source": [
    "# Here we check the ranges of the variables,\n",
    "# because later we want to save memory in later parts\n",
    "# We are trading off precision via converting float64 to float32, int64 to int32\n",
    "lst = df.columns\n",
    "arith_range_lst = np.max(df[lst])-np.min(df[lst])\n",
    "print(arith_range_lst.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64300,
     "status": "ok",
     "timestamp": 1635930758418,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "VO0nZsm7qncm",
    "outputId": "ef512b32-c459-4199-963c-bcd48bcb1dd9"
   },
   "outputs": [],
   "source": [
    "int_var = ['permno','yyyymm','SHROUT']\n",
    "float_var = [x for x in df.columns if x not in int_var]\n",
    "\n",
    "# Convert these two to in32\n",
    "df[['permno','yyyymm']] = df[['permno','yyyymm']].astype('uint32')\n",
    "\n",
    "# Since float32 covers the range of these variables well, we convert it from float64\n",
    "for col in float_var:\n",
    "    df[col] = df[col].astype('float32')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKOIpM7sq-2X"
   },
   "outputs": [],
   "source": [
    "print(np.max(df['mvel1']))\n",
    "print(np.max(df['ill']))\n",
    "print(np.min(df['ill']))\n",
    "df.dtypes\n",
    "print(np.max(df['yyyymm']))\n",
    "print(np.max(df['yyyymm']))\n",
    "print(np.min(df['yyyymm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpYh-aiqyEgR"
   },
   "outputs": [],
   "source": [
    "#### In this part we will fill the missing values of each of the 94 stock characteristics with cross-sectional median\n",
    "\n",
    "macro = ['dp_SnP','ep_SnP','bm_DJI','ntis','tbl','tms','dfy','svar']\n",
    "non_predictor = ['yyyymm','RET','SHROUT','mve0','prc','permno','excess_return']\n",
    "industry = ['sic2']\n",
    "lst = macro + non_predictor + industry\n",
    "# Get the 94 column names of the stock characteristics\n",
    "stock_char = [x for x in df.columns if x not in lst]\n",
    "\n",
    "#Store the distinct numbers of period\n",
    "period_list = np.unique(df['yyyymm'])\n",
    "len(stock_char)\n",
    "len(period_list)\n",
    "\n",
    "# We know that for some variables in some period, all values may be missing for all\n",
    "## stocks\n",
    "# We thus filter out this anticipated warnings\n",
    "warnings.filterwarnings(action='ignore', message='All-NaN slice encountered')\n",
    "\n",
    "## Note that this step takes relatively long as it loops through all \n",
    "## data point and do assignment in case of missing values\n",
    "## Around 20-30 minutes on Colab is needed\n",
    "\n",
    "for char in stock_char:\n",
    "    current_row = 0\n",
    "    for period in period_list:\n",
    "        # Store the number of stocks in the period\n",
    "        n_period = df[df['yyyymm']==period].shape[0]\n",
    "        # Get the cross-sectional median of this stock characteristic\n",
    "        crx_median_char = np.nanmedian(df.loc[current_row:(current_row+n_period-1),char])\n",
    "        # Replace missing values with median\n",
    "        df.loc[current_row:(current_row+n_period-1),char] = df.loc[current_row:(current_row+n_period-1),char].replace(np.nan,crx_median_char)\n",
    "        current_row = current_row + n_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EToO7J34WF8"
   },
   "outputs": [],
   "source": [
    "## Check for missing values after the above process\n",
    "## For some variables, some have missing values for all stock in a period\n",
    "for char in df.columns:\n",
    "    print(len(df[df[char].isnull()].index.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21379,
     "status": "ok",
     "timestamp": 1635932588295,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "s6fepskP2izz",
    "outputId": "945388bc-3e27-4cf8-83b8-9cec873dc9e6"
   },
   "outputs": [],
   "source": [
    "####################### Handling Industries Dummy Variables####################\n",
    "# Taking the first two digits of SIC2 for industry\n",
    "df['sic2'] = df['sic2'].apply(np.floor)\n",
    "\n",
    "# Check whether there are 74 industries as said in the paper\n",
    "#df.groupby('sic2').count().iloc[:,0:1] \n",
    "df['sic2'] = df['sic2'].astype(str)\n",
    "len(np.unique(df['sic2']))\n",
    "np.unique(df['sic2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpRq_O5X4vs0"
   },
   "outputs": [],
   "source": [
    "# Convert the industries to dummies variable\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2624,
     "status": "ok",
     "timestamp": 1635932683210,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "fJLNZ0jXGRc1",
    "outputId": "3599d926-458c-4247-cdc3-ed81ba803bbf"
   },
   "outputs": [],
   "source": [
    "## Note that the lags have already been adjusted in the data set provided\n",
    "## we will not need any shifting of observations\n",
    "\n",
    "# Drop the variables that will no longer be used\n",
    "df = df.drop(columns=['RET','prc','SHROUT','mve0'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fOiHOkc_rxT"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"df_merge_fill.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d3aPIpl-E3s"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_merge_fill.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1635915968769,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "E7BcTeGsFzxk",
    "outputId": "5063d515-5c0c-4356-96de-29391def930c"
   },
   "outputs": [],
   "source": [
    "macro = ['dp_SnP','ep_SnP','bm_DJI','ntis','tbl','tms','dfy','svar']\n",
    "non_predictor = ['yyyymm','RET','SHROUT','mve0','prc','permno','excess_return']\n",
    "industry = ['sic2']\n",
    "lst = macro + non_predictor + industry\n",
    "df.shape\n",
    "stock_char = [x for x in df.columns[:105] if x not in lst]\n",
    "len(stock_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccSR0nVWOxJW"
   },
   "outputs": [],
   "source": [
    "## As said in the paper\n",
    "# we normalize the stock data by the cross-sectional mean and variance\n",
    "# before we put them into neural network training\n",
    "# The macro variable is the same in one cross-section, so we skip this for the 8 macros\n",
    "# The process takes long (about 20-30 mins)\n",
    "\n",
    "period_list = np.unique(df['yyyymm'])\n",
    "len(period_list)\n",
    "\n",
    "for char in stock_char:\n",
    "    current_row = 0\n",
    "    for period in period_list:\n",
    "        # Store the number of stocks in the period\n",
    "        n_period = df[df['yyyymm']==period].shape[0]\n",
    "        # Get the cross-sectional mean and sd of this stock characteristic\n",
    "        crx_mean_char = np.nanmean(df.loc[current_row:(current_row+n_period-1),char])\n",
    "        crx_std_char = np.nanstd(df.loc[current_row:(current_row+n_period-1),char])\n",
    "        \n",
    "        # De-mean and re-scale the data by cross-sectional mean and sd\n",
    "        # Only do so for a cross section with non-missing values\n",
    "        if (not(np.isnan(crx_mean_char) or np.isnan(crx_std_char))):\n",
    "            df.loc[current_row:(current_row+n_period-1),char] = (df.loc[current_row:(current_row+n_period-1),char] - crx_mean_char)/crx_std_char\n",
    "            current_row = current_row + n_period\n",
    "\n",
    "#np.nanmean(df.loc[2000:200000,'bm_ia'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_merge_fill_demean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_merge_fill_demean.csv\",sep=',')\n",
    "#df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1635932646088,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "sg76JXeWLPeA",
    "outputId": "b09c0d90-2292-4fdc-fde2-8c9eb6601932"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3760033, 180)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "#df.dtypes\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "##################################### 5. Neural Network#############################################\n",
    "####################################################################################################\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "use_gpu = False\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1635932805673,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "VNOM8YB7WTv7",
    "outputId": "523e0657-164d-45fc-9e9c-f5211649b034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN1(\n",
      "  (fc1): Linear(in_features=177, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (output): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the NN1\n",
    "class NN1(Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NN1, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_size)\n",
    "        self.output = nn.Linear(self.hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn1(relu)\n",
    "        output = self.output(bn_x)\n",
    "        return output\n",
    "\n",
    "net = NN1(input_size=177, hidden_size=32).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "id": "gQkDTVhgeE5r"
   },
   "outputs": [],
   "source": [
    "# Define the dataset the pytorch\n",
    "class stock_data(Dataset):\n",
    "    def __init__(self, stockdata, predictor_col, mode = 'train'):\n",
    "        self.mode = mode\n",
    "        ## During pre-processing of data, missing values have been replaced by cross-sectional median\n",
    "        ## If they are still missing, this means the variable has missing values in the cross-section for all stocks\n",
    "        ## So we take such observations as zero\n",
    "        stockdata_fill = stockdata.fillna(0)\n",
    "        del(stockdata)\n",
    "        if self.mode == 'train':\n",
    "            self.inp = stockdata_fill.loc[:,predictor_col].values\n",
    "            self.oup = stockdata_fill.loc[:,'excess_return'].values\n",
    "        else:\n",
    "            self.inp = stockdata_fill.loc[:,predictor_col].values\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            inpt  = torch.from_numpy(np.array(self.inp[idx]).astype('float32'))\n",
    "            oupt  = torch.from_numpy(np.array(self.oup[idx]).astype('float32'))\n",
    "            return { 'inp': inpt,\n",
    "                     'oup': oupt,\n",
    "            }\n",
    "        else:\n",
    "            inpt = torch.from_numpy(np.array(self.inp[idx]))\n",
    "            return { 'inp': inpt\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictor list\n",
    "lst = ['yyyymm','permno','excess_return']\n",
    "predictor_col = [x for x in df.columns if x not in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_train_end = 1957+18\n",
    "year_valid_end = 1957+18+12\n",
    "year_test_start = 1987\n",
    "data = stock_data(stockdata=df[df['yyyymm']//100 < year_train_end], predictor_col=predictor_col)\n",
    "BATCH_SIZE = 10000\n",
    "data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the validation data set\n",
    "dataset_valid = stock_data(stockdata=df[(df['yyyymm']//100 >= year_train_end) & (df['yyyymm']//100 < year_valid_end)], predictor_col=predictor_col)\n",
    "data_valid = DataLoader(dataset = dataset_valid, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "del dataset_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the testing data set\n",
    "dataset_test = stock_data(stockdata=df[(df['yyyymm']//100 >= year_test_start) & (df['yyyymm']//100 < year_test_start+1)], predictor_col=predictor_col)\n",
    "data_test = DataLoader(dataset = dataset_test, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "del dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### This is a trial of one-snapshot feature importance\n",
    "\n",
    "year1 = 1957\n",
    "year2 = 2004\n",
    "index_start = np.min(df[df['yyyymm']//100 >=year1].index.to_list())\n",
    "index_end = np.max(df[df['yyyymm']//100 <=year2].index.to_list())\n",
    "print(index_start)\n",
    "index_end\n",
    "\n",
    "net = NN1(input_size=177, hidden_size=32).to(device)\n",
    "net.load_state_dict(torch.load(\"NN1_2016_end.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optimizer, criterion, lambda_reg):\n",
    "    model.zero_grad()\n",
    "    output = model(x)\n",
    "    loss =criterion(output,y)\n",
    "    l1_norm = sum(abs(p).sum() for p in model.parameters())\n",
    "    loss += lambda_reg * l1_norm\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, output\n",
    "\n",
    "def valid(model, x_valid, y_valid):\n",
    "    output = model(x_valid)\n",
    "    error = output.view(-1,1) - y_valid.view(-1,1)\n",
    "    sum_resid_error = torch.sum(error*error)\n",
    "    sum_sq = torch.sum(y_valid*y_valid)\n",
    "    return sum_resid_error, sum_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the general training function for our neural networks\n",
    "\n",
    "def train_model(model, data_train, data_valid, learning_rate, l1_lambda, epsilon, printout):\n",
    "    #Parameters/setting that will be fixed as with the paper\n",
    "    EPOCHS = 100\n",
    "    optimizer = Adam(model.parameters(),lr=learning_rate)\n",
    "    criterion = MSELoss()\n",
    "    patience = 5\n",
    "    \n",
    "    # The variable for detecting early-stopping\n",
    "    ## Initialize the variable to store the best validation R_2 in training and the number of times no improvement \n",
    "    ## above threshold - epsilon\n",
    "    j = 0\n",
    "    best_r_sq_up_to_now = -500000\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for bidx, batch in enumerate(data_train):\n",
    "            x_train, y_train = batch['inp'], batch['oup']\n",
    "            x_train = x_train.view(-1,177)\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.view(-1,1)\n",
    "            y_train = y_train.to(device)\n",
    "            loss, predictions = train(model,x_train,y_train, optimizer, criterion, l1_lambda)\n",
    "            epoch_loss+=loss\n",
    "        if (printout == True):\n",
    "            print('Epoch {} : Training MSE Loss : {}'.format((epoch+1),epoch_loss))\n",
    "        \n",
    "        running_sum_sq_error = 0\n",
    "        running_sum_sq = 0\n",
    "        for bidx, batch in enumerate(data_valid):\n",
    "            x_valid, y_valid  = batch['inp'], batch['oup']\n",
    "            x_valid = x_valid.view(-1,177)\n",
    "            x_valid = x_valid.to(device)\n",
    "            y_valid = y_valid.view(-1,1)\n",
    "            y_valid = y_valid.to(device)\n",
    "            sum_resid_error, sum_sq = valid(model, x_valid, y_valid)\n",
    "            running_sum_sq += sum_sq\n",
    "            running_sum_sq_error += sum_resid_error\n",
    "        R_sq_oos = 1 - running_sum_sq_error/running_sum_sq\n",
    "        if (printout == True):\n",
    "            print('Epoch {} Validation R^2_oos : {}'.format((epoch+1),R_sq_oos))\n",
    "\n",
    "        if (R_sq_oos > best_r_sq_up_to_now):\n",
    "            if (R_sq_oos <= best_r_sq_up_to_now + epsilon):\n",
    "                best_r_sq_up_to_now = R_sq_oos\n",
    "                j += 1\n",
    "            else:\n",
    "                best_r_sq_up_to_now = R_sq_oos\n",
    "                j = 0\n",
    "        else:\n",
    "            j +=1\n",
    "        if (j >= patience):\n",
    "            if (printout == True): \n",
    "                print('Early stopping triggered and the best validation R^2: {}'.format(best_r_sq_up_to_now))\n",
    "            break\n",
    "    if (printout == True):\n",
    "        print('Trainin stopped after {} epochs'.format(epoch+1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "7j6okjWArTwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training MSE Loss : 2.2216711044311523\n",
      "Epoch 1 Validation R^2_oos : 0.008648812770843506\n",
      "Epoch 2 : Training MSE Loss : 1.6966581344604492\n",
      "Epoch 2 Validation R^2_oos : 0.059522032737731934\n",
      "Epoch 3 : Training MSE Loss : 1.5079433917999268\n",
      "Epoch 3 Validation R^2_oos : 0.0798293948173523\n",
      "Epoch 4 : Training MSE Loss : 1.4520869255065918\n",
      "Epoch 4 Validation R^2_oos : 0.09413975477218628\n",
      "Epoch 5 : Training MSE Loss : 1.3866275548934937\n",
      "Epoch 5 Validation R^2_oos : 0.10377782583236694\n",
      "Epoch 6 : Training MSE Loss : 1.2993717193603516\n",
      "Epoch 6 Validation R^2_oos : 0.12315541505813599\n",
      "Epoch 7 : Training MSE Loss : 1.2304725646972656\n",
      "Epoch 7 Validation R^2_oos : 0.1244896650314331\n",
      "Epoch 8 : Training MSE Loss : 1.1957405805587769\n",
      "Epoch 8 Validation R^2_oos : 0.12963330745697021\n",
      "Epoch 9 : Training MSE Loss : 1.1854984760284424\n",
      "Epoch 9 Validation R^2_oos : 0.12779444456100464\n",
      "Epoch 10 : Training MSE Loss : 1.152513027191162\n",
      "Epoch 10 Validation R^2_oos : 0.1329948902130127\n",
      "Epoch 11 : Training MSE Loss : 1.129888892173767\n",
      "Epoch 11 Validation R^2_oos : 0.1341206431388855\n",
      "Epoch 12 : Training MSE Loss : 1.0949336290359497\n",
      "Epoch 12 Validation R^2_oos : 0.13631653785705566\n",
      "Epoch 13 : Training MSE Loss : 1.0779838562011719\n",
      "Epoch 13 Validation R^2_oos : 0.13702481985092163\n",
      "Epoch 14 : Training MSE Loss : 1.0574018955230713\n",
      "Epoch 14 Validation R^2_oos : 0.13725900650024414\n",
      "Epoch 15 : Training MSE Loss : 1.0486392974853516\n",
      "Epoch 15 Validation R^2_oos : 0.1374310851097107\n",
      "Epoch 16 : Training MSE Loss : 1.0356109142303467\n",
      "Epoch 16 Validation R^2_oos : 0.13751107454299927\n",
      "Epoch 17 : Training MSE Loss : 1.0285289287567139\n",
      "Epoch 17 Validation R^2_oos : 0.1375333070755005\n",
      "Epoch 18 : Training MSE Loss : 1.0200729370117188\n",
      "Epoch 18 Validation R^2_oos : 0.1376338005065918\n",
      "Early stopping triggered and the best validation R^2: 0.1376338005065918\n",
      "Trainin stopped after 18 epochs\n"
     ]
    }
   ],
   "source": [
    "################## Specify the parameters for NN1\n",
    "\n",
    "### Training for NN1\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-4\n",
    "\n",
    "## We will regularize the model by early stopping with patience p = 5; epsilon = 0.01\n",
    "## That is, we will break the loop when the R-square in the validation set cannot improve by more than 0.005 in 5 epochs\n",
    "epsilon = 0.0005\n",
    "\n",
    "train_model(net, data_train, data_valid, learning_rate, l1_lambda, epsilon, printout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the test function\n",
    "def test(model, x_test, y_test):\n",
    "    output = model(x_test)\n",
    "    error = output.view(-1,1) - y_test.view(-1,1)\n",
    "    sum_resid_error = torch.sum(error*error)\n",
    "    sum_sq = torch.sum(y_test*y_test)\n",
    "    return sum_resid_error, sum_sq\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a general testing loop for all NN_i models, for i = 1,2, ...,5\n",
    "def test_model(model, data_test):\n",
    "    running_sum_sq_error = 0\n",
    "    running_sum_sq = 0\n",
    "    for bidx, batch in enumerate(data_test):\n",
    "        x_test, y_test  = batch['inp'], batch['oup']\n",
    "        x_test = x_test.view(-1,177)\n",
    "        x_test = x_test.to(device)\n",
    "        y_test = y_test.view(-1,1)\n",
    "        y_test = y_test.to(device)\n",
    "        sum_resid_error, sum_sq = test(model, x_test, y_test)\n",
    "        running_sum_sq += sum_sq\n",
    "        running_sum_sq_error += sum_resid_error\n",
    "    #print('R_sq_oos in the tested year:')\n",
    "    R_sq_oos = 1 - running_sum_sq_error/running_sum_sq\n",
    "    return R_sq_oos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"NN1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NN1(input_size=177, hidden_size=32).to(device)\n",
    "net.load_state_dict(torch.load(\"NN1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_sq_oos in the following year:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1176, grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(net, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN1_1987_end.pt\n",
      "NN1_1988_end.pt\n",
      "NN1_1989_end.pt\n",
      "NN1_1990_end.pt\n",
      "NN1_1991_end.pt\n",
      "NN1_1992_end.pt\n",
      "NN1_1993_end.pt\n",
      "NN1_1994_end.pt\n",
      "NN1_1995_end.pt\n",
      "NN1_1996_end.pt\n",
      "NN1_1997_end.pt\n",
      "NN1_1998_end.pt\n",
      "NN1_1999_end.pt\n",
      "NN1_2000_end.pt\n",
      "NN1_2001_end.pt\n",
      "NN1_2002_end.pt\n",
      "NN1_2003_end.pt\n",
      "NN1_2004_end.pt\n",
      "NN1_2005_end.pt\n",
      "NN1_2006_end.pt\n",
      "NN1_2007_end.pt\n",
      "NN1_2008_end.pt\n",
      "NN1_2009_end.pt\n",
      "NN1_2010_end.pt\n",
      "NN1_2011_end.pt\n",
      "NN1_2012_end.pt\n",
      "NN1_2013_end.pt\n",
      "NN1_2014_end.pt\n",
      "NN1_2015_end.pt\n",
      "NN1_2016_end.pt\n"
     ]
    }
   ],
   "source": [
    "# Define a recursive training loop for NN1\n",
    "BATCH_SIZE = 10000\n",
    "### Training for NN1\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-4\n",
    "epsilon = 0.0005\n",
    "\n",
    "for year in range(1975,2005):\n",
    "    data = stock_data(stockdata=df.loc[df['yyyymm']//100 <= year,:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    dataset_valid = stock_data(stockdata=df.loc[((df['yyyymm']//100 > year)&(df['yyyymm']//100 <= year+12)),:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_valid = DataLoader(dataset = dataset_valid, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    \n",
    "    net = NN1(input_size=177, hidden_size=32).to(device)\n",
    "    train_model(net, data_train, data_valid, learning_rate, l1_lambda, epsilon, printout=False)\n",
    "    del(data)\n",
    "    del(data_train)\n",
    "    del(dataset_valid)\n",
    "    del(data_valid)\n",
    "    gc.collect()\n",
    "    \n",
    "    filename = \"NN1_\" + str(year+12) + \"_end.pt\"\n",
    "    print(filename)\n",
    "    torch.save(net.state_dict(), filename)\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 1987 completed\n",
      "Year 1988 completed\n",
      "Year 1989 completed\n",
      "Year 1990 completed\n",
      "Year 1991 completed\n",
      "Year 1992 completed\n",
      "Year 1993 completed\n",
      "Year 1994 completed\n",
      "Year 1995 completed\n",
      "Year 1996 completed\n",
      "Year 1997 completed\n",
      "Year 1998 completed\n",
      "Year 1999 completed\n",
      "Year 2000 completed\n",
      "Year 2001 completed\n",
      "Year 2002 completed\n",
      "Year 2003 completed\n",
      "Year 2004 completed\n",
      "Year 2005 completed\n",
      "Year 2006 completed\n",
      "Year 2007 completed\n",
      "Year 2008 completed\n",
      "Year 2009 completed\n",
      "Year 2010 completed\n",
      "Year 2011 completed\n",
      "Year 2012 completed\n",
      "Year 2013 completed\n",
      "Year 2014 completed\n",
      "Year 2015 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09770667552947998,\n",
       " 0.08060711622238159,\n",
       " 0.1437341570854187,\n",
       " 0.11548066139221191,\n",
       " 0.002632617950439453,\n",
       " -0.022261857986450195,\n",
       " -0.11080455780029297,\n",
       " -0.013328194618225098,\n",
       " -0.11183226108551025,\n",
       " -0.08114683628082275,\n",
       " -0.040172457695007324,\n",
       " 0.0436328649520874,\n",
       " -0.009955883026123047,\n",
       " 0.05666869878768921,\n",
       " -0.0200042724609375,\n",
       " -0.04290151596069336,\n",
       " -0.2799196243286133,\n",
       " -0.12814664840698242,\n",
       " 0.04209405183792114,\n",
       " 0.04735046625137329,\n",
       " 0.10398834943771362,\n",
       " 0.09305596351623535,\n",
       " -0.07486248016357422,\n",
       " -0.16078686714172363,\n",
       " -0.058098793029785156,\n",
       " -0.12744951248168945,\n",
       " -0.22547292709350586,\n",
       " -0.09148252010345459,\n",
       " -0.010153412818908691]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sq_list_NN1 = []\n",
    "#cum_feat_imp_NN1 = np.zeros(len(predictor_col))\n",
    "\n",
    "for year in range(1987,2016):\n",
    "    \n",
    "    net1 = NN1(input_size=177, hidden_size=32).to(device)\n",
    "    filename = \"NN1_\" + str(year) + \"_end.pt\"\n",
    "    net1.load_state_dict(torch.load(filename))\n",
    "    year_test_start = year\n",
    "    \n",
    "    # Note the coming 12 years of data are for validation but not for testing or model estimates\n",
    "    # So we test the model fitted with 1957-1974 data with 1987 data, and so on\n",
    "    dataset_test = stock_data(stockdata=df[((df['yyyymm']//100 >= year_test_start) & (df['yyyymm']//100 < year_test_start+1))], predictor_col=predictor_col)\n",
    "    data_test = DataLoader(dataset = dataset_test, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del dataset_test\n",
    "    \n",
    "    # Save the R_sq_oos for each year\n",
    "    R_sq_oos = test_model(net1, data_test).item()\n",
    "    R_sq_list_NN1.append(R_sq_oos)\n",
    "    del(data_test)\n",
    "    \n",
    "    # At the same time we find the feature importance of the net\n",
    "    \n",
    "    year_train_end = year-12\n",
    "    data = stock_data(stockdata=df[df['yyyymm']//100 < year_train_end], predictor_col=predictor_col)\n",
    "    BATCH_SIZE = 10000\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(data)\n",
    "    cum_feat_imp_NN4 += feat_imp_net(net4, data_train)\n",
    "    del(data_train)\n",
    "    print(\"Year {} completed\".format(year))\n",
    "    \n",
    "R_sq_list_NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.02695962067308097\n",
      "-0.0200042724609375\n",
      "0.1437341570854187\n",
      "0.02021492852105035\n",
      "0.002632617950439453\n",
      "0.1437341570854187\n"
     ]
    }
   ],
   "source": [
    "NN1_R2 = np.array(R_sq_list_NN1)\n",
    "print(np.mean(NN1_R2))\n",
    "print(np.median(NN1_R2))\n",
    "print(np.max(NN1_R2))\n",
    "\n",
    "print(np.mean(NN1_R2[0:9]))\n",
    "print(np.median(NN1_R2[0:9]))\n",
    "print(np.max(NN1_R2[0:9]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN2(\n",
      "  (fc1): Linear(in_features=177, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (output): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##### Define the class for NN2\n",
    "\n",
    "class NN2(Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2):\n",
    "        super(NN2, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1  = hidden_size_1\n",
    "        self.hidden_size_2  = hidden_size_2\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size_1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_size_1)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size_1, self.hidden_size_2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.hidden_size_2)\n",
    "        self.output = nn.Linear(self.hidden_size_2, 1)\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn1(relu)\n",
    "        hidden = self.fc2(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn2(relu)       \n",
    "        output = self.output(bn_x)\n",
    "        return output\n",
    "\n",
    "net2 = NN2(input_size=177, hidden_size_1=32, hidden_size_2=16).to(device)\n",
    "print(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training MSE Loss : 2.276771306991577\n",
      "Epoch 1 Validation R^2_oos : 0.015022873878479004\n",
      "Epoch 2 : Training MSE Loss : 1.2826963663101196\n",
      "Epoch 2 Validation R^2_oos : 0.031631529331207275\n",
      "Epoch 3 : Training MSE Loss : 1.0725053548812866\n",
      "Epoch 3 Validation R^2_oos : 0.0585179328918457\n",
      "Epoch 4 : Training MSE Loss : 1.032969355583191\n",
      "Epoch 4 Validation R^2_oos : 0.0776473879814148\n",
      "Epoch 5 : Training MSE Loss : 0.98606938123703\n",
      "Epoch 5 Validation R^2_oos : 0.09348207712173462\n",
      "Epoch 6 : Training MSE Loss : 0.9709440469741821\n",
      "Epoch 6 Validation R^2_oos : 0.09754437208175659\n",
      "Epoch 7 : Training MSE Loss : 0.9373044371604919\n",
      "Epoch 7 Validation R^2_oos : 0.10720998048782349\n",
      "Epoch 8 : Training MSE Loss : 0.9285129904747009\n",
      "Epoch 8 Validation R^2_oos : 0.10986757278442383\n",
      "Epoch 9 : Training MSE Loss : 0.9225953221321106\n",
      "Epoch 9 Validation R^2_oos : 0.11343425512313843\n",
      "Epoch 10 : Training MSE Loss : 0.9171926379203796\n",
      "Epoch 10 Validation R^2_oos : 0.11591613292694092\n",
      "Epoch 11 : Training MSE Loss : 0.9134923815727234\n",
      "Epoch 11 Validation R^2_oos : 0.11840593814849854\n",
      "Epoch 12 : Training MSE Loss : 0.9091255068778992\n",
      "Epoch 12 Validation R^2_oos : 0.120208740234375\n",
      "Epoch 13 : Training MSE Loss : 0.9055596590042114\n",
      "Epoch 13 Validation R^2_oos : 0.12154239416122437\n",
      "Epoch 14 : Training MSE Loss : 0.9020724296569824\n",
      "Epoch 14 Validation R^2_oos : 0.12231427431106567\n",
      "Epoch 15 : Training MSE Loss : 0.89853435754776\n",
      "Epoch 15 Validation R^2_oos : 0.1229599118232727\n",
      "Epoch 16 : Training MSE Loss : 0.8957542777061462\n",
      "Epoch 16 Validation R^2_oos : 0.12386202812194824\n",
      "Epoch 17 : Training MSE Loss : 0.8930536508560181\n",
      "Epoch 17 Validation R^2_oos : 0.12492775917053223\n",
      "Epoch 18 : Training MSE Loss : 0.8907253742218018\n",
      "Epoch 18 Validation R^2_oos : 0.12595713138580322\n",
      "Epoch 19 : Training MSE Loss : 0.8884017467498779\n",
      "Epoch 19 Validation R^2_oos : 0.12674576044082642\n",
      "Epoch 20 : Training MSE Loss : 0.8861491680145264\n",
      "Epoch 20 Validation R^2_oos : 0.12728554010391235\n",
      "Epoch 21 : Training MSE Loss : 0.8841347098350525\n",
      "Epoch 21 Validation R^2_oos : 0.12761682271957397\n",
      "Epoch 22 : Training MSE Loss : 0.8819680213928223\n",
      "Epoch 22 Validation R^2_oos : 0.12763762474060059\n",
      "Epoch 23 : Training MSE Loss : 0.8804358839988708\n",
      "Epoch 23 Validation R^2_oos : 0.12790662050247192\n",
      "Epoch 24 : Training MSE Loss : 0.8792911171913147\n",
      "Epoch 24 Validation R^2_oos : 0.12591439485549927\n",
      "Epoch 25 : Training MSE Loss : 0.8773343563079834\n",
      "Epoch 25 Validation R^2_oos : 0.1286299228668213\n",
      "Epoch 26 : Training MSE Loss : 0.8751996159553528\n",
      "Epoch 26 Validation R^2_oos : 0.12859368324279785\n",
      "Epoch 27 : Training MSE Loss : 0.8740649819374084\n",
      "Epoch 27 Validation R^2_oos : 0.12869149446487427\n",
      "Epoch 28 : Training MSE Loss : 0.8725475668907166\n",
      "Epoch 28 Validation R^2_oos : 0.12898123264312744\n",
      "Epoch 29 : Training MSE Loss : 0.8713589906692505\n",
      "Epoch 29 Validation R^2_oos : 0.12908726930618286\n",
      "Epoch 30 : Training MSE Loss : 0.8698766827583313\n",
      "Epoch 30 Validation R^2_oos : 0.12929844856262207\n",
      "Early stopping triggered and the best validation R^2: 0.12929844856262207\n",
      "Trainin stopped after 30 epochs\n"
     ]
    }
   ],
   "source": [
    "### Training for a single NN2 for a trial\n",
    "### The rationale is similar to that of NN1\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-5\n",
    "epsilon = 0.0005\n",
    "\n",
    "train_model(net2, data_train, data_valid, learning_rate, l1_lambda, epsilon, printout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_sq_oos in the following year:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09917950630187988"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(net2, data_test).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1787: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, val, pi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_sq_oos in the following year:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0797, grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Select one model to test for a trial\n",
    "net2 = NN2(input_size=177, hidden_size_1=32, hidden_size_2=16).to(device)\n",
    "net2.load_state_dict(torch.load(\"NN2_1988_end.pt\"))\n",
    "year_test_start = 1988\n",
    "dataset_test = stock_data(stockdata=df[(df.loc[:,'yyyymm']//100 >= year_test_start) & (df['yyyymm']//100 < year_test_start+1)], predictor_col=predictor_col)\n",
    "data_test = DataLoader(dataset = dataset_test, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "del dataset_test\n",
    "test_model(net2, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a recursive training loop for NN2\n",
    "BATCH_SIZE = 10000\n",
    "### Training for NN2\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-4\n",
    "epsilon = 0.0005\n",
    "\n",
    "# Here year represents \n",
    "for year in range(1974,2005):\n",
    "    data = stock_data(stockdata=df.loc[df['yyyymm']//100 < year,:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(data)\n",
    "    \n",
    "    dataset_valid = stock_data(stockdata=df.loc[((df['yyyymm']//100 >= year)&(df['yyyymm']//100 < year+12)),:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_valid = DataLoader(dataset = dataset_valid, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(dataset_valid)\n",
    "    \n",
    "    net = NN2(input_size=177, hidden_size_1=32, hidden_size_2=16).to(device)\n",
    "    train_model(net, data_train, data_valid, learning_rate, l1_lambda, epsilon, printout=False)\n",
    "    \n",
    "    del(data_train)\n",
    "    del(data_valid)\n",
    "    gc.collect()\n",
    "    \n",
    "    filename = \"NN2_\" + str(year+12) + \"_end.pt\"\n",
    "    print(filename)\n",
    "    torch.save(net.state_dict(), filename)\n",
    "    del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the feature importance\n",
    "\n",
    "def feat_imp_net(model, data_train):\n",
    "    \n",
    "    R_sq_loss_table = np.zeros(len(predictor_col))\n",
    "    \n",
    "    x = np.array(data_train.dataset.inp[:])\n",
    "    y = torch.from_numpy(np.array(data_train.dataset.oup[:]).astype('float32')).view(-1,1)\n",
    "    y = y.to(device)\n",
    "    x_o = torch.from_numpy(np.array(data_train.dataset.inp[:]).astype('float32')).to(device)\n",
    "    del(data_train)\n",
    "    gc.collect()\n",
    "\n",
    "    y_predict = model(x_o).view(-1,1)\n",
    "    a = y.view(-1,1) - y_predict\n",
    "    R_sq_full_model = 1 - sum(a*a)/sum(y*y)\n",
    "\n",
    "    for i in range(0,len(predictor_col)):\n",
    "        tmp = np.array(x[:,i])\n",
    "        x[:,i] = np.zeros(x.shape[0])\n",
    "        x_twisted = torch.from_numpy(x.astype('float32')).to(device)\n",
    "        y_predict = model(x_twisted).view(-1,1)\n",
    "        error = y - y_predict\n",
    "        R_sq_twisted = 1 - sum(error*error)/sum(y*y)\n",
    "        R_sq_loss_table[i] = R_sq_full_model-R_sq_twisted\n",
    "        # Restore the original values of predictor i\n",
    "        x[:,i] = tmp\n",
    "        if (i%80 == 0):\n",
    "            print(i)\n",
    "    #R_sq_loss_table = R_sq_loss_table/sum(R_sq_loss_table) #Do not use this since sometimes sum could be close to zero\n",
    "    \n",
    "    return R_sq_loss_table\n",
    "#zipped = zip(predictor_col,R_sq_loss_table)\n",
    "#list(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 1987 completed\n",
      "Year 1988 completed\n",
      "Year 1989 completed\n",
      "Year 1990 completed\n",
      "Year 1991 completed\n",
      "Year 1992 completed\n",
      "Year 1993 completed\n",
      "Year 1994 completed\n",
      "Year 1995 completed\n",
      "Year 1996 completed\n",
      "Year 1997 completed\n",
      "Year 1998 completed\n",
      "Year 1999 completed\n",
      "Year 2000 completed\n",
      "Year 2001 completed\n",
      "Year 2002 completed\n",
      "Year 2003 completed\n",
      "Year 2004 completed\n",
      "Year 2005 completed\n",
      "Year 2006 completed\n",
      "Year 2007 completed\n",
      "Year 2008 completed\n",
      "Year 2009 completed\n",
      "Year 2010 completed\n",
      "Year 2011 completed\n",
      "Year 2012 completed\n",
      "Year 2013 completed\n",
      "Year 2014 completed\n",
      "Year 2015 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09726208448410034,\n",
       " 0.07974183559417725,\n",
       " 0.14403396844863892,\n",
       " 0.11529743671417236,\n",
       " 0.0006988048553466797,\n",
       " -0.0067746639251708984,\n",
       " -0.046611905097961426,\n",
       " 0.04551297426223755,\n",
       " -0.10236501693725586,\n",
       " -0.07226836681365967,\n",
       " -0.10307204723358154,\n",
       " 0.024225950241088867,\n",
       " -0.02914905548095703,\n",
       " 0.056396543979644775,\n",
       " -0.011753320693969727,\n",
       " -0.014590859413146973,\n",
       " -0.30480706691741943,\n",
       " -0.2521495819091797,\n",
       " 0.018039405345916748,\n",
       " 0.05874842405319214,\n",
       " 0.06871652603149414,\n",
       " 0.07689827680587769,\n",
       " -0.06861937046051025,\n",
       " -0.12308287620544434,\n",
       " -0.06620633602142334,\n",
       " -0.14722275733947754,\n",
       " -0.18053078651428223,\n",
       " -0.13116776943206787,\n",
       " -0.03563892841339111]"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sq_list_NN2 = []\n",
    "#cum_feat_imp_NN2 = np.zeros(len(predictor_col))\n",
    "\n",
    "for year in range(1987,2016):\n",
    "    \n",
    "    net2 = NN2(input_size=177, hidden_size_1=32, hidden_size_2=16).to(device)\n",
    "    filename = \"NN2_\" + str(year) + \"_end.pt\"\n",
    "    net2.load_state_dict(torch.load(filename))\n",
    "    year_test_start = year\n",
    "    \n",
    "    # Note the coming 12 years of data are for validation but not for testing or model estimates\n",
    "    # So we test the model fitted with 1957-1974 data with 1987 data, and so on\n",
    "    dataset_test = stock_data(stockdata=df[((df['yyyymm']//100 >= year_test_start) & (df['yyyymm']//100 < year_test_start+1))], predictor_col=predictor_col)\n",
    "    data_test = DataLoader(dataset = dataset_test, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del dataset_test\n",
    "    \n",
    "    # Save the R_sq_oos for each year\n",
    "    R_sq_oos = test_model(net2, data_test).item()\n",
    "    R_sq_list_NN2.append(R_sq_oos)\n",
    "    del(data_test)\n",
    "    \n",
    "    # At the same time we find the feature importance of the net\n",
    "    year_train_end = year-12\n",
    "    data = stock_data(stockdata=df[df['yyyymm']//100 < year_train_end], predictor_col=predictor_col)\n",
    "    BATCH_SIZE = 10000\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(data)\n",
    "    cum_feat_imp_NN2 += feat_imp_net(net2, data_train)\n",
    "    del(data_train)\n",
    "    print(\"Year {} completed\".format(year))\n",
    "    \n",
    "R_sq_list_NN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03139443027562108\n",
      "-0.014590859413146973\n",
      "0.14403396844863892\n",
      "0.03631061315536499\n",
      "0.04551297426223755\n",
      "0.14403396844863892\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(R_sq_list_NN2))\n",
    "print(np.median(R_sq_list_NN2))\n",
    "print(np.max(R_sq_list_NN2))\n",
    "print(np.mean(R_sq_list_NN2[0:9]))\n",
    "print(np.median(R_sq_list_NN2[0:9]))\n",
    "print(np.max(R_sq_list_NN2[0:9]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.76138928e-05, -8.58663205e-05, -1.30268722e-04, -2.35505303e-05,\n",
       "        1.38845916e-05, -9.60900135e-05, -1.99179611e-05,  7.13440704e-05,\n",
       "       -1.65091730e-04, -6.56199577e-05, -3.16919931e-05, -1.52136351e-05,\n",
       "       -3.21786871e-05, -1.15240922e-06, -4.46045079e-07,  7.98237206e-06,\n",
       "        1.29071817e-04, -5.31195989e-05,  1.66734820e-03,  6.05054165e-05,\n",
       "        2.13447898e-04, -1.54543038e-05, -1.32480065e-05, -1.73805284e-06,\n",
       "       -5.63375576e-06,  8.42963270e-05,  4.25364883e-05,  5.37040919e-05,\n",
       "       -7.74076341e-06, -2.08652357e-06,  7.27006737e-06, -1.00993498e-05,\n",
       "        1.25832490e-05, -1.14319104e-05,  2.76565277e-05, -3.06495031e-06,\n",
       "       -1.92018410e-05,  5.76362455e-06,  3.92099551e-05,  1.96872692e-05,\n",
       "        1.83859525e-04,  1.46656128e-05,  5.26802892e-02, -2.20630890e-05,\n",
       "       -6.86365285e-07, -3.96165952e-04,  2.45258845e-05, -3.62227929e-05,\n",
       "       -2.49828671e-05, -1.21814070e-06, -1.74736668e-05,  4.20085458e-05,\n",
       "       -5.95042163e-05, -1.45037273e-05, -3.93844199e-06, -1.19394427e-06,\n",
       "        1.00144671e-04,  3.52059746e-05, -1.72030378e-05,  7.14915782e-06,\n",
       "        2.02607664e-05,  7.35790209e-05,  8.07093129e-04, -4.65258727e-05,\n",
       "        1.77589999e-04, -1.35889626e-05, -8.67926057e-07,  2.28890275e-06,\n",
       "       -2.99387825e-06, -3.83607112e-04, -1.00685325e-05,  7.00025270e-05,\n",
       "       -1.35684196e-05,  1.19837244e-05,  2.78089630e-06,  1.50723149e-04,\n",
       "       -5.08301650e-06, -5.99468712e-06, -1.26599656e-05, -8.91057252e-06,\n",
       "       -7.68533014e-06,  1.23818763e-05,  1.43042642e-04,  1.16773621e-04,\n",
       "        3.27886196e-04, -4.51204500e-05, -7.51463582e-06, -5.43300159e-05,\n",
       "       -8.31131617e-05, -6.41287813e-06, -5.31115971e-05,  2.43404697e-05,\n",
       "        2.32326354e-05,  1.37816176e-04,  6.45057029e-06, -4.95661719e-05,\n",
       "        4.55231530e-05,  1.66394765e-05,  6.60648294e-06, -6.72575890e-06,\n",
       "        7.28821688e-07,  0.00000000e+00, -2.30851115e-06,  1.57495365e-05,\n",
       "       -3.37553832e-06, -3.84131281e-05, -2.00089903e-08, -7.77290288e-06,\n",
       "       -5.84857792e-06,  4.67616732e-07,  6.17897592e-09,  2.53733548e-05,\n",
       "       -1.59172673e-05,  1.19305335e-05, -7.14045201e-06, -5.05224206e-06,\n",
       "       -1.94026808e-06, -2.02420576e-05, -5.14912958e-06,  1.93792836e-05,\n",
       "       -6.44510396e-06, -2.34643031e-07, -1.54585817e-06, -6.29482914e-06,\n",
       "        1.24807019e-05,  1.32521077e-05,  1.27734829e-05,  4.74196255e-06,\n",
       "       -1.11914185e-05,  1.24071305e-05,  1.71136867e-06, -1.69556072e-06,\n",
       "        8.04547120e-09,  4.28450053e-06, -1.01451899e-05, -5.62598073e-06,\n",
       "        0.00000000e+00,  1.21802227e-06, -1.59503991e-05,  2.94774710e-06,\n",
       "        2.75279871e-06,  4.77038194e-06, -5.14128544e-06, -2.59023282e-06,\n",
       "        2.19817101e-07,  8.04547120e-09,  6.90280915e-06, -4.46744205e-07,\n",
       "       -9.46044494e-06, -1.66720847e-05, -2.21394085e-05, -3.47941992e-06,\n",
       "        8.87819143e-07,  3.82754817e-05,  5.28801307e-06, -8.99830948e-06,\n",
       "       -4.40799653e-06, -6.69073174e-08,  5.56462812e-07,  2.64147757e-06,\n",
       "       -8.76456096e-06,  6.27936757e-08,  8.04547120e-09,  1.27476768e-06,\n",
       "       -1.62083626e-06, -6.31661794e-08,  7.49984699e-07,  0.00000000e+00,\n",
       "        8.14666058e-06,  4.30433317e-09,  8.81169380e-09,  0.00000000e+00,\n",
       "       -5.78313061e-06,  0.00000000e+00,  0.00000000e+00, -3.56693333e-06,\n",
       "        1.28096007e-05])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NN2_R2  <---- this store the R_sq_oos in 1987-2005 testing\n",
    "#NN2_R2_2 = R_sq_list #<---- this store the R_sq_oos in 1987-2004 testing\n",
    "NN3_R2\n",
    "#NN3_feat_imp = np.array(cum_feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 12 artists>"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAFlCAYAAAC9RQ9zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAc4ElEQVR4nO3de5RlZX3m8e8TGuXSCEHRCCS2ESMTQJBqFUSjOOgiRCMuQbzEu6vFGyHL69IJoo4XonEU8ZKOY5wE1kAgkvEyikqAKAJ2NQ3doIBGmVFgxbQYRBHE7t/8UbvHslJFnaZOnXPec76ftWrVOXu/+92/3Xv16qffd19SVUiSJKkNvzHsAiRJktQ7w5skSVJDDG+SJEkNMbxJkiQ1xPAmSZLUEMObJElSQ1YMu4BBecADHlCrVq0adhmSJEmLWr9+/eaq2mu+dRMT3latWsX09PSwy5AkSVpUkv+z0DqnTSVJkhpieJMkSWqI4U2SJKkhhjdJkqSGGN4kSZIaYniTJElqiOFNkiSpIYY3SZKkhhjeJEmSGmJ4kyRJaojhTZIkqSGGN0mSpIakqoZdw0AkmYwDlSRJy2KQmSnJ+qpaPd86R94kSZIaYniTJElqiOFNkiSpIYY3SZKkhhjeJEmSGmJ4kyRJaojhTZIkqSHNhLckf5zkzcOuQ5IkaZh8SK8kSVIPxuYhvUlWJbkuySeSXJPkrCRHJbk0ybeTPCbJjUn2mLXNd5I8KMleSf4hybru54h72M+Lk5zRfX56kiuSbEjylSQPWupxSJIktaBf06b7AR8CHgnsDzwPeDzweuAtwP8CngmQ5LHAjVX1r902/62qHg08C/hEj/v7GnBYVT0KOBt443yNkqxJMp1k+t4emCRJ0ihZ0ad+vldVmwCSXAtcWFWVZBOwCvgL4BTgb4DnAOd02x0F/H6Sbf3cL8luVXX7IvvbFzgnyYOB+wDfm69RVa0F1nZ1OW0qSZKa16+Rt7tmfd466/tWZgLiZcB+SfYCjgU+PWv/h1fVId3PPj0EN4APA2dU1UHAK4Cd+nAMkiRJI28gd5vWzBV+5wMfAL5VVT/qVn0JeM22dkkO6bHL3YGbus8v6lOZkiRJI2+Qjwo5B/gTfjVlCnASsDrJxiTfBE7ssa9TgXOTfBXY3NcqJUmSRpiPCpEkSerB2DwqRJIkSYPTr7tN+ybJS4A/nbP40qp69TDqkSRJGiVOm0qSJPXAaVNJkiRtt5GbNl0uU1NTTE/7ogVJktQ2R94kSZIaYniTJElqiOFNkiSpIYY3SZKkhviokO0wKX9WkiRpuHxUiCRJ0pgwvEmSJDXE8CZJktQQw5skSVJDDG+SJEkNGZnwluRJST437DokSZJG2ciEt6VKMjHvaZUkSZOrr+EtyQuTbExydZK/S/L0JFck2ZDkK0ke1LV7YpKrup8NSXbruliZ5Lwk1yU5K0m69jcmOS3JN7qf/brln0rygSQXAaf181gkSZJGUd9Gq5IcALwVOKKqNifZEyjgsKqqJC8H3gi8Dng98OqqujTJSuDOrptHAQcANwOXAkcAX+vW/aSqHpPkhcAHgad1y38POKqqtvTrWCRJkkZVP0fengycV1WbAarqVmBf4IIkm4A3MBPMYCaYfSDJScAeVfXLbvk3quoHVbUVuApYNav//znr9+Gzlp+7UHBLsibJdJLpJR+dJEnSCOhneAszI22zfRg4o6oOAl4B7ARQVe8FXg7sDFyeZP+u/V2ztt3Cr48M1gKff7ZQQVW1tqpWL/R6CUmSpNb0M7xdCDw7yf0BumnT3YGbuvUv2tYwycOqalNVnQZMA/vP7WweJ8z6fVnfqpYkSWpI3655q6prk7wLuCTJFmADcCpwbpKbgMuBh3bNT05yJDOja98EvsCvT4XO575JrmAmcD63X3VLkiS1JFVzZzpHT5IbgdXbrqe7l30s+UBb+LOSJEntS7J+ocu+xuY5b5IkSZOgiQfbVtWqYdcgSZI0Chx5kyRJaojhTZIkqSGGN0mSpIZMTHibmpqiqpb0I0mSNGwTE94kSZLGgeFNkiSpIYY3SZKkhhjeJEmSGtLE67H6YbHXY03Kn4MkSRp9vh5LkiRpTBjeJEmSGmJ4kyRJaojhTZIkqSGGN0mSpIb0Nbwl+VSS4/rZ5wL7uTHJA5Z7P5IkSaPGkTdJkqSGLBrekuya5PNJrk5yTZITkpySZF33fW2SzLPdVJJLkqxPckGSB3fL90vyla6/K5M8LMnKJBd23zclecZC+561i9fOar9/3/5EJEmSRlgvI29HAzdX1cFVdSDwReCMqnp0931n4GmzN0iyI/Bh4LiqmgI+CbyrW30W8JGqOhh4HHALcCfwzKo6FDgS+MsuEM637202d+0/Brz+3hy8JElSa3oJb5uAo5KcluQJVXUbcGSSK5JsAp4MHDBnm0cABwJfTnIV8F+AfZPsBuxTVecDVNWdVXUHEODdSTYCXwH2AR60wL63+XT3ez2war7Ck6xJMp1kuofjlCRJGnkrFmtQVTckmQKOAd6T5EvAq4HVVfX9JKcCO83ZLMC1VXX4ry1M7rfAbp4P7AVMVdXdSW4Edppv31X1jm6bu7rfWxY6jqpaC6zt9u37ryRJUvN6ueZtb+COqjoTeD9waLdqc5KVwHx3l14P7JXk8K6PHZMcUFU/AX6Q5Nhu+X2T7ALsDvywC25HAg9ZZN+SJEkTadGRN+Ag4H1JtgJ3A68EjmVmSvNGYN3cDarqF90jQ05Psnu3nw8C1wIvAP4qyTu6/o5n5jq4z3bTm1cB193DviVJkiZWqiZjNnGxadNJ+XOQJEmjL8n6qlo93zqf8yZJktQQw5skSVJDDG+SJEkNMbxJkiQ1xPAmSZLUkIkJb1NTU1TVgj+SJEktmJjwJkmSNA4Mb5IkSQ0xvEmSJDXE8CZJktSQiXw91qQcsyRJapOvx5IkSRoThjdJkqSGGN4kSZIaYniTJElqiOFNkiSpIYY3SZKkhhjeJEmSGjKS4S3JqiTXJflEkmuSnJXkqCSXJvl2ksckeWKSq7qfDUl2G3bdkiRJy23FsAu4B/sBxwNrgHXA84DHA38MvAXYAXh1VV2aZCVw57AKlSRJGpSRHHnrfK+qNlXVVuBa4MKaeTXCJmAVcCnwgSQnAXtU1S/ndpBkTZLpJNODLFySJGm5jHJ4u2vW562zvm8FVlTVe4GXAzsDlyfZf24HVbW2qlYv9HoJSZKk1ozytOk9SvKwqtoEbEpyOLA/cN2Qy5IkSVpWozzytpiTu5sZrgZ+Dnxh2AVJkiQtt8xcRjb+kvz/A52UY5YkSW1Ksn6hy75aHnmTJEmaOIY3SZKkhhjeJEmSGmJ4kyRJaojhTZIkqSETE96mpqaoKu80lSRJTZuY8CZJkjQODG+SJEkNMbxJkiQ1xPAmSZLUkIl7PdakHK8kSWqXr8eSJEkaE4Y3SZKkhhjeJEmSGmJ4kyRJaojhTZIkqSGGN0mSpIYMNLwl2SvJFUk2JHnCdm77jiRHLVdtkiRJLVgx4P39Z+C6qnrR9m5YVacsQz2SJElNWdaRtyQvTLIxydVJPgv8BXBMkquS7Jzkp0n+MsmVSS5Mstc99PWpJMd1n09Jsi7JNUnWJslyHockSdKoWLbwluQA4K3Ak6vqYOBFwCnAOVV1SFX9HNgVuLKqDgUuAd7WY/dnVNWjq+pAYGfgaQvUsCbJdJLppR6PJEnSKFjOkbcnA+dV1WaAqrp1njZbgXO6z2cCj++x7yO7a+c2dfs5YL5GVbW2qlYv9HoJSZKk1iznNW8BtvdFoou2T7IT8FFgdVV9P8mpwE7bX54kSVJ7lnPk7ULg2UnuD5BkzwX2f1z3+XnA13rod1tQ25xk5aztJUmSxt6yjbxV1bVJ3gVckmQLsAG4eE6znwEHJFkP3Aac0EO//57kr4FNwI3Aun7WLUmSNMpStb0zm33cefLTqlo5oH0VwDCPV5IkqRdJ1i90zb5vWJAkSWrIoB/S+2vmG3VL8hHgiDmLP1RVfzOYqiRJkkbXUMPbfKrq1cOuQZIkaVQ5bSpJktSQiQlvU1NT3qwgSZKaNzHhTZIkaRwY3iRJkhpieJMkSWqI4U2SJKkhQ33DwiD5hgVJktQK37AgSZI0JgxvkiRJDTG8SZIkNcTwJkmS1BDDmyRJUkMMb5IkSQ0ZyfCW5KfDrkGSJGkUjWR4kyRJ0vxGPrwleUOSdUk2Jnl7t+y0JK+a1ebUJK8bXpWSJEmDMdLhLclTgYcDjwEOAaaS/AFwNnDCrKbPBs6dZ/s1SaaTTA+gXEmSpGW3YtgFLOKp3c+G7vtK4OFV9d+TPDDJ3sBewI+r6v/O3biq1gJr4Vevx5IkSWrZqIe3AO+pqr+aZ915wHHAbzEzEidJkjT2RnraFLgAeGmSlQBJ9knywG7d2cBzmAlw5w2pPkmSpIEa6ZG3qvpSkv8EXJYE4KfAnwA/rKprk+wG3FRVtwyzTkmSpEFJ1WRcCrbtmrdJOV5JktSuJOuravV860Z92lSSJEmzGN4kSZIaYniTJElqiOFNkiSpIRMT3qamprxZQZIkNW9iwpskSdI4MLxJkiQ1xPAmSZLUEMObJElSQ3zDgiRJ0ojxDQuSJEljwvAmSZLUEMObJElSQwxvkiRJDTG8SZIkNcTwJkmS1JCBhrckn0py3CD3KUmSNE5GeuQtyYph1yBJkjRKlhyOkuwK/D2wL7AD8E7gEcDTgZ2BrwOvqDlPx00yBXwAWAlsBl5cVbckubjb5gjgM933+drtB3wc2AvYAhxfVf+y1OORJEkaZf0YeTsauLmqDq6qA4EvAmdU1aO77zsDT5u9QZIdgQ8Dx1XVFPBJ4F2zmuxRVU8ETr+HdmcBH6mqg4HHAbfMLSzJmiTTSab7cJySJElD149pyU3A+5OcBnyuqr6a5FlJ3gjsAuwJXAt8dtY2jwAOBL6cBGZG7GaHr3PuqV2S3YB9qup8gKq6c77CqmotsBZ+9XosSZKkli05vFXVDd0U6DHAe5J8CXg1sLqqvp/kVGCnOZsFuLaqDl+g25/dU7sk91tq3ZIkSS1a8rRpkr2BO6rqTOD9wKHdqs1JVgLz3V16PbBXksO7PnZMckCv7arqJ8APkhzbLb9vkl2WeiySJEmjrh/TpgcB70uyFbgbeCVwLDPTqTcC6+ZuUFW/6B4ZcnqS3bs6PsjM9Gqv7V4A/FWSd3T7PR74bh+OR5IkaWRlzk2gY2vbNW+TcrySJKldSdZX1er51o30c94kSZL06wxvkiRJDTG8SZIkNcTwJkmS1JCJCW9TU1PerCBJkpo3MeFNkiRpHBjeJEmSGmJ4kyRJaojhTZIkqSET9YaFSTlWSZLUNt+wIEmSNCYMb5IkSQ0xvEmSJDXE8CZJktQQw5skSVJDDG+SJEkN6Xt4S3Jikhf2u9959rNHklct934kSZJGSbPPeUuyCvhcVR3YY3uf8yZJkpqwrM95S/LCJBuTXJ3k75KcmuT13bqLk5yW5BtJbkjyhG75FUkOmNXHxUmmkuya5JNJ1iXZkOQZ3foDuj6u6vb1cOC9wMO6Ze9b6nFIkiS1YMVSNu4C2FuBI6pqc5I9gZPm7qOqHpPkGOBtwFHA2cCzgbcleTCwd1WtT/Ju4J+q6qVJ9gC+keQrwInAh6rqrCT3AXYA3gwcWFWH3EN9a4A1SzlGSZKkUbLUkbcnA+dV1WaAqrp1njaf7n6vB1Z1n/8eOL77/Gzg3O7zU4E3J7kKuBjYCfgd4DLgLUneBDykqn7eS3FVtbaqVi807ChJktSaJY28AQEWu5Dsru73lm37q6qbkvwoySOBE4BXzOrvWVV1/Zw+vpXkCuCPgAuSvBz47hJrlyRJas5SR94uBJ6d5P4A3bRpr84G3gjsXlWbumUXAK9Nkq6/R3W/fxf4blWdDnwGeCRwO7DbEuuXJElqypLCW1VdC7wLuCTJ1cAHtmPz84DnMDOFus07gR2BjUmu6b7DzOjcNd106v7A31bVj4BLk1zjDQuSJGlSNPuokO3lo0IkSVIrlvVRIZIkSRocw5skSVJDDG+SJEkNMbxJkiQ1ZGLC29TU1LBLkCRJWrKJCW+SJEnjwPAmSZLUEMObJElSQwxvkiRJDfENC5IkSSPGNyxIkiSNCcObJElSQwxvkiRJDTG8SZIkNcTwJkmS1BDDmyRJUkOWPbwlOTnJLguse3GSM5a7BkmSpHExiJG3k4F5w5skSZK2T1/DW5Jdk3w+ydVJrknyNmBv4KIkF3VtXpLkhiSXAEfM2vbpSa5IsiHJV5I8qFu+MsnfJNmUZGOSZ3XLj05yZbevC/t5HJIkSaNqRZ/7Oxq4uar+CCDJ7sBLgCOranOSBwNvB6aA24CLgA3dtl8DDquqSvJy4I3A64A/B26rqoO6Pn8zyV7AXwN/UFXfS7Jnn49DkiRpJPV72nQTcFSS05I8oapum7P+scDFVfVvVfUL4JxZ6/YFLkiyCXgDcEC3/CjgI9saVdWPgcOAf66q73XLbp2vmCRrkkwnme7HwUmSJA1bX8NbVd3AzKjaJuA9SU6Zr9kCm38YOKMbYXsFsFO3PPNsM9+y+epZW1WrF3o3mCRJUmv6fc3b3sAdVXUm8H7gUOB2YLeuyRXAk5LcP8mOwPGzNt8duKn7/KJZy78EvGbWPn4TuAx4YpKHdsucNpUkSROh39e8HQS8L8lW4G7glcDhwBeS3FJVRyY5lZnwdQtwJbBDt+2pwLlJbgIuBx7aLf+vwEeSXANsAd5eVZ9Osgb4dJLfAH4IPKXPxyJJkjRyUrXo7ONYSFKTcqySJKltSdYvdNmXb1iQJElqiOFNkiSpIYY3SZKkhhjeJEmSGmJ4kyRJasjEhLepqalhlyBJkrRkExPeJEmSxoHhTZIkqSGGN0mSpIYY3iRJkhri67EkSZJGjK/HkiRJGhOGN0mSpIYY3iRJkhpieJMkSWqI4U2SJKkhhjdJkqSGLFt4S7IqyTV97G/vJOf1qz9JkqQWrRh2Ab2qqpuB44ZdhyRJ0jAt97TpiiT/I8nGJOcl2SXJjUneneSyJNNJDk1yQZJ/SXLiQh3NHsnrPn81yZXdz+OW+TgkSZJGwnKHt0cAa6vqkcBPgFd1y79fVYcDXwU+xcyI2mHAO3rs94fAU6rqUOAE4PT5GiVZ0wXE6Xt/CJIkSaNjuadNv19Vl3afzwRO6j5/pvu9CVhZVbcDtye5M8keVfXvi/S7I3BGkkOALcDvzdeoqtYCa2Hm9Vj3+igkSZJGxHKHt7mBadv3u7rfW2d93va9l5r+DPhX4GBmRg/vXEKNkiRJzVjuadPfSXJ49/m5wNf61O/uwC1VtRV4AbBDn/qVJEkaacsd3r4FvCjJRmBP4GN96vejXb+XMzNl+rM+9StJkjTSUjUZl4IlqUk5VkmS1LYk66tq9XzrfMOCJElSQ0buIb1JDgL+bs7iu6rqscOoR5IkaZSMXHirqk3AIcOuQ5IkaRQ5bSpJktSQiQlvU1NTwy5BkiRpySYmvEmSJI0Dw5skSVJDDG+SJEkNMbxJkiQ1xDcsSJIkjRjfsCBJkjQmDG+SJEkNMbxJkiQ1xPAmSZLUEMObJElSQwxvkiRJDRmb8JZkxbBrkCRJWm4jF3iS/CPw28BOwIeqam2SlwFvAm4Gvg3cVVWvSfIp4FbgUcCVwOuGUrQkSdKAjFx4A15aVbcm2RlYl+TzwJ8DhwK3A/8EXD2r/e8BR1XVlsGXKkmSNFijOG16UpKrgcuZGYF7AXBJVd1aVXcD585pf+5CwS3JmiTTSaaXt2RJkqTBGKnwluRJwFHA4VV1MLABuH6RzX620IqqWltVqxd6vYQkSVJrRiq8AbsDP66qO5LsDxwG7AI8MclvdjclPGuoFUqSJA3RqF3z9kXgxCQbmRlxuxy4CXg3cAUzNyx8E7htaBVKkiQNUapq2DUsKsnKqvppN/J2PvDJqjp/O/uoFo5VkiQpyfqFLvsatWnThZya5CrgGuB7wD8OtRpJkqQhaWLkrR8ceZMkSa0Yh5E3SZIkYXiTJElqiuFNkiSpIRMT3qampoZdgiRJ0pJNTHiTJEkaB4Y3SZKkhhjeJEmSGmJ4kyRJaogP6ZUkSRoxPqRXkiRpTBjeJEmSGmJ4kyRJaojhTZIkqSGGN0mSpIYMPbwlOTnJLguse3GSMxbZ/qQk30py1vJUKEmSNDqGHt6Ak4F5w1uPXgUcU1XP7085kiRJo2ug4S3Jrkk+n+TqJNckeRuwN3BRkou6Ni9JckOSS4AjZm37oCTnd9teneRxST4O/C7wmSR/NshjkSRJGoYVA97f0cDNVfVHAEl2B14CHFlVm5M8GHg7MAXcBlwEbOi2PR24pKqemWQHYGVVnZjk6G3bD/hYJEmSBm7Q06abgKOSnJbkCVV125z1jwUurqp/q6pfAOfMWvdk4GMAVbVlnm3/gyRrkkwnme7XAUiSJA3TQEfequqGJFPAMcB7knxpvmZ93N9aYC3MvB6rX/1KkiQNy6CvedsbuKOqzgTeDxwK3A7s1jW5AnhSkvsn2RE4ftbmFwKv7PrZIcn9Ble5JEnSaBj0NW8HAe9LshW4m5kwdjjwhSS3VNWRSU4FLgNuAa4Edui2/VNgbZKXAVu6bS8bcP2SJElDlarJmE1MUpNyrJIkqW1J1lfV6vnWjcJz3iRJktQjw5skSVJDDG+SJEkNMbxJkiQ1xPAmSZLUkIkJb1NTU8MuQZIkackmJrxJkiSNA8ObJElSQwxvkiRJDTG8SZIkNcTwJkmS1BDDmyRJUkMMb5IkSQ0xvEmSJDXE8CZJktQQw5skSVJDDG+SJEkNMbxJkiQ1xPAmSZLUEMObJElSQ1JVw65hIJLcDlw/7Dp0rz0A2DzsInSvef7a5vlrm+evTQ+pqr3mW7Fi0JUM0fVVtXrYRejeSTLt+WuX569tnr+2ef7Gj9OmkiRJDTG8SZIkNWSSwtvaYRegJfH8tc3z1zbPX9s8f2NmYm5YkCRJGgeTNPImSZLUvLEKb0mOTnJ9ku8kefM865Pk9G79xiSHDqNOLayHc7h/ksuS3JXk9cOoUQvr4fw9v/u7tzHJ15McPIw6Nb8ezt8zunN3VZLpJI8fRp2a32Lnb1a7RyfZkuS4Qdan/hmbadMkOwA3AE8BfgCsA55bVd+c1eYY4LXAMcBjgQ9V1WOHUK7m0eM5fCDwEOBY4MdV9f4hlKp59Hj+Hgd8q6p+nOQPgVP9Ozgaejx/K4GfVVUleSTw91W1/1AK1q/p5fzNavdl4E7gk1V13qBr1dKN08jbY4DvVNV3q+oXwNnAM+a0eQbwtzXjcmCPJA8edKFa0KLnsKp+WFXrgLuHUaDuUS/n7+tV9ePu6+XAvgOuUQvr5fz9tH71P/5dgfH43/946OXfQJgZwPgH4IeDLE79NU7hbR/g+7O+/6Bbtr1tNDyen7Zt7/l7GfCFZa1I26On85fkmUmuAz4PvHRAtWlxi56/JPsAzwQ+PsC6tAzGKbxlnmVz/1fYSxsNj+enbT2fvyRHMhPe3rSsFWl79HT+qur8bqr0WOCdy12UetbL+fsg8Kaq2rL85Wg5jdPrsX4A/Pas7/sCN9+LNhoez0/bejp/3bVSnwD+sKp+NKDatLjt+vtXVf+c5GFJHlBVvjdz+Ho5f6uBs5PAzPtOj0nyy6r6x4FUqL4Zp5G3dcDDkzw0yX2A5wCfmdPmM8ALu7tODwNuq6pbBl2oFtTLOdToWvT8Jfkd4NPAC6rqhiHUqIX1cv72S/cvf3e3/n0AA/hoWPT8VdVDq2pVVa0CzgNeZXBr09iMvFXVL5O8BrgA2IGZu2iuTXJit/7jwP9m5k7T7wB3AC8ZVr36j3o5h0l+C5gG7gdsTXIy8PtV9ZNh1a0ZPf4dPAW4P/DRLgP80hdmj4Yez9+zmPkP8N3Az4ETZt3AoCHq8fxpTIzNo0IkSZImwThNm0qSJI09w5skSVJDDG+SJEkNMbxJkiQ1xPAmSZLUEMObJElSQwxvkiRJDTG8SZIkNeT/AZYZqJdnNVWwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cum_feat_imp\n",
    "cum_feat_imp_df = df.loc[0,predictor_col]\n",
    "cum_feat_imp_df[predictor_col] = cum_feat_imp_NN2/sum(cum_feat_imp_NN2)\n",
    "cum_feat_imp_df\n",
    "# PLot the normalized average feature importance in the 30 training samples\n",
    "feat_imp = cum_feat_imp_df.sort_values(ascending=False)/2\n",
    "feat_imp_to_plot = feat_imp[0:12].sort_values(ascending=True)\n",
    "\n",
    "# Plot the feature importances in horizontal bar charts\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 6))\n",
    "ax = plt.subplot()\n",
    "ax.barh(list(feat_imp_to_plot.index),\n",
    "    feat_imp_to_plot, \n",
    "    align = 'center', color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc1.weight \t torch.Size([32, 177])\n",
      "fc1.bias \t torch.Size([32])\n",
      "bn1.weight \t torch.Size([32])\n",
      "bn1.bias \t torch.Size([32])\n",
      "bn1.running_mean \t torch.Size([32])\n",
      "bn1.running_var \t torch.Size([32])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "fc2.weight \t torch.Size([16, 32])\n",
      "fc2.bias \t torch.Size([16])\n",
      "bn2.weight \t torch.Size([16])\n",
      "bn2.bias \t torch.Size([16])\n",
      "bn2.running_mean \t torch.Size([16])\n",
      "bn2.running_var \t torch.Size([16])\n",
      "bn2.num_batches_tracked \t torch.Size([])\n",
      "output.weight \t torch.Size([1, 16])\n",
      "output.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "## Save the model for future use\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net2.state_dict():\n",
    "    print(param_tensor, \"\\t\", net2.state_dict()[param_tensor].size())\n",
    "\n",
    "torch.save(net2.state_dict(), \"NN2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_trial = NN2(input_size=177, hidden_size_1=32, hidden_size_2=16).to(device)\n",
    "net_trial.load_state_dict(torch.load(\"NN2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_sq_oos in the following year:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0822, grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(net_trial, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1635933356697,
     "user": {
      "displayName": "Alfred Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "07405245146433801024"
     },
     "user_tz": -480
    },
    "id": "zNy-AjgBSNdW",
    "outputId": "49740f0c-8c8d-4f6b-8f47-1f54f3a1005f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN3(\n",
      "  (fc1): Linear(in_features=177, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (bn3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##### Define the class for NN3\n",
    "\n",
    "class NN3(Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3):\n",
    "        super(NN3, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1  = hidden_size_1\n",
    "        self.hidden_size_2  = hidden_size_2\n",
    "        self.hidden_size_3  = hidden_size_3\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size_1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_size_1)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size_1, self.hidden_size_2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.hidden_size_2)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size_2, self.hidden_size_3)\n",
    "        self.bn3 = nn.BatchNorm1d(self.hidden_size_3)\n",
    "        self.output = nn.Linear(self.hidden_size_3, 1)\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn1(relu)\n",
    "        hidden = self.fc2(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn2(relu)\n",
    "        hidden = self.fc3(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn3(relu)\n",
    "        output = self.output(bn_x)\n",
    "        return output\n",
    "\n",
    "net3 = NN3(input_size=177, hidden_size_1=32, hidden_size_2=16, hidden_size_3=8).to(device)\n",
    "print(net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training MSE Loss : 3.543133020401001\n",
      "Epoch 1 Validation R^2_oos : 0.0006534457206726074\n",
      "Epoch 2 : Training MSE Loss : 1.265673041343689\n",
      "Epoch 2 Validation R^2_oos : 0.08968621492385864\n",
      "Epoch 3 : Training MSE Loss : 1.0433968305587769\n",
      "Epoch 3 Validation R^2_oos : 0.08780354261398315\n",
      "Epoch 4 : Training MSE Loss : 1.0189517736434937\n",
      "Epoch 4 Validation R^2_oos : 0.09974539279937744\n",
      "Epoch 5 : Training MSE Loss : 1.0101655721664429\n",
      "Epoch 5 Validation R^2_oos : 0.09299051761627197\n",
      "Epoch 6 : Training MSE Loss : 1.0070650577545166\n",
      "Epoch 6 Validation R^2_oos : 0.09106969833374023\n",
      "Epoch 7 : Training MSE Loss : 1.0027154684066772\n",
      "Epoch 7 Validation R^2_oos : 0.08716869354248047\n",
      "Epoch 8 : Training MSE Loss : 0.9997090101242065\n",
      "Epoch 8 Validation R^2_oos : 0.08658218383789062\n",
      "Epoch 9 : Training MSE Loss : 0.9985541701316833\n",
      "Epoch 9 Validation R^2_oos : 0.0868954062461853\n",
      "Early stopping triggered and the best validation R^2: 0.09974539279937744\n",
      "Trainin stopped after 9 epochs\n"
     ]
    }
   ],
   "source": [
    "### Training for NN3 of a single period\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-5\n",
    "epsilon = 0.0005\n",
    "train_model(net3, data_train, data_valid, learning_rate, l1_lambda, epsilon, printout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net3.state_dict(), \"NN3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_sq_oos in the following year:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1115, grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(net3, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN3_1997_end.pt\n",
      "NN3_1998_end.pt\n",
      "NN3_1999_end.pt\n",
      "NN3_2000_end.pt\n",
      "NN3_2001_end.pt\n",
      "NN3_2002_end.pt\n",
      "NN3_2003_end.pt\n",
      "NN3_2004_end.pt\n",
      "NN3_2005_end.pt\n",
      "NN3_2006_end.pt\n",
      "NN3_2007_end.pt\n",
      "NN3_2008_end.pt\n",
      "NN3_2009_end.pt\n",
      "NN3_2010_end.pt\n",
      "NN3_2011_end.pt\n",
      "NN3_2012_end.pt\n",
      "NN3_2013_end.pt\n",
      "NN3_2014_end.pt\n",
      "NN3_2015_end.pt\n",
      "NN3_2016_end.pt\n"
     ]
    }
   ],
   "source": [
    "# Define a recursive training loop for NN3\n",
    "BATCH_SIZE = 10000\n",
    "### Training for NN3\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-4\n",
    "epsilon = 0.005\n",
    "\n",
    "# Here year represents \n",
    "for year in range(1975,2005):\n",
    "    data = stock_data(stockdata=df.loc[df['yyyymm']//100 < year,:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(data)\n",
    "    \n",
    "    dataset_valid = stock_data(stockdata=df.loc[((df['yyyymm']//100 >= year)&(df['yyyymm']//100 < year+12)),:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_valid = DataLoader(dataset = dataset_valid, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(dataset_valid)\n",
    "    \n",
    "    net = NN3(input_size=177, hidden_size_1=32, hidden_size_2=16, hidden_size_3=8).to(device)\n",
    "    train_model(net, data_train, data_valid, learning_rate, l1_lambda, epsilon, printout=False)\n",
    "    \n",
    "    del(data_train)\n",
    "    del(data_valid)\n",
    "    gc.collect()\n",
    "    \n",
    "    filename = \"NN3_\" + str(year+12) + \"_end.pt\"\n",
    "    print(filename)\n",
    "    torch.save(net.state_dict(), filename)\n",
    "    del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "80\n",
      "160\n",
      "Year 1995 completed\n",
      "0\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "#R_sq_list_NN3 = []\n",
    "#cum_feat_imp_NN3 = np.zeros(len(predictor_col))\n",
    "\n",
    "for year in range(1987,2016):\n",
    "    \n",
    "    net3 = NN3(input_size=177, hidden_size_1=32, hidden_size_2=16, hidden_size_3=8).to(device)\n",
    "    filename = \"NN3_\" + str(year) + \"_end.pt\"\n",
    "    net3.load_state_dict(torch.load(filename))\n",
    "    year_test_start = year\n",
    "    \n",
    "    # Note the coming 12 years of data are for validation but not for testing or model estimates\n",
    "    # So we test the model fitted with 1957-1974 data with 1987 data, and so on\n",
    "    dataset_test = stock_data(stockdata=df[((df['yyyymm']//100 >= year_test_start) & (df['yyyymm']//100 < year_test_start+1))], predictor_col=predictor_col)\n",
    "    data_test = DataLoader(dataset = dataset_test, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del dataset_test\n",
    "    \n",
    "    # Save the R_sq_oos for each year\n",
    "    R_sq_oos = test_model(net3, data_test).item()\n",
    "    R_sq_list_NN3.append(R_sq_oos)\n",
    "    del(data_test)\n",
    "    \n",
    "    # At the same time we find the feature importance of the net\n",
    "    year_train_end = year-12\n",
    "    data = stock_data(stockdata=df[df['yyyymm']//100 < year_train_end], predictor_col=predictor_col)\n",
    "    BATCH_SIZE = 10000\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(data)\n",
    "    cum_feat_imp_NN3 += feat_imp_net(net3, data_train)\n",
    "    del(data_train)\n",
    "    \n",
    "    print(\"Year {} completed\".format(year))\n",
    "    \n",
    "R_sq_list_NN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05861739043531747\n",
      "-0.026080012321472168\n",
      "0.07488685846328735\n",
      "-0.03688135412004259\n",
      "0.0062787532806396484\n",
      "0.059754133224487305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04713523,  0.05975413,  0.03392744,  0.04211557,  0.00627875,\n",
       "       -0.00551903, -0.11080503, -0.10928094, -0.29553831, -0.08536482,\n",
       "       -0.09010637,  0.02650642, -0.02608001,  0.05243403, -0.01269984,\n",
       "       -0.02595353, -0.370309  , -0.26088119,  0.02911311,  0.05694848,\n",
       "        0.04973662,  0.07488686, -0.09561956, -0.11908782, -0.0703932 ,\n",
       "       -0.14538836, -0.18080342, -0.13677883, -0.03813171])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN3_R_2 = np.array(R_sq_list_NN3)\n",
    "#print(cum_feat_imp_NN3)\n",
    "print(np.mean(R_sq_list_NN3))\n",
    "print(np.median(R_sq_list_NN3))\n",
    "print(np.max(R_sq_list_NN3))\n",
    "print(np.mean(R_sq_list_NN3[0:9]))\n",
    "print(np.median(R_sq_list_NN3[0:9]))\n",
    "print(np.max(R_sq_list_NN3[0:9]))\n",
    "NN3_R_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 12 artists>"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NN3_R2 = np.array(R_sq_list_NN3)\n",
    "#NN3_feat_imp = np.array(cum_feat_imp_NN3)\n",
    "\n",
    "#cum_feat_imp\n",
    "cum_feat_imp_df = df.loc[0,predictor_col]\n",
    "cum_feat_imp_df[predictor_col] = NN3_feat_imp_2/sum(NN3_feat_imp_2)\n",
    "cum_feat_imp_df\n",
    "# PLot the normalized average feature importance in the 30 training samples\n",
    "feat_imp = cum_feat_imp_df.sort_values(ascending=False)\n",
    "feat_imp_to_plot = feat_imp[0:12].sort_values(ascending=True)\n",
    "\n",
    "# Plot the feature importances in horizontal bar charts\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 6))\n",
    "ax = plt.subplot()\n",
    "ax.barh(list(feat_imp_to_plot.index),\n",
    "    feat_imp_to_plot, \n",
    "    align = 'center', color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN4(\n",
      "  (fc1): Linear(in_features=177, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (bn3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (bn4): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (output): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##### Define the class for NN4\n",
    "\n",
    "class NN4(Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4):\n",
    "        super(NN4, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1  = hidden_size_1\n",
    "        self.hidden_size_2  = hidden_size_2\n",
    "        self.hidden_size_3  = hidden_size_3\n",
    "        self.hidden_size_4  = hidden_size_4\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size_1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_size_1)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size_1, self.hidden_size_2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.hidden_size_2)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size_2, self.hidden_size_3)\n",
    "        self.bn3 = nn.BatchNorm1d(self.hidden_size_3)\n",
    "        self.fc4 = torch.nn.Linear(self.hidden_size_3, self.hidden_size_4)\n",
    "        self.bn4 = nn.BatchNorm1d(self.hidden_size_4)\n",
    "        self.output = nn.Linear(self.hidden_size_4, 1)\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn1(relu)\n",
    "        hidden = self.fc2(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn2(relu)\n",
    "        hidden = self.fc3(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn3(relu)\n",
    "        hidden = self.fc4(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn4(relu)\n",
    "        output = self.output(bn_x)\n",
    "        return output\n",
    "\n",
    "net4 = NN4(input_size=177, hidden_size_1=32, hidden_size_2=16, hidden_size_3=8, hidden_size_4=4).to(device)\n",
    "print(net4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training MSE Loss : 3.3649988174438477\n",
      "Epoch 1 Validation R^2_oos : 0.0780753493309021\n",
      "Epoch 2 : Training MSE Loss : 1.0662082433700562\n",
      "Epoch 2 Validation R^2_oos : 0.11865705251693726\n",
      "Epoch 3 : Training MSE Loss : 1.0139415264129639\n",
      "Epoch 3 Validation R^2_oos : 0.1251511573791504\n",
      "Epoch 4 : Training MSE Loss : 0.9980990886688232\n",
      "Epoch 4 Validation R^2_oos : 0.12443691492080688\n",
      "Epoch 5 : Training MSE Loss : 0.9975364804267883\n",
      "Epoch 5 Validation R^2_oos : 0.12297779321670532\n",
      "Epoch 6 : Training MSE Loss : 0.9902791380882263\n",
      "Epoch 6 Validation R^2_oos : 0.11890459060668945\n",
      "Epoch 7 : Training MSE Loss : 0.9903302788734436\n",
      "Epoch 7 Validation R^2_oos : 0.11546593904495239\n",
      "Epoch 8 : Training MSE Loss : 0.9851861596107483\n",
      "Epoch 8 Validation R^2_oos : 0.11488866806030273\n",
      "Early stopping triggered and the best validation R^2: 0.1251511573791504\n",
      "Trainin stopped after 8 epochs\n"
     ]
    }
   ],
   "source": [
    "### Training for NN4\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-5\n",
    "epsilon = 0.0005\n",
    "train_model(net4, data_train, data_valid, learning_rate, l1_lambda, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training MSE Loss : 2.937375068664551\n",
      "Epoch 1 Validation R^2_oos : 0.08512461185455322\n",
      "Epoch 2 : Training MSE Loss : 2.9446606636047363\n",
      "Epoch 2 Validation R^2_oos : 0.08020991086959839\n",
      "Epoch 3 : Training MSE Loss : 2.9041643142700195\n",
      "Epoch 3 Validation R^2_oos : 0.07446169853210449\n",
      "Epoch 4 : Training MSE Loss : 2.8876259326934814\n",
      "Epoch 4 Validation R^2_oos : 0.07079464197158813\n",
      "Epoch 5 : Training MSE Loss : 2.8781983852386475\n",
      "Epoch 5 Validation R^2_oos : 0.06938886642456055\n",
      "Epoch 6 : Training MSE Loss : 2.8613812923431396\n",
      "Epoch 6 Validation R^2_oos : 0.07069742679595947\n",
      "Early stopping triggered and the best validation R^2: 0.08512461185455322\n",
      "Trainin stopped after 6 epochs\n"
     ]
    }
   ],
   "source": [
    "### Training for NN4 with the full dataset (training + validation set)\n",
    "train_model(net4, data_train_full, data_test, learning_rate, l1_lambda, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net4.state_dict(), \"NN4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_sq_oos in the following year:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1035, grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(net4, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN4_1997_end.pt\n",
      "NN4_1998_end.pt\n",
      "NN4_1999_end.pt\n",
      "NN4_2000_end.pt\n",
      "NN4_2001_end.pt\n",
      "NN4_2002_end.pt\n",
      "NN4_2003_end.pt\n",
      "NN4_2004_end.pt\n",
      "NN4_2005_end.pt\n",
      "NN4_2006_end.pt\n",
      "NN4_2007_end.pt\n",
      "NN4_2008_end.pt\n",
      "NN4_2009_end.pt\n",
      "NN4_2010_end.pt\n",
      "NN4_2011_end.pt\n",
      "NN4_2012_end.pt\n",
      "NN4_2013_end.pt\n",
      "NN4_2014_end.pt\n",
      "NN4_2015_end.pt\n",
      "NN4_2016_end.pt\n"
     ]
    }
   ],
   "source": [
    "# Define a recursive training loop for NN4\n",
    "BATCH_SIZE = 10000\n",
    "### Training for NN4\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-4\n",
    "epsilon = 0.001\n",
    "\n",
    "for year in range(1975,2005):\n",
    "    data = stock_data(stockdata=df.loc[df['yyyymm']//100 < year,:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(data)\n",
    "    \n",
    "    dataset_valid = stock_data(stockdata=df.loc[((df['yyyymm']//100 >= year)&(df['yyyymm']//100 < year+12)),:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_valid = DataLoader(dataset = dataset_valid, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(dataset_valid)\n",
    "    \n",
    "    net = NN4(input_size=177, hidden_size_1=32, hidden_size_2=16, hidden_size_3=8, hidden_size_4=4).to(device)\n",
    "    train_model(net, data_train, data_valid, learning_rate, l1_lambda, epsilon, printout=False)\n",
    "    \n",
    "    del(data_train)\n",
    "    del(data_valid)\n",
    "    gc.collect()\n",
    "    \n",
    "    filename = \"NN4_\" + str(year+12) + \"_end.pt\"\n",
    "    print(filename)\n",
    "    torch.save(net.state_dict(), filename)\n",
    "    del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 1997 completed\n",
      "Year 1998 completed\n",
      "Year 1999 completed\n",
      "Year 2000 completed\n",
      "Year 2001 completed\n",
      "Year 2002 completed\n",
      "Year 2003 completed\n",
      "Year 2004 completed\n",
      "Year 2005 completed\n",
      "Year 2006 completed\n",
      "Year 2007 completed\n",
      "Year 2008 completed\n",
      "Year 2009 completed\n",
      "Year 2010 completed\n",
      "Year 2011 completed\n",
      "Year 2012 completed\n",
      "Year 2013 completed\n",
      "Year 2014 completed\n",
      "Year 2015 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04940450191497803,\n",
       " 0.06414300203323364,\n",
       " 0.05964648723602295,\n",
       " 0.05428183078765869,\n",
       " 0.005689382553100586,\n",
       " -0.017958521842956543,\n",
       " -0.1940622329711914,\n",
       " -0.040036797523498535,\n",
       " -0.2886390686035156,\n",
       " -0.013357281684875488,\n",
       " -0.04052877426147461,\n",
       " 0.04371136426925659,\n",
       " -0.009341716766357422,\n",
       " 0.05293834209442139,\n",
       " -0.024434447288513184,\n",
       " -0.04148149490356445,\n",
       " -0.42702770233154297,\n",
       " -0.22869861125946045,\n",
       " 0.04338568449020386,\n",
       " -0.050274014472961426,\n",
       " 0.08281964063644409,\n",
       " 0.10839813947677612,\n",
       " -0.08624231815338135,\n",
       " -0.0897599458694458,\n",
       " -0.08715283870697021,\n",
       " -0.1428285837173462,\n",
       " -0.16103196144104004,\n",
       " -0.16258549690246582,\n",
       " -0.027772903442382812]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_sq_list_NN4 = []\n",
    "cum_feat_imp_NN4 = np.zeros(len(predictor_col))\n",
    "\n",
    "for year in range(1987,2016):\n",
    "    \n",
    "    net4 = NN4(input_size=177, hidden_size_1=32, hidden_size_2=16, hidden_size_3=8, hidden_size_4=4).to(device)\n",
    "    filename = \"NN4_\" + str(year) + \"_end.pt\"\n",
    "    net4.load_state_dict(torch.load(filename))\n",
    "    year_test_start = year\n",
    "    \n",
    "    # Note the coming 12 years of data are for validation but not for testing or model estimates\n",
    "    # So we test the model fitted with 1957-1974 data with 1987 data, and so on\n",
    "    dataset_test = stock_data(stockdata=df[((df['yyyymm']//100 >= year_test_start) & (df['yyyymm']//100 < year_test_start+1))], predictor_col=predictor_col)\n",
    "    data_test = DataLoader(dataset = dataset_test, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del dataset_test\n",
    "    \n",
    "    # Save the R_sq_oos for each year\n",
    "    R_sq_oos = test_model(net4, data_test).item()\n",
    "    R_sq_list_NN4.append(R_sq_oos)\n",
    "    del(data_test)\n",
    "    \n",
    "    # At the same time we find the feature importance of the net\n",
    "    \n",
    "    year_train_end = year-12\n",
    "    data = stock_data(stockdata=df[df['yyyymm']//100 < year_train_end], predictor_col=predictor_col)\n",
    "    BATCH_SIZE = 10000\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(data)\n",
    "    cum_feat_imp_NN4 += feat_imp_net(net4, data_train)\n",
    "    del(data_train)\n",
    "    print(\"Year {} completed\".format(year))\n",
    "    \n",
    "R_sq_list_NN4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05409642540175339\n",
      "-0.027772903442382812\n",
      "0.10839813947677612\n",
      "-0.03417015737957425\n",
      "0.005689382553100586\n",
      "0.06414300203323364\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(R_sq_list_NN4))\n",
    "print(np.median(R_sq_list_NN4))\n",
    "print(np.max(R_sq_list_NN4))\n",
    "print(np.mean(R_sq_list_NN4[0:9]))\n",
    "print(np.median(R_sq_list_NN4[0:9]))\n",
    "print(np.max(R_sq_list_NN4[0:9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN5(\n",
      "  (fc1): Linear(in_features=177, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (bn3): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (bn4): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (bn5): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (output): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##### Define the class for NN5\n",
    "\n",
    "class NN5(Module):\n",
    "    def __init__(self, input_size, hidden_size_1, hidden_size_2, hidden_size_3, hidden_size_4, hidden_size_5):\n",
    "        super(NN5, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1  = hidden_size_1\n",
    "        self.hidden_size_2  = hidden_size_2\n",
    "        self.hidden_size_3  = hidden_size_3\n",
    "        self.hidden_size_4  = hidden_size_4\n",
    "        self.hidden_size_5  = hidden_size_5\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size_1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_size_1)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size_1, self.hidden_size_2)\n",
    "        self.bn2 = nn.BatchNorm1d(self.hidden_size_2)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden_size_2, self.hidden_size_3)\n",
    "        self.bn3 = nn.BatchNorm1d(self.hidden_size_3)\n",
    "        self.fc4 = torch.nn.Linear(self.hidden_size_3, self.hidden_size_4)\n",
    "        self.bn4 = nn.BatchNorm1d(self.hidden_size_4)\n",
    "        self.fc5 = torch.nn.Linear(self.hidden_size_4, self.hidden_size_5)\n",
    "        self.bn5 = nn.BatchNorm1d(self.hidden_size_5)\n",
    "        self.output = nn.Linear(self.hidden_size_5, 1)\n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn1(relu)\n",
    "        hidden = self.fc2(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn2(relu)\n",
    "        hidden = self.fc3(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn3(relu)\n",
    "        hidden = self.fc4(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn4(relu)\n",
    "        hidden = self.fc5(bn_x)\n",
    "        relu = self.relu(hidden)\n",
    "        bn_x = self.bn5(relu)\n",
    "        output = self.output(bn_x)\n",
    "        return output\n",
    "\n",
    "net5 = NN5(input_size=177, hidden_size_1=32, hidden_size_2=16, hidden_size_3=8, hidden_size_4=4, hidden_size_5=2).to(device)\n",
    "print(net5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training MSE Loss : 3.1147713661193848\n",
      "Epoch 1 Validation R^2_oos : 0.09682649374008179\n",
      "Epoch 2 : Training MSE Loss : 1.0857855081558228\n",
      "Epoch 2 Validation R^2_oos : 0.12495166063308716\n",
      "Epoch 3 : Training MSE Loss : 1.0187106132507324\n",
      "Epoch 3 Validation R^2_oos : 0.12788766622543335\n",
      "Epoch 4 : Training MSE Loss : 0.9978020191192627\n",
      "Epoch 4 Validation R^2_oos : 0.13164788484573364\n",
      "Epoch 5 : Training MSE Loss : 0.9847981929779053\n",
      "Epoch 5 Validation R^2_oos : 0.1336836814880371\n",
      "Epoch 6 : Training MSE Loss : 0.9789547920227051\n",
      "Epoch 6 Validation R^2_oos : 0.13331443071365356\n",
      "Epoch 7 : Training MSE Loss : 0.975648820400238\n",
      "Epoch 7 Validation R^2_oos : 0.13371747732162476\n",
      "Epoch 8 : Training MSE Loss : 0.9729490280151367\n",
      "Epoch 8 Validation R^2_oos : 0.13387048244476318\n",
      "Epoch 9 : Training MSE Loss : 0.9704071283340454\n",
      "Epoch 9 Validation R^2_oos : 0.13368433713912964\n",
      "Epoch 10 : Training MSE Loss : 0.9683550000190735\n",
      "Epoch 10 Validation R^2_oos : 0.13348931074142456\n",
      "Early stopping triggered and the best validation R^2: 0.13387048244476318\n",
      "Trainin stopped after 10 epochs\n"
     ]
    }
   ],
   "source": [
    "### Training for NN5\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-5\n",
    "epsilon = 0.0005\n",
    "train_model(net5, data_train, data_valid, learning_rate, l1_lambda, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net5.state_dict(), \"NN5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_sq_oos in the following year:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1157, grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(net5, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a recursive training loop for NN5\n",
    "BATCH_SIZE = 10000\n",
    "### Training for NN5\n",
    "learning_rate = 5e-3\n",
    "# l1 regularization\n",
    "l1_lambda = 1e-4\n",
    "epsilon = 0.001\n",
    "\n",
    "# Here year represents \n",
    "for year in range(1975,2005):\n",
    "    data = stock_data(stockdata=df.loc[df['yyyymm']//100 < year,:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(data)\n",
    "    \n",
    "    dataset_valid = stock_data(stockdata=df.loc[((df['yyyymm']//100 >= year)&(df['yyyymm']//100 < year+12)),:].reset_index().iloc[:,1:], predictor_col=predictor_col)\n",
    "    data_valid = DataLoader(dataset = dataset_valid, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(dataset_valid)\n",
    "    \n",
    "    net = NN5(input_size=177, hidden_size_1=32, hidden_size_2=16, hidden_size_3=8, hidden_size_4=4, hidden_size_5=2).to(device)\n",
    "    train_model(net, data_train, data_valid, learning_rate, l1_lambda, epsilon, printout=False)\n",
    "    \n",
    "    del(data_train)\n",
    "    del(data_valid)\n",
    "    gc.collect()\n",
    "    \n",
    "    filename = \"NN5_\" + str(year+12) + \"_end.pt\"\n",
    "    print(filename)\n",
    "    torch.save(net.state_dict(), filename)\n",
    "    del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_sq_list_NN5 = []\n",
    "cum_feat_imp_NN5 = np.zeros(len(predictor_col))\n",
    "\n",
    "for year in range(1987,2016):\n",
    "    \n",
    "    net5 = NN5(input_size=177, hidden_size_1=32, hidden_size_2=16, hidden_size_3=8, hidden_size_4=4, hidden_size_5=2).to(device)\n",
    "    filename = \"NN5_\" + str(year) + \"_end.pt\"\n",
    "    net5.load_state_dict(torch.load(filename))\n",
    "    year_test_start = year\n",
    "    \n",
    "    # Note the coming 12 years of data are for validation but not for testing or model estimates\n",
    "    # So we test the model fitted with 1957-1974 data with 1987 data, and so on\n",
    "    dataset_test = stock_data(stockdata=df[((df['yyyymm']//100 >= year_test_start) & (df['yyyymm']//100 < year_test_start+1))], predictor_col=predictor_col)\n",
    "    data_test = DataLoader(dataset = dataset_test, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del dataset_test\n",
    "    \n",
    "    # Save the R_sq_oos for each year\n",
    "    R_sq_oos = test_model(net5, data_test).item()\n",
    "    R_sq_list_NN5.append(R_sq_oos)\n",
    "    del(data_test)\n",
    "    \n",
    "    # At the same time we find the feature importance of the net\n",
    "    \n",
    "    year_train_end = year-12\n",
    "    data = stock_data(stockdata=df[df['yyyymm']//100 < year_train_end], predictor_col=predictor_col)\n",
    "    BATCH_SIZE = 10000\n",
    "    data_train = DataLoader(dataset = data, batch_size = BATCH_SIZE, drop_last=True, shuffle=False)\n",
    "    del(data)\n",
    "    cum_feat_imp_NN5 += feat_imp_net(net5, data_train)\n",
    "    del(data_train)\n",
    "    print(\"Year {} completed\".format(year))\n",
    "    \n",
    "R_sq_list_NN5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "##################################### 6. Elastic Net   #############################################\n",
    "####################################################################################################\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from numpy import unravel_index\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the Elastic Net model and fit\n",
    "# After filling nan with cross-sectional medians and doing de-means, we fill the rest of nan with zeros\n",
    "# This can be skipped if you have already done this in other sections\n",
    "df = df.fillna(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit our data to the first training sample\n",
    "year1 = 1957\n",
    "year2 = 1975\n",
    "index_start = np.min(df[df['yyyymm']//100 >=year1].index.to_list())\n",
    "index_end = np.max(df[df['yyyymm']//100 <=year2].index.to_list())\n",
    "#print(index_start)\n",
    "#index_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419282"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_valid_end = np.max(df[df['yyyymm']//100 <=year2+12].index.to_list())\n",
    "index_test_end = np.max(df[df['yyyymm']//100 <=year2+13].index.to_list())\n",
    "#print(index_valid_end)\n",
    "index_test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.001, epsilon=0.05, l1_ratio=0.5, loss='huber',\n",
       "             max_iter=1000000.0, penalty='elasticnet', shuffle=False)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SGDRegressor(loss='huber',penalty='elasticnet',alpha=1e-3,l1_ratio=0.5, epsilon=0.05,max_iter=1e6,shuffle=False)\n",
    "model2.fit(X=df.loc[:index_end,predictor_col].values, y=df.loc[:index_end,'excess_return'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1406791900060662"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model2.predict(df.loc[(index_end+1):index_valid_end,predictor_col].values)\n",
    "y_hat\n",
    "error = y_hat-df.loc[(index_end+1):index_valid_end,'excess_return'].values\n",
    "y = df.loc[(index_end+1):index_valid_end,'excess_return'].values\n",
    "R_sq_oos_ENet_valid = 1-sum(error*error)/sum(y*y)\n",
    "R_sq_oos_ENet_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05551768, -0.05897161, -0.07056275, ..., -0.0608321 ,\n",
       "       -0.06301658, -0.07021729])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(df.loc[(index_end+1):index_valid_end,predictor_col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this model to a file\n",
    "filename = 'ElasticNet_1975_end.sav'\n",
    "pickle.dump(model2, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do a mini-grid search to fine-tune the parameters\n",
    "alpha_list = [1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\n",
    "epsilon_list = [2,1.5,1,0.5,0.25,0.1,0.05,0.01]\n",
    "R_sq_table_on_paras = np.zeros((len(alpha_list),len(epsilon_list)))\n",
    "row_count, col_count = 0, 0\n",
    "for i in range(0,len(alpha_list)):\n",
    "    for j in range(0,len(epsilon_list)):\n",
    "        model2 = SGDRegressor(loss='huber',penalty='elasticnet',alpha=alpha_list[i],l1_ratio=0.5, epsilon=epsilon_list[j],max_iter=1e6,shuffle=False)\n",
    "        model2.fit(X=df.loc[:index_end,predictor_col], y=df.loc[:index_end,'excess_return'])\n",
    "        \n",
    "        # Compute validation R_sq_oos\n",
    "        y_hat = model2.predict(df.loc[(index_end+1):index_valid_end,predictor_col])\n",
    "        error = y_hat-df.loc[(index_end+1):index_valid_end,'excess_return'].values\n",
    "        y = df.loc[(index_end+1):index_valid_end,'excess_return'].values\n",
    "        R_sq_oos_ENet_valid = 1-sum(error*error)/sum(y*y)\n",
    "        R_sq_table_on_paras[i,j] = R_sq_oos_ENet_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13437115  0.13437136  0.13449889  0.13572966  0.13830325  0.14057917\n",
      "   0.14105062  0.14155384]\n",
      " [ 0.13437115  0.13437136  0.13449889  0.13572966  0.13832375  0.14060679\n",
      "   0.14105062  0.14155384]\n",
      " [ 0.134473    0.13452549  0.13464561  0.13585127  0.13837606  0.14060679\n",
      "   0.14106374  0.14158759]\n",
      " [ 0.13467633  0.13462766  0.13474415  0.13593484  0.13843307  0.14065455\n",
      "   0.14106462  0.14163717]\n",
      " [ 0.13655382  0.13671244  0.13654115  0.13693827  0.13855844  0.14070149\n",
      "   0.14083425  0.14164336]\n",
      " [ 0.13661396  0.12738323  0.13696857  0.13779884  0.14005244  0.12979707\n",
      "   0.14161019  0.14164712]\n",
      " [ 0.13318272  0.13375284  0.13385425  0.13562613  0.1387106   0.13035376\n",
      "   0.14067919  0.13858187]\n",
      " [ 0.13056612  0.12956469  0.12712995  0.12804904  0.13427343  0.13827315\n",
      "   0.13702916  0.13853529]\n",
      " [ 0.11222691  0.11184487  0.11201977  0.11672148 -0.17594548  0.12925524\n",
      "   0.13232242  0.13657232]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 7)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(R_sq_table_on_paras)\n",
    "unravel_index(R_sq_table_on_paras.argmax(), R_sq_table_on_paras.shape)\n",
    "# Going down the row is alpha, going from left to right is epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:438: UserWarning: X has feature names, but SGDRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08873837088823988"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute testing R_sq_oos\n",
    "y_hat_test = model2.predict(df.loc[(index_valid_end+1):index_test_end,predictor_col])\n",
    "error = y_hat_test-df.loc[(index_valid_end+1):index_test_end,'excess_return'].values\n",
    "y = df.loc[(index_valid_end+1):index_test_end,'excess_return'].values\n",
    "R_sq_oos_ENet_test = 1-sum(error*error)/sum(y*y)\n",
    "R_sq_oos_ENet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define a function to find feature importance\n",
    "def feature_importance(model,data,response,predictors):\n",
    "    # First calculate full model R_sq\n",
    "    y_hat = model.predict(data.loc[:,predictors].values)\n",
    "    y = data.loc[:,response].values\n",
    "    error = y_hat-y\n",
    "    R_sq = 1-sum(error*error)/sum(y*y)\n",
    "    #print(R_sq)\n",
    "    \n",
    "    # Create a empty data for storing the feature importance\n",
    "    R_sq_loss_table = data.loc[0,predictors]\n",
    "    R_sq_loss_table[predictors] = 0\n",
    "    \n",
    "    for pred in predictors:\n",
    "        # We adjust the y_hat by removing the term corresponding feature x features weight\n",
    "        y_hat_new = y_hat - model.coef_[predictor_col.index(pred)]*data[pred]\n",
    "        error_new = y_hat_new-y\n",
    "        R_sq_new = 1-sum(error_new*error_new)/sum(y*y)\n",
    "        #print(R_sq_new)\n",
    "        \n",
    "        # Compute loss in R_sq\n",
    "        loss_in_R2 = R_sq - R_sq_new\n",
    "        R_sq_loss_table[pred] = loss_in_R2\n",
    "    \n",
    "    R_sq_loss_table = R_sq_loss_table/sum(R_sq_loss_table)\n",
    "    return R_sq_loss_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year ended 1975 training completed\n",
      "Year ended 1976 training completed\n",
      "Year ended 1977 training completed\n",
      "Year ended 1978 training completed\n",
      "Year ended 1979 training completed\n",
      "Year ended 1980 training completed\n",
      "Year ended 1981 training completed\n",
      "Year ended 1982 training completed\n",
      "Year ended 1983 training completed\n",
      "Year ended 1984 training completed\n",
      "Year ended 1985 training completed\n",
      "Year ended 1986 training completed\n",
      "Year ended 1987 training completed\n",
      "Year ended 1988 training completed\n",
      "Year ended 1989 training completed\n",
      "Year ended 1990 training completed\n",
      "Year ended 1991 training completed\n",
      "Year ended 1992 training completed\n",
      "Year ended 1993 training completed\n",
      "Year ended 1994 training completed\n",
      "Year ended 1995 training completed\n",
      "Year ended 1996 training completed\n",
      "Year ended 1997 training completed\n",
      "Year ended 1998 training completed\n",
      "Year ended 1999 training completed\n",
      "Year ended 2000 training completed\n",
      "Year ended 2001 training completed\n",
      "Year ended 2002 training completed\n",
      "Year ended 2003 training completed\n",
      "Year ended 2004 training completed\n"
     ]
    }
   ],
   "source": [
    "### We do a recursive training and performance evaluation for elastic net\n",
    "### This process takes roughly 4-5 hours\n",
    "\n",
    "R_sq_list = []\n",
    "cum_feat_imp = df.loc[0,predictor_col]\n",
    "cum_feat_imp[predictor_col] = 0\n",
    "\n",
    "for year in range(1975,2005):\n",
    "    train_index_end = np.max(df[df['yyyymm']//100 <year].index.to_list())\n",
    "    model2 = SGDRegressor(loss='huber',penalty='elasticnet',alpha=1e-3,l1_ratio=0.5, epsilon=0.05,max_iter=1e6,shuffle=False)\n",
    "    model2.fit(X=df.loc[:train_index_end,predictor_col].values, y=df.loc[:train_index_end,'excess_return'].values)\n",
    "    \n",
    "    #Save the model, where the filename contains the last year of the training data set\n",
    "    filename = 'ElasticNet_' + str(year) + '_end.sav'\n",
    "    pickle.dump(model2, open(filename, 'wb'))\n",
    "    \n",
    "    # Note the coming 12 years of data are for validation but not for testing or model estimates\n",
    "    # So we test the model fitted with 1957-1974 data with 1987 data, and so on\n",
    "    test_index_start = np.max(df[df['yyyymm']//100 <year+12].index.to_list())+1\n",
    "    test_index_end = np.max(df[df['yyyymm']//100 <year+13].index.to_list())\n",
    "    y_hat_test = model2.predict(df.loc[test_index_start:test_index_end,predictor_col].values)\n",
    "    y = df.loc[test_index_start:test_index_end,'excess_return'].values\n",
    "    error = y_hat_test-y\n",
    "    R_sq_oos_ENet_test = 1-sum(error*error)/sum(y*y)\n",
    "    R_sq_list.append(R_sq_oos_ENet_test)\n",
    "    \n",
    "    # Feature Importance\n",
    "    feat_imp = feature_importance(model=model2,data=df.loc[:train_index_end,:],response='excess_return',predictors=predictor_col)\n",
    "    cum_feat_imp += feat_imp\n",
    "    print(\"Year ended {} training completed\".format(year))\n",
    "    del(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.001, epsilon=0.01, l1_ratio=0.5, loss='huber',\n",
       "             max_iter=1000000.0, penalty='elasticnet', shuffle=False)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = pickle.load(open('ElasticNet_1989_end.sav', 'rb'))\n",
    "#feat_imp = feature_importance(model=loaded_model,data=df.loc[:index_end,:],response='excess_return',predictors=predictor_col)\n",
    "alpha_best = loaded_model.best_params_['alpha']\n",
    "epsilon_best = loaded_model.best_params_['epsilon']\n",
    "model2 = SGDRegressor(loss='huber',penalty='elasticnet',l1_ratio=0.5,alpha=alpha_best, epsilon=epsilon_best,max_iter=1e6,shuffle=False)\n",
    "model2.fit(X=df.loc[:index_end,predictor_col].values, y=df.loc[:index_end,'excess_return'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08981932579571605"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_test = model_gs.predict(df.loc[(index_valid_end+1):index_test_end,predictor_col])\n",
    "error = y_hat_test-df.loc[(index_valid_end+1):index_test_end,'excess_return'].values\n",
    "y = df.loc[(index_valid_end+1):index_test_end,'excess_return'].values\n",
    "R_sq_oos_ENet_test = 1-sum(error*error)/sum(y*y)\n",
    "R_sq_oos_ENet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14867141597362843"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = pickle.load(open('ElasticNet_1989_end.sav', 'rb'))\n",
    "test_index_start = np.max(df[df['yyyymm']//100 <=1989].index.to_list())+1\n",
    "test_index_end = np.max(df[df['yyyymm']//100 <=1990].index.to_list())\n",
    "\n",
    "y_hat_test = loaded_model.predict(df.loc[test_index_start:test_index_end,predictor_col])\n",
    "y = df.loc[test_index_start:test_index_end,'excess_return'].values\n",
    "error = y_hat_test-y\n",
    "R_sq_oos_ENet_test = 1-sum(error*error)/sum(y*y)\n",
    "R_sq_oos_ENet_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mom1m       0.306123\n",
       "retvol      0.207962\n",
       "age         0.194719\n",
       "mom12m      0.076180\n",
       "ms          0.063263\n",
       "              ...   \n",
       "tb          0.000000\n",
       "aeavol      0.000000\n",
       "sic2_nan    0.000000\n",
       "chmom      -0.000002\n",
       "idiovol    -0.072731\n",
       "Name: 0, Length: 177, dtype: float64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_feat_imp.sort_values(ascending=False)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 12 artists>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAFlCAYAAACA124VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAc4ElEQVR4nO3de5RlZX3m8e8jaLyATYTGeMOOgOIFZKwSRRHB6eV4mYgEjY7E+6yOF4ZohpnlIhMH41LxEo3GMdphvEUcXV5wSBQkIurYgukqgW5QRGbA8bZGUJeiIKL9mz/ObqkU1dWnuqrOPm+d72ets/rss/e792+/vWme9e5bqgpJkiS16Q59FyBJkqQ9Z5iTJElqmGFOkiSpYYY5SZKkhhnmJEmSGmaYkyRJatjefRfQlwMOOKA2bNjQdxmSJEm7NTs7e0NVrV9o3sSGuQ0bNjAzM9N3GZIkSbuV5Nu7mudpVkmSpIYZ5iRJkhpmmJMkSWqYYU6SJKlhhjlJkqSGGeYkSZIaZpiTJElqmGFOkiSpYYY5SZKkhhnmJEmSGmaYkyRJaphhTpIkqWF7911AX2ZnIem7CkmS1KqqvisYcGROkiSpYYY5SZKkhhnmJEmSGmaYkyRJaphhTpIkqWGGOUmSpIaNdZhLckqSa5JUkgP6rkeSJGncjHWYA7YAG4Fv912IJEnSONrjhwYn2QCcD3wZeDRwOfA+4DXAgcDJwDXAe4EHADcBm6pqW5IzgN8H7gU8EPizbh1PBr4H/EFV3VpVl3bbmr/todrv6b5JkiS1Yrkjc4cAbweOAA4DngMcA5wGnM4g2F1aVUd00x+c0/Zg4KnACcCHgIuq6nDg5u733Vlue0mSpOYt93Ve11bVdoAkVwIXVlUl2Q5sAO4PnARQVZ9Psn+SdV3b86rq1m7ZvRiM8gHsbLs7S26fZBOwaTB10PB7KUmSNKaWOzJ3y5zvO+ZM72AQFBd6++nON5ndAlBVO4Bbq377hrOdbYfa9lLaV9XmqpquqmlYP8QmJEmSxttq3wDxJQbXzpHkOOCGqvrZKm9TkiRpYqx2mDsDmE6yDTgTeP5SGic5Ncl3gfsC25KctfIlSpIktSu3nZ2cLMl0wUzfZUiSpEaNMkIlmR1cJnZ74/6cOUmSJC3CMCdJktQww5wkSVLDDHOSJEkNM8xJkiQ1bLlvgGjW1BTMeDOrJElqnCNzkiRJDTPMSZIkNcwwJ0mS1DDDnCRJUsMm9gaI2VlI+q5CkrSQCX3TpLRHHJmTJElqmGFOkiSpYYY5SZKkhhnmJEmSGmaYkyRJatjYhbkk+yV52TLaH5fkH1eyJkmSpHHVW5jLwELb3w/Y4zAnSZI0SUYa5pJsSPKNJO8Cvgb8RZKtSbYleU232JnAwUkuS/LmJB9N8pQ563h/kpOS3DnJ+5JsT3JpkuNHuS+SJEnjoI+HBj8IeCHwKeAZwFFAgHOTHAu8CnhYVR0JkORE4FnAZ5LcCfjXwEuBlwNU1eFJDgMuSPLA0e6KJElSv/o4zfrtqroEeGL3uZTBKN1hwKELLH8e8IQkvwM8GfhSVd0MHAP8PUBVXQV8G1g0zCXZlGQmyQxcv1L7I0mS1Js+RuZ+0f0Z4A1V9Z65M5NsmDtdVb9M8gXg3zAYofsfc9ovSVVtBjYPtjPty2IkSVLz+ryb9bPAi5LsA5DkPkkOBG4E9p237EcYnJp9XNcO4EvAyV3bBwIHAd8cQd2SJEljo7cwV1UXAB8GLk6yHfg4sG9V/QjYkuSKJG/uFr8AOBb4XFX9qvvtXcBeXduPAi+oqltGuxeSJEn9StVknm0cnGad6bsMSdICJvR/TdIuJZmtqumF5o3dQ4MlSZI0PMOcJElSwwxzkiRJDTPMSZIkNcwwJ0mS1LA+Hho8FqamYMabWSVJUuMcmZMkSWqYYU6SJKlhhjlJkqSGGeYkSZIaNrE3QMzOQtJ3FZLULl+5JY0HR+YkSZIaZpiTJElqmGFOkiSpYYY5SZKkhhnmJEmSGmaYkyRJaphhTpIkqWFjG+aSfCrJbJIrk2zqfntxkquTfCHJ3yV5Z/f7+iSfSLK1+zy23+olSZJGY5wfGvyiqvpxkrsAW5N8GvgL4BHAjcDngcu7Zd8OvK2qvpzkIOCzwIP7KFqSJGmUxjnMnZrkxO77/YDnAl+sqh8DJPkY8MBu/kbgIbntlQ53T7JvVd04d4XdCN+mwdRBq1q8JEnSKIxlmEtyHIOAdnRV3ZTkC8A32fVo2x26ZW9ebL1VtRnYPNjGtC+ikSRJzRvXa+bWAT/pgtxhwKOBuwKPT/K7SfYGTpqz/AXAKTsnkhw5ymIlSZL6Mq5h7nxg7yTbgNcClwDfA14PfBX4HPB14Kfd8qcC00m2Jfk68JLRlyxJkjR6Y3matapuAZ48//ckM1W1uRuZO4fBiBxVdQPwrNFWKUmS1L9xHZnblTOSXAZcAVwLfKrXaiRJkno2liNzu1JVp/VdgyRJ0jhpbWROkiRJcxjmJEmSGmaYkyRJalhT18ytpKkpmJnpuwpJkqTlcWROkiSpYYY5SZKkhhnmJEmSGmaYkyRJatjE3gAxOwtJ31VoXFT1XYEkSXvGkTlJkqSGGeYkSZIaZpiTJElqmGFOkiSpYYY5SZKkho1FmEtySpJrklSSA+b8fnKSbd3nK0ke3medkiRJ42YswhywBdgIfHve79cCj6+qI4DXAptHXZgkSdI4222YS7IhyVVJzkpyRZKzk2xMsiXJt5IcleQeST7VjaBdkuSIru0ZST6Q5IIk1yX5wyRvSrI9yflJ7ghQVZdW1XXzt11VX6mqn3STlwD3HbamFeshSZKkMTbsyNwhwNuBI4DDgOcAxwCnAacDrwEu7UbQTgc+OKftwcBTgROADwEXVdXhwM3d78N6MXDeEmqSJEla84Z9A8S1VbUdIMmVwIVVVUm2AxuA+wMnAVTV55Psn2Rd1/a8qrq1W3Yv4Pzu951tdyvJ8QzC3DFLqGmh9WwCNg2mDhpm05IkSWNt2JG5W+Z83zFnegeDQLjQi7F2viDpFoCq2gHcWvXbFyftbLuo7pTtWcAJVfWjJdR0+4KqNlfVdFVNw/rdbVqSJGnsrdQNEF8CTgZIchxwQ1X9bLkrTXIQ8EnguVV19XLXJ0mStNasVJg7A5hOsg04E3j+UhonOTXJdxnc4LAtyVndrFcD+wPvSnJZkpkVqleSJGlNyG1nPSdLMl1gNtTAhP5nIElqRJLZwWVitzcuz5mTJEnSHjDMSZIkNcwwJ0mS1DDDnCRJUsMMc5IkSQ0b9g0Qa87UFMx4M6skSWqcI3OSJEkNM8xJkiQ1zDAnSZLUMMOcJElSwyb2BojZWUj6rkJL5Wu3JEn6lxyZkyRJaphhTpIkqWGGOUmSpIYZ5iRJkhpmmJMkSWqYYU6SJKlhhjlJkqSGGeYkSZIa1kSYS7IhyVVJzkpyRZKzk2xMsiXJt5IcleTxSS7rPpcm2bfvuiVJklZbS2+AOAR4JrAJ2Ao8BzgGeBpwOrAX8PKq2pJkH+CX81eQZFPXHjhoJEVLkiStpiZG5jrXVtX2qtoBXAlcWFUFbAc2AFuAtyY5Fdivqn49fwVVtbmqpqtqGtaPsnZJkqRV0VKYu2XO9x1zpncAe1fVmcC/B+4CXJLksBHXJ0mSNHItnWZdVJKDq2o7sD3J0cBhwFU9lyVJkrSqWhqZ251XdDdHXA7cDJzXd0GSJEmrLYPLziZPMl0w03cZWqIJPVwlSRMuyezgmv/bW0sjc5IkSRPHMCdJktQww5wkSVLDDHOSJEkNWzOPJlmqqSmY8f4HSZLUOEfmJEmSGmaYkyRJaphhTpIkqWGGOUmSpIZN7A0Qs7OQ9F1Fe3wDgyRJ48WROUmSpIYZ5iRJkhpmmJMkSWqYYU6SJKlhhjlJkqSGGeYkSZIatqbCXJLjkjym7zokSZJGpbkwl2SxZ+MdBxjmJEnSxFjxMJdkQ5KrkpyV5IokZyfZmGRLkm8lOar7fCXJpd2fD+ra/lmS93bfD+/a3zXJGUk2J7kA+GCS9Uk+kWRr93lskg3AS4BXJrksyeNWet8kSZLGzWq9AeIQ4JnAJmAr8BzgGOBpwOnA84Bjq+rXSTYCrwdOAv4a+EKSE4E/B/6kqm7K4FUNU8AxVXVzkg8Db6uqLyc5CPhsVT04ybuBn1fVWxYqKsmmribgoFXZcUmSpFFarTB3bVVtB0hyJXBhVVWS7cAGYB3wgSSHAgXcEaCqdiR5AbANeE9VbZmzznOr6ubu+0bgIbntfVx3T7Lv7oqqqs3A5kFd076YSpIkNW+1wtwtc77vmDO9o9vma4GLqurE7vToF+Ysfyjwc+De89b5iznf7wAcPSfcARBftipJkiZMXzdArAO+131/wc4fk6wD3g4cC+yf5Bm7aH8BcMqcdkd2X28EdjtCJ0mStFb0FebeBLwhyRZgrzm/vw14V1VdDbwYODPJgQu0PxWYTrItydcZ3PgA8A/Aid4AIUmSJkWqJvPSscE1czN9l9GcCT1cJEnqVZLZqppeaF5zz5mTJEnSbQxzkiRJDTPMSZIkNcwwJ0mS1LDVes7c2Juaghnvf5AkSY1zZE6SJKlhhjlJkqSGGeYkSZIaZpiTJElq2MTeADE7C0nfVfTLtzlIktQ+R+YkSZIaZpiTJElqmGFOkiSpYYY5SZKkhhnmJEmSGmaYkyRJatiqhrkk70/yjNXcRred65IcsNrbkSRJGjeOzEmSJDVsyWEuyd2SfDrJ5UmuSPKsJK9OsrWb3pzc/nG8SaaSfDHJbJLPJrlX9/shST7Xre9rSQ5Osk+SC7vp7UlO2NW252ziP8xZ/rA97hFJkqSG7MnI3JOA71fVw6vqYcD5wDur6pHd9F2Afzu3QZI7An8DPKOqpoD3Aq/rZp8N/LeqejjwGOAHwC+BE6vqEcDxwF91AXGhbe90Q7f83wKnLVR4kk1JZpLMwPV7sOuSJEnjZU/C3HZgY5I3JnlcVf0UOD7JV5NsB54APHRemwcBDwP+KcllwH8B7ptkX+A+VXUOQFX9sqpuAgK8Psk24HPAfYB77mLbO32y+3MW2LBQ4VW1uaqmq2oa1u/BrkuSJI2XJb+btaquTjIFPAV4Q5ILgJcD01X1nSRnAHee1yzAlVV19L/4Mbn7LjZzMoO0NVVVtya5DrjzQtuuqr/s2tzS/fmbPdkvSZKkFu3JNXP3Bm6qqg8BbwEe0c26Ick+wEJ3r34TWJ/k6G4dd0zy0Kr6GfDdJE/vfv+dJHcF1gE/7ILc8cD9d7NtSZKkibQnI1iHA29OsgO4FXgp8HQGp0CvA7bOb1BVv+oeUfKOJOu67f41cCXwXOA9Sf6yW98zGVxH9w+Da9u4DLhqkW1LkiRNrFRV3zX0IpkumOm7jF5N6F+9JEnNSTI7uOb/9nzOnCRJUsMMc5IkSQ0zzEmSJDXMMCdJktSwiX0e29QUzEz2/Q+SJGkNcGROkiSpYYY5SZKkhhnmJEmSGmaYkyRJatjE3gAxOwtJ31WMnm99kCRpbXFkTpIkqWGGOUmSpIYZ5iRJkhpmmJMkSWqYYU6SJKlhhjlJkqSGNRHmkvy87xokSZLGURNhTpIkSQtrLswl+U9JtibZluQ13W9vTPKyOcuckeQ/9lelJEnSaDQV5pI8ETgUOAo4EphKcizwEeBZcxb9I+BjC7TflGQmyQxcP4KKJUmSVldrr/N6Yve5tJveBzi0qv57kgOT3BtYD/ykqv7v/MZVtRnYDJBM+2IrSZLUvNbCXIA3VNV7Fpj3ceAZwO8xGKmTJEla85o6zQp8FnhRkn0AktwnyYHdvI8Az2YQ6D7eU32SJEkj1dTIXFVdkOTBwMVJAH4O/DHww6q6Msm+wPeq6gd91ilJkjQqqZrMS8cG18zN9F3GyE3oX7ckSU1LMltV0wvNa+00qyRJkuYwzEmSJDXMMCdJktQww5wkSVLDmrqbdSVNTcHM5N3/IEmS1hhH5iRJkhpmmJMkSWqYYU6SJKlhhjlJkqSGTewNELOzMHgj2Nrl2x4kSVr7HJmTJElqmGFOkiSpYYY5SZKkhhnmJEmSGmaYkyRJaphhTpIkqWGGOUmSpIY1G+aSPC3Jq/quQ5IkqU/NPjS4qs4Fzu27DkmSpD6t+Mhckg1JrkpyVpIrkpydZGOSLUm+leSoJNcl2W9Om2uS3DPJ+iSfSLK1+zx2ke28IMk7u+9/kOSrSS5N8rkk91zp/ZIkSRpHq3Wa9RDg7cARwGHAc4BjgNOA04H/CZwIkORRwHVV9f+6Nm+rqkcCJwFnDbm9LwOPrqp/BXwE+M8LLZRkU5KZJDNw/Z7umyRJ0thYrdOs11bVdoAkVwIXVlUl2Q5sAN4EvBp4H/Bs4KNdu43AQ3LbS1PvnmTfqrpxN9u7L/DRJPcC7gRcu9BCVbUZ2Dyoa9o3l0qSpOat1sjcLXO+75gzvYNBgLwYOCTJeuDpwCfn1HN0VR3Zfe4zRJAD+BvgnVV1OPAnwJ1XYB8kSZLGXi93s1ZVAecAbwW+UVU/6mZdAJyyc7kkRw65ynXA97rvz1+hMiVJksZen48m+Sjwx9x2ihXgVGA6ybYkXwdeMuS6zgA+luR/ATesaJWSJEljLINBsskzuGZupu8yVtWE/tVKkrTmJJmtqumF5jX70GBJkiQ18NDgJC8E/nTez1uq6uV91CNJkjROxj7MVdX7GDzCRJIkSfN4mlWSJKlhYz8yt1qmpmBmbd//IEmSJoAjc5IkSQ0zzEmSJDXMMCdJktQww5wkSVLDJvYGiNlZSPquYml8o4MkSZrPkTlJkqSGGeYkSZIaZpiTJElqmGFOkiSpYYY5SZKkho1dmEvyiiR33cW8FyR5527an5rkG0nOXp0KJUmSxsfYhTngFcCCYW5ILwOeUlUnr0w5kiRJ46vXMJfkbkk+neTyJFck+a/AvYGLklzULfPCJFcn+SLw2Dlt75nknK7t5Ukek+TdwAOAc5O8spedkiRJGqG+Hxr8JOD7VfVUgCTrgBcCx1fVDUnuBbwGmAJ+ClwEXNq1fQfwxao6MclewD5V9ZIkT9rZftQ7I0mSNGp9n2bdDmxM8sYkj6uqn86b/yjgC1V1fVX9CvjonHlPAP4WoKp+s0Db20myKclMkhm4fqX2QZIkqTe9hrmquprBqNt24A1JXr3QYiu4vc1VNV1V07B+pVYrSZLUm76vmbs3cFNVfQh4C/AI4EZg326RrwLHJdk/yR2BZ85pfiHw0m49eyW5++gqlyRJGg99XzN3OPDmJDuAWxmEs6OB85L8oKqOT3IGcDHwA+BrwF5d2z8FNid5MfCbru3FI65fkiSpV6lasbOYTUmmC2b6LmNJJvSvSpKkiZdkdnCZ2O31fQOEJEmSlsEwJ0mS1DDDnCRJUsMMc5IkSQ0zzEmSJDVsYsPc1NTg7tCWPpIkSfNNbJiTJElaCwxzkiRJDTPMSZIkNcwwJ0mS1LC+383am9lZSFZufd6gIEmS+uDInCRJUsMMc5IkSQ0zzEmSJDXMMCdJktQww5wkSVLDDHOSJEkNM8xJkiQ1rLkwl2RDkquSfCDJtiQfT3LXJGcm+Xr321v6rlOSJGkUWn1o8IOAF1fVliTvBU4BTgQOq6pKsl+v1UmSJI1IcyNzne9U1Zbu+4eAY4FfAmcl+UPgpoUaJdmUZCbJDFw/olIlSZJWT6thbv7Ls24FjgI+ATwdOH/BRlWbq2q6qqZh/epWKEmSNAKtnmY9KMnRVXUx8O+Ay4B1VfWZJJcA1/RanSRJ0oi0OjL3DeD5SbYB9wDOAv6xm/4i8Mo+i5MkSRqVVkfmdlTVS+b9dlQvlUiSJPWo1ZE5SZIk0eDIXFVdBzys7zokSZLGgSNzkiRJDTPMSZIkNcwwJ0mS1LCJDXNTU1C1ch9JkqQ+TGyYkyRJWgsMc5IkSQ0zzEmSJDXMMCdJktSw5h4avFJmZyHZ8/be9CBJksaBI3OSJEkNM8xJkiQ1zDAnSZLUMMOcJElSwwxzkiRJDVtTYS7J6X3XIEmSNEpjHeaS7LXY9AIMc5IkaaKMLMwleV6SbUkuT/L3Sd6f5Blz5v+8+/O4JBcl+TCwff50t8ynkswmuTLJpu63M4G7JLksydmj2i9JkqQ+jeShwUkeCvw58NiquiHJPYC3LtLkKOBhVXVtkuPmTnfzX1RVP05yF2Brkk9U1auSnFJVR67enkiSJI2XUY3MPQH4eFXdAFBVP97N8v88J7gtNH1qksuBS4D7AYcOU0SSTUlmkszA9UsoX5IkaTyNKswFmP8CrF/v3H6SAHeaM+8X85b97XQ3UrcROLqqHg5cCtx5mCKqanNVTVfVNKxfSv2SJEljaVRh7kLgj5LsD9CdZr0OmOrmnwDccch1rQN+UlU3JTkMePScebcmGXY9kiRJzRtJmKuqK4HXAV/sTo++Ffg74PFJ/hl4FLcfjduV84G9k2wDXsvgVOtOm4Ft3gAhSZImRarmn/2cDMl0wcwet5/QbpMkST1IMju4TOz2xvo5c5IkSVqcYU6SJKlhhjlJkqSGGeYkSZIaZpiTJElq2MSGuampwR2pe/qRJEkaBxMb5iRJktYCw5wkSVLDDHOSJEkNM8xJkiQ1zDAnSZLUMMOcJElSwwxzkiRJDTPMSZIkNcwwJ0mS1DDDnCRJUsMMc5IkSQ0zzEmSJDXMMCdJktQww5wkSVLDUlV919CLJDcC3+y7jjXqAOCGvotYg+zX1WPfrh77dvXYt6tnHPv2/lW1fqEZe4+6kjHyzaqa7ruItSjJjH278uzX1WPfrh77dvXYt6untb71NKskSVLDDHOSJEkNm+Qwt7nvAtYw+3Z12K+rx75dPfbt6rFvV09TfTuxN0BIkiStBZM8MidJktS8NRfmkjwpyTeTXJPkVQvMT5J3dPO3JXnEsG0n3TL79rok25NclmRmtJWPvyH69rAkFye5JclpS2k76ZbZtx63uzBEv57c/TuwLclXkjx82LaTbpl96zG7iCH69oSuXy9LMpPkmGHb9qqq1swH2Av438ADgDsBlwMPmbfMU4DzgACPBr46bNtJ/iynb7t51wEH9L0f4/gZsm8PBB4JvA44bSltJ/mznL7t5nnc7nm/Pgb43e77k/23dvX7tpv2mF1e3+7DbZegHQFcNWzbPj9rbWTuKOCaqvo/VfUr4CPACfOWOQH4YA1cAuyX5F5Dtp1ky+lbLW63fVtVP6yqrcCtS2074ZbTt9q1Yfr1K1X1k27yEuC+w7adcMvpWy1umL79eXXpDbgbUMO27dNaC3P3Ab4zZ/q73W/DLDNM20m2nL6FwX8QFySZTbJp1aps03KOPY/bxS23fzxuF7bUfn0xg1H7PWk7aZbTt+Axu5ih+jbJiUmuAj4NvGgpbfuy1t4AkQV+m3+77q6WGabtJFtO3wI8tqq+n+RA4J+SXFVVX1rRCtu1nGPP43Zxy+0fj9uFDd2vSY5nEDh2XnvkMbu45fQteMwuZqi+rapzgHOSHAu8Ftg4bNu+rLWRue8C95szfV/g+0MuM0zbSbacvqWqdv75Q+AcBkPWGljOsedxu7hl9Y/H7S4N1a9JjgDOAk6oqh8tpe0EW07feswubknHXheCD05ywFLbjtpaC3NbgUOT/H6SOwHPBs6dt8y5wPO6Oy8fDfy0qn4wZNtJtsd9m+RuSfYFSHI34InAFaMsfswt59jzuF3cHvePx+2idtuvSQ4CPgk8t6quXkrbCbfHfesxu1vD9O0hSdJ9fwSDmx1+NEzbPq2p06xV9eskpwCfZXDnyXur6sokL+nmvxv4DIO7Lq8BbgJeuFjbHnZjLC2nb4F7MhiyhsEx9+GqOn/EuzC2hunbJL8HzAB3B3YkeQWDO6l+5nG7a8vpW+AAPG4XNOS/B68G9gfe1fXhr6tq2n9rF7ecvsV/axc1ZN+exGBQ4lbgZuBZ3Q0RY33c+gYISZKkhq2106ySJEkTxTAnSZLUMMOcJElSwwxzkiRJDTPMSZIkNcwwJ0mS1DDDnCRJUsMMc5IkSQ37/6XfTr5zN9HcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLot the normalized average feature importance in the 30 training samples\n",
    "feat_imp = cum_feat_imp.sort_values(ascending=False)/30\n",
    "feat_imp_to_plot = feat_imp[0:12].sort_values(ascending=True)\n",
    "\n",
    "# Plot the feature importances in horizontal bar charts\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 6))\n",
    "ax = plt.subplot()\n",
    "ax.barh(list(feat_imp_to_plot.index),\n",
    "    feat_imp_to_plot, \n",
    "    align = 'center', color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "print(np.max(R_sq_list))\n",
    "print(np.median(R_sq_list))\n",
    "scipy.stats.trim_mean(R_sq_list, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project2_Empiracal_Asset_Pricing_20211103.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
