{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os.path as op\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = np.arange(2001,2020,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1403975, 64, 60)\n",
      "(1403975, 8)\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "label_df = []\n",
    "for year in year_list:\n",
    "    images.append(np.memmap(op.join(\"./monthly_20d\", f\"20d_month_has_vb_[20]_ma_{year}_images.dat\"), dtype=np.uint8, mode='r').reshape(\n",
    "                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20])))\n",
    "    label_df.append(pd.read_feather(op.join(\"./monthly_20d\", f\"20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\")))\n",
    "    \n",
    "images = np.concatenate(images)\n",
    "label_df = pd.concat(label_df)\n",
    "\n",
    "print(images.shape)\n",
    "print(label_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img, label):\n",
    "        self.img = torch.Tensor(img.copy())\n",
    "        self.label = torch.Tensor(label)\n",
    "        self.len = len(img)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(images, (label_df.Ret_20d > 0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_path = '/home/clidg/proj_2/pt/baseline_epoch_10_train_0.688653_eval_0.686537_.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load(net_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.device_ids = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Net(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(5, 3), stride=(3, 1), padding=(12, 1), dilation=(2, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(5, 3), stride=(3, 1), padding=(12, 1), dilation=(2, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(5, 3), stride=(3, 1), padding=(12, 1), dilation=(2, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (fc1): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=46080, out_features=2, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(dataloader, net, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    running_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    current = 0\n",
    "    net.eval()\n",
    "    target = []\n",
    "    result = []\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        y_pred = net(X)\n",
    "        target.append(y.detach().cpu().numpy())\n",
    "        result.append(y_pred.detach().cpu().numpy())\n",
    "        loss = loss_fn(y_pred, y.long())\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        total_loss = (len(X) * running_loss + total_loss * current) / (len(X) + current)\n",
    "        current += len(X)\n",
    "        running_loss = 0.0\n",
    "            \n",
    "    return total_loss, np.concatenate(result), np.concatenate(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loss, y_pred, y_target = eval_loop(test_dataloader, net, loss_fn)\n",
    "\n",
    "print(net_path)\n",
    "print(test_loss)\n",
    "\n",
    "#np.save('baseline_y_pred.npy', y_pred)\n",
    "#np.save('baseline_y_target.npy', y_target)\n",
    "#np.save('baseline_y_ret.npy', label_df.Ret_20d.values)\n",
    "\n",
    "acc = (np.argmax(y_pred, axis = 1) == y_target).sum()/len(y_pred)\n",
    "print(acc)\n",
    "\n",
    "\n",
    "print(pd.Series(y_pred[:,1]).corr(label_df.Ret_20d, method='pearson'))\n",
    "\n",
    "print(pd.Series(y_pred[:,1]).corr(label_df.Ret_20d, method='spearman'))\n",
    "\n",
    "ret = pd.Series(np.concatenate([label_df.Ret_20d.values[np.argmax(y_pred, axis = 1)], \n",
    "                -label_df.Ret_20d.values[1-np.argmax(y_pred, axis = 1)]]))\n",
    "\n",
    "print(ret.mean()/ret.std() * np.sqrt(252/20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/clidg/proj_2/pt/gelu_epoch_9_train_0.686015_eval_0.686316_.pt\n",
      "0.6935985478675337\n",
      "0.524918891005894\n",
      "0.003044967773768082\n",
      "0.004660699432851827\n",
      "0.050686978323795276\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loss, y_pred, y_target = eval_loop(test_dataloader, net, loss_fn)\n",
    "\n",
    "print(net_path)\n",
    "print(test_loss)\n",
    "\n",
    "acc = (np.argmax(y_pred, axis = 1) == y_target).sum()/len(y_pred)\n",
    "print(acc)\n",
    "\n",
    "\n",
    "print(pd.Series(y_pred[:,1]).corr(label_df.Ret_20d, method='pearson'))\n",
    "\n",
    "print(pd.Series(y_pred[:,1]).corr(label_df.Ret_20d, method='spearman'))\n",
    "\n",
    "ret = pd.Series(np.concatenate([label_df.Ret_20d.values[np.argmax(y_pred, axis = 1)], \n",
    "                -label_df.Ret_20d.values[1-np.argmax(y_pred, axis = 1)]]))\n",
    "\n",
    "print(ret.mean()/ret.std() * np.sqrt(252/20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/clidg/proj_2/pt/gelu_ln_epoch_10_train_0.681694_eval_0.685956_.pt\n",
      "0.692950108825877\n",
      "0.526350540429851\n",
      "0.0032196061346426988\n",
      "0.004933092240856449\n",
      "0.14208357799707144\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loss, y_pred, y_target = eval_loop(test_dataloader, net, loss_fn)\n",
    "\n",
    "print(net_path)\n",
    "print(test_loss)\n",
    "\n",
    "acc = (np.argmax(y_pred, axis = 1) == y_target).sum()/len(y_pred)\n",
    "print(acc)\n",
    "\n",
    "\n",
    "print(pd.Series(y_pred[:,1]).corr(label_df.Ret_20d, method='pearson'))\n",
    "\n",
    "print(pd.Series(y_pred[:,1]).corr(label_df.Ret_20d, method='spearman'))\n",
    "\n",
    "ret = pd.Series(np.concatenate([label_df.Ret_20d.values[np.argmax(y_pred, axis = 1)], \n",
    "                -label_df.Ret_20d.values[1-np.argmax(y_pred, axis = 1)]]))\n",
    "\n",
    "print(ret.mean()/ret.std() * np.sqrt(252/20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/clidg/proj_2/pt/ln_epoch_13_train_0.680397_eval_0.685894_.pt\n",
      "0.6923807588988327\n",
      "0.5285528588472017\n",
      "0.002919619184014595\n",
      "0.004514477115001684\n",
      "0.5040300091688483\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loss, y_pred, y_target = eval_loop(test_dataloader, net, loss_fn)\n",
    "\n",
    "print(net_path)\n",
    "print(test_loss)\n",
    "\n",
    "acc = (np.argmax(y_pred, axis = 1) == y_target).sum()/len(y_pred)\n",
    "print(acc)\n",
    "\n",
    "print(pd.Series(y_pred[:,1]).corr(label_df.Ret_20d, method='pearson'))\n",
    "\n",
    "print(pd.Series(y_pred[:,1]).corr(label_df.Ret_20d, method='spearman'))\n",
    "\n",
    "ret = pd.Series(np.concatenate([label_df.Ret_20d.values[np.argmax(y_pred, axis = 1)], \n",
    "                -label_df.Ret_20d.values[1-np.argmax(y_pred, axis = 1)]]))\n",
    "\n",
    "print(ret.mean()/ret.std() * np.sqrt(252/20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
