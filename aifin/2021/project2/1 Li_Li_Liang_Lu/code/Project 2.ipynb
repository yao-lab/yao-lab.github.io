{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainX\n",
    "\n",
    "images = {}\n",
    "\n",
    "for year in np.arange(1993, 2000):\n",
    "    images[str(year)] = np.memmap(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_images.dat\"), dtype=np.uint8, mode='r').reshape(\n",
    "                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "\n",
    "\n",
    "trainX = np.r_[images['1993'], images['1994'], images['1995'], images['1996'], images['1997'], images['1998'], \n",
    "               images['1999']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21221/3373700141.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  trainY = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "# trainY\n",
    "\n",
    "trainY = pd.Series()\n",
    "\n",
    "for year in np.arange(1993, 2000):\n",
    "    label_df = pd.read_feather(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\"))\\\n",
    "               ['Retx_20d_label']\n",
    "    \n",
    "    trainY = pd.concat([trainY, label_df])\n",
    "    \n",
    "trainY.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testX\n",
    "\n",
    "images = {}\n",
    "\n",
    "for year in np.arange(2000, 2020):\n",
    "    images[str(year)] = np.memmap(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_images.dat\"), dtype=np.uint8, mode='r').reshape(\n",
    "                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "\n",
    "testX = np.r_[images['2000'], images['2001'], images['2002'], images['2003'], images['2004'], images['2005'], \n",
    "               images['2006'], images['2007'], images['2008'], images['2009'], images['2010'], images['2011'], \n",
    "               images['2012'], images['2013'], images['2014'], images['2015'], images['2016'], images['2017'],\n",
    "               images['2018'], images['2019']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21221/2517823635.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  testY = pd.Series()\n"
     ]
    }
   ],
   "source": [
    "# testY\n",
    "\n",
    "testY = pd.Series()\n",
    "\n",
    "for year in np.arange(2000, 2020):\n",
    "    label_df = pd.read_feather(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\"))\\\n",
    "               ['Retx_20d_label']\n",
    "    \n",
    "    testY = pd.concat([testY, label_df])\n",
    "\n",
    "testY.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainX[128:].reshape((-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "Y = trainY\n",
    "\n",
    "twoIds = Y[Y == 2].index\n",
    "Ids = [x for x in range(0, X.shape[0]) if x not in twoIds]\n",
    "\n",
    "X = torch.Tensor(X[Ids])\n",
    "Y = torch.Tensor(Y[Ids].values.reshape(1, -1))\n",
    "\n",
    "ids = [x for x in range(0, X.shape[0])]\n",
    "random.shuffle(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[ids[int(len(ids)*0.3):]]\n",
    "train_y = Y[0][ids[int(len(ids)*0.3):]]\n",
    "\n",
    "validate_X = X[ids[:int(len(ids)*0.3)]]\n",
    "validate_y = Y[0][ids[:int(len(ids)*0.3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_X = testX.reshape((-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "test_y = testY\n",
    "\n",
    "twoIds = test_y[test_y == 2].index\n",
    "Ids = [x for x in range(0, test_X.shape[0]) if x not in twoIds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(dataset=TensorDataset(X_train, train_y), \\\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = Data.DataLoader(dataset=TensorDataset(validate_X, validate_y), \\\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = Data.DataLoader(dataset=TensorDataset(torch.Tensor(test_X[Ids]), torch.Tensor(test_y.loc[Ids].values.reshape(1, -1)[0])), \\\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, Dropout, BN, Xavier, Activation, Pool, Filter, Dilation, Stride):\n",
    "        super(CNN, self).__init__()\n",
    "        self.Dropout = Dropout\n",
    "        self.BN = BN\n",
    "        self.Xavier = Xavier\n",
    "        self.Activation = Activation\n",
    "        self.Pool = Pool\n",
    "        self.Filter = Filter\n",
    "        self.Dilation = Dilation\n",
    "        self.Stride = Stride\n",
    "        \n",
    "        if self.BN:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=1,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self.Filter,\n",
    "                    stride=self.Stride,\n",
    "                    dilation=self.Dilation,\n",
    "                    padding=(67,1)\n",
    "                    ),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool),\n",
    "            )\n",
    "\n",
    "            self.conv2 = nn.Sequential( \n",
    "                nn.Conv2d(\n",
    "                    in_channels=64,    \n",
    "                    out_channels=128,  \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "\n",
    "            self.conv3 = nn.Sequential(  \n",
    "                nn.Conv2d(\n",
    "                    in_channels=128,  \n",
    "                    out_channels=256, \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "        \n",
    "        if self.BN == False:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=1,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self.Filter,\n",
    "                    stride=self.Stride,\n",
    "                    dilation=self.Dilation,\n",
    "                    padding=(67,1)\n",
    "                    ),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool),\n",
    "            )\n",
    "\n",
    "            self.conv2 = nn.Sequential( \n",
    "                nn.Conv2d(\n",
    "                    in_channels=64,    \n",
    "                    out_channels=128,  \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "\n",
    "            self.conv3 = nn.Sequential(  \n",
    "                nn.Conv2d(\n",
    "                    in_channels=128,  \n",
    "                    out_channels=256, \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "        \n",
    "        if self.Activation == 'LReLU':\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=1,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self.Filter,\n",
    "                    stride=self.Stride,\n",
    "                    dilation=self.Dilation,\n",
    "                    padding=(67,1)\n",
    "                    ),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool),\n",
    "            )\n",
    "\n",
    "            self.conv2 = nn.Sequential( \n",
    "                nn.Conv2d(\n",
    "                    in_channels=64,    \n",
    "                    out_channels=128,  \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "\n",
    "            self.conv3 = nn.Sequential(  \n",
    "                nn.Conv2d(\n",
    "                    in_channels=128,  \n",
    "                    out_channels=256, \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "        \n",
    "        if self.Activation == 'ReLU':\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=1,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self.Filter,\n",
    "                    stride=self.Stride,\n",
    "                    dilation=self.Dilation,\n",
    "                    padding=(67,1)\n",
    "                    ),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool),\n",
    "            )\n",
    "\n",
    "            self.conv2 = nn.Sequential( \n",
    "                nn.Conv2d(\n",
    "                    in_channels=64,    \n",
    "                    out_channels=128,  \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "\n",
    "            self.conv3 = nn.Sequential(  \n",
    "                nn.Conv2d(\n",
    "                    in_channels=128,  \n",
    "                    out_channels=256, \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "        \n",
    "        if self.Xavier:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "        if self.Xavier == False:\n",
    "            pass\n",
    "        \n",
    "        self.FC1 = nn.Linear(in_features=256*8*60, out_features=2)\n",
    "        self.dropout = nn.Dropout(self.Dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.FC1(x)\n",
    "        x = self.dropout(x)\n",
    "        output = nn.functional.softmax(x,dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oldold:10000000.000\n",
      "old:10000000.000\n",
      "Validation times:  1 | validation loss: 0.701 | validation accuracy: 0.508\n",
      "--------------------------------------------------\n",
      "oldold:10000000.000\n",
      "old:0.701\n",
      "Validation times:  2 | validation loss: 0.698 | validation accuracy: 0.500\n",
      "--------------------------------------------------\n",
      "oldold:0.701\n",
      "old:0.698\n",
      "Validation times:  3 | validation loss: 0.705 | validation accuracy: 0.511\n",
      "--------------------------------------------------\n",
      "oldold:0.698\n",
      "old:0.705\n",
      "Validation times:  4 | validation loss: 0.699 | validation accuracy: 0.500\n",
      "--------------------------------------------------\n",
      "oldold:0.705\n",
      "old:0.699\n",
      "Validation times:  5 | validation loss: 0.707 | validation accuracy: 0.512\n",
      "--------------------------------------------------\n",
      "oldold:0.699\n",
      "old:0.707\n",
      "Validation times:  6 | validation loss: 0.703 | validation accuracy: 0.512\n",
      "--------------------------------------------------\n",
      "oldold:0.707\n",
      "old:0.703\n",
      "Validation times:  7 | validation loss: 0.700 | validation accuracy: 0.497\n",
      "--------------------------------------------------\n",
      "oldold:0.703\n",
      "old:0.700\n",
      "Validation times:  8 | validation loss: 0.697 | validation accuracy: 0.507\n",
      "--------------------------------------------------\n",
      "oldold:0.700\n",
      "old:0.697\n",
      "Validation times:  9 | validation loss: 0.698 | validation accuracy: 0.508\n",
      "--------------------------------------------------\n",
      "oldold:0.697\n",
      "old:0.698\n",
      "Validation times:  10 | validation loss: 0.697 | validation accuracy: 0.503\n",
      "--------------------------------------------------\n",
      "oldold:0.698\n",
      "old:0.697\n",
      "Validation times:  11 | validation loss: 0.702 | validation accuracy: 0.494\n",
      "--------------------------------------------------\n",
      "TERMINAL\n",
      "Accuracy :0.491\n",
      "oldold:0.697\n",
      "old:0.702\n",
      "new:0.703\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(Dropout=0.5, BN=True, Xavier=True, Activation='LReLU', Pool=(2, 1), Filter=(5, 3), Dilation=(2,1), Stride=(3,1))\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.00001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "validate_loss_oldold = 10000000\n",
    "validate_loss_old = 10000000\n",
    "\n",
    "# Training and validating\n",
    "for step,(b_x,b_y) in enumerate(train_loader):\n",
    "    cnn.train()\n",
    "    b_x = b_x.to(device)\n",
    "    b_y = b_y.to(device)\n",
    "    b_x = torch.unsqueeze(b_x,dim=1).float()\n",
    "    output = cnn(b_x)\n",
    "    loss = loss_func(output, b_y.long())\n",
    "    optimizer.zero_grad()           # clear gradients for this training step\n",
    "    loss.backward()                 # backpropagation, compute gradients\n",
    "    optimizer.step()                # apply gradients\n",
    "    \n",
    "    if ((step % 220 == 0) & (step != 0)):\n",
    "        loss_val = 0\n",
    "        total_val = 0\n",
    "        correct_val = 0\n",
    "        cnn.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_step,(v_x,v_y) in enumerate(valid_loader):\n",
    "                v_x = v_x.to(device)\n",
    "                v_y = v_y.to(device)\n",
    "                v_x = torch.unsqueeze(v_x,dim=1).float()\n",
    "                validate_output = cnn(v_x)\n",
    "                validate_loss_new = loss_func(validate_output,v_y.long())\n",
    "                loss_val += validate_loss_new.item()\n",
    "                \n",
    "                pred_y = torch.max(validate_output,1)[1].data\n",
    "                total_val += v_y.size(0)\n",
    "                correct_val += (pred_y == v_y).squeeze().sum()\n",
    "            loss_val /= len(valid_loader)\n",
    "                \n",
    "        if (validate_loss_oldold <= validate_loss_old) and (validate_loss_old <= loss_val):\n",
    "            \n",
    "            print('TERMINAL')\n",
    "            print('Accuracy :{:.3f}'.format(correct_val / total_val))\n",
    "            print('oldold:{:.3f}'.format(validate_loss_oldold))\n",
    "            print('old:{:.3f}'.format(validate_loss_old))\n",
    "            print('new:{:.3f}'.format(loss_val))\n",
    "            break\n",
    "            \n",
    "        print('oldold:{:.3f}'.format(validate_loss_oldold))\n",
    "        print('old:{:.3f}'.format(validate_loss_old))\n",
    "\n",
    "        validate_loss_oldold = validate_loss_old\n",
    "        validate_loss_old = loss_val\n",
    "\n",
    "        print('Validation times: ', step//220, '| validation loss: %.3f' % loss_val, '| validation accuracy: %.3f' % (correct_val / total_val))\n",
    "        print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.506\n",
      "Loss:0.700\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "total_val = 0\n",
    "correct_val = 0\n",
    "\n",
    "for step,(t_x,t_y) in enumerate(test_loader):\n",
    "    \n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        t_x = t_x.to(device)\n",
    "        t_y = t_y.to(device)\n",
    "        t_x = torch.unsqueeze(t_x,dim=1).float()\n",
    "        test_output = cnn(t_x)\n",
    "        pred_y = torch.max(test_output,1)[1].data\n",
    "        total_val += t_y.size(0)\n",
    "        correct_val += (pred_y == t_y).squeeze().sum()\n",
    "        \n",
    "        validate_loss_new = loss_func(test_output,t_y.long())\n",
    "        loss_val += validate_loss_new.item()\n",
    "        \n",
    "loss_val /= len(test_loader)\n",
    "\n",
    "print('Accuracy:{:.3f}'.format(correct_val/ total_val))\n",
    "print('Loss:{:.3f}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robusted Test\n",
    "\n",
    "def robustest(Dropout, BN, Xavier, Activation, Pool, Filter, Dilation, Stride):\n",
    "    \n",
    "    cnn = CNN(Dropout=Dropout, BN=BN, Xavier=Xavier, Activation=Activation, Pool=Pool, Filter=Filter, Dilation=Dilation, Stride=Stride)\n",
    "    cnn = cnn.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=0.00001)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    validate_loss_oldold = 10000000\n",
    "    validate_loss_old = 10000000\n",
    "\n",
    "    for step,(b_x,b_y) in enumerate(train_loader):\n",
    "        cnn.train()\n",
    "        b_x = b_x.to(device)\n",
    "        b_y = b_y.to(device)\n",
    "        b_x = torch.unsqueeze(b_x,dim=1).float()\n",
    "        output = cnn(b_x)\n",
    "        loss = loss_func(output, b_y.long())\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "\n",
    "        if ((step % 220 == 0) & (step != 0)):\n",
    "            loss_val = 0\n",
    "            total_val = 0\n",
    "            correct_val = 0\n",
    "            cnn.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_step,(v_x,v_y) in enumerate(valid_loader):\n",
    "                    v_x = v_x.to(device)\n",
    "                    v_y = v_y.to(device)\n",
    "                    v_x = torch.unsqueeze(v_x,dim=1).float()\n",
    "                    validate_output = cnn(v_x)\n",
    "                    validate_loss_new = loss_func(validate_output,v_y.long())\n",
    "                    loss_val += validate_loss_new.item()\n",
    "\n",
    "                    pred_y = torch.max(validate_output,1)[1].data\n",
    "                    total_val += v_y.size(0)\n",
    "                    correct_val += (pred_y == v_y).squeeze().sum()\n",
    "                loss_val /= len(valid_loader)\n",
    "\n",
    "            if (validate_loss_oldold <= validate_loss_old) and (validate_loss_old <= loss_val):\n",
    "\n",
    "                print('TERMINAL')\n",
    "                print('Accuracy :{:.3f}'.format(correct_val / total_val))\n",
    "                print('oldold:{:.3f}'.format(validate_loss_oldold))\n",
    "                print('old:{:.3f}'.format(validate_loss_old))\n",
    "                print('new:{:.3f}'.format(loss_val))\n",
    "                print('----------------------------------------')\n",
    "                break\n",
    "\n",
    "            validate_loss_oldold = validate_loss_old\n",
    "            validate_loss_old = loss_val\n",
    "            \n",
    "    total_val = 0\n",
    "    correct_val = 0\n",
    "\n",
    "    for step,(t_x,t_y) in enumerate(test_loader):\n",
    "\n",
    "        cnn.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            t_x = t_x.to(device)\n",
    "            t_y = t_y.to(device)\n",
    "            t_x = torch.unsqueeze(t_x,dim=1).float()\n",
    "            test_output = cnn(t_x)\n",
    "            pred_y = torch.max(test_output,1)[1].data\n",
    "            total_val += t_y.size(0)\n",
    "            correct_val += (pred_y == t_y).squeeze().sum()\n",
    "\n",
    "            validate_loss_new = loss_func(test_output,t_y.long())\n",
    "            loss_val += validate_loss_new.item()\n",
    "\n",
    "    loss_val /= len(test_loader)\n",
    "\n",
    "    print('Accuracy:{:.3f}'.format(correct_val / total_val))\n",
    "    print('Loss:{:.3f}'.format(loss_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout= [0, 0.25, 0.75]\n",
    "Filter = [(3, 3), (7, 3)]\n",
    "DilationStride = [[(2,1), (1,1)], [(1,1), (3,1)], [(1,1), (1,1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "TERMINAL\n",
      "Accuracy :0.515\n",
      "oldold:0.692\n",
      "old:0.696\n",
      "new:0.698\n",
      "----------------------------------------\n",
      "Accuracy:0.516\n",
      "Loss:0.700\n",
      "0.25\n",
      "TERMINAL\n",
      "Accuracy :0.511\n",
      "oldold:0.691\n",
      "old:0.695\n",
      "new:0.700\n",
      "----------------------------------------\n",
      "Accuracy:0.515\n",
      "Loss:0.701\n",
      "0.75\n",
      "TERMINAL\n",
      "Accuracy :0.512\n",
      "oldold:0.722\n",
      "old:0.783\n",
      "new:0.785\n",
      "----------------------------------------\n",
      "Accuracy:0.494\n",
      "Loss:0.803\n"
     ]
    }
   ],
   "source": [
    "for d in Dropout:\n",
    "    print(d)\n",
    "    robustest(d, BN=True, Xavier=True, Activation='LReLU', Pool=(2,1), Filter=(5,3), Dilation=(2,1), Stride=(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERMINAL\n",
      "Accuracy :0.493\n",
      "oldold:0.701\n",
      "old:0.705\n",
      "new:0.715\n",
      "----------------------------------------\n",
      "Accuracy:0.509\n",
      "Loss:0.712\n"
     ]
    }
   ],
   "source": [
    "robustest(Dropout=0.5, BN=False, Xavier=True, Activation='LReLU', Pool=(2,1), Filter=(5,3), Dilation=(2,1), Stride=(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERMINAL\n",
      "Accuracy :0.516\n",
      "oldold:0.697\n",
      "old:0.705\n",
      "new:0.705\n",
      "----------------------------------------\n",
      "Accuracy:0.499\n",
      "Loss:0.711\n"
     ]
    }
   ],
   "source": [
    "robustest(Dropout=0.5, BN=True, Xavier=False, Activation='LReLU', Pool=(2,1), Filter=(5,3), Dilation=(2,1), Stride=(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TERMINAL\n",
      "Accuracy :0.517\n",
      "oldold:0.696\n",
      "old:0.700\n",
      "new:0.706\n",
      "----------------------------------------\n",
      "Accuracy:0.496\n",
      "Loss:0.717\n"
     ]
    }
   ],
   "source": [
    "robustest(Dropout=0.5, BN=True, Xavier=True, Activation='ReLU', Pool=(2,1), Filter=(5,3), Dilation=(2,1), Stride=(3,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:liangxin]",
   "language": "python",
   "name": "conda-env-liangxin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
