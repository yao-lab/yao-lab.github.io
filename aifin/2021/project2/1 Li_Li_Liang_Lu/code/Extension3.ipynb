{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20935/974457549.py:16: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  trainY = pd.Series()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20935/974457549.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m testX = np.r_[images['2000'], images['2001'], images['2002'], images['2003'], images['2004'], images['2005'], \n\u001b[0m\u001b[1;32m     35\u001b[0m                \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2006'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2007'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2008'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2009'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2010'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2011'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2012'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2013'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2014'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2015'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2016'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2017'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/liangxin/lib/python3.9/site-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 5_day_label\n",
    "\n",
    "# trainX\n",
    "images = {}\n",
    "\n",
    "for year in np.arange(1993, 2000):\n",
    "    images[str(year)] = np.memmap(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_images.dat\"), dtype=np.uint8, mode='r').reshape(\n",
    "                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "\n",
    "\n",
    "trainX = np.r_[images['1993'], images['1994'], images['1995'], images['1996'], images['1997'], images['1998'], \n",
    "               images['1999']]\n",
    "\n",
    "\n",
    "# trainY\n",
    "trainY = pd.Series()\n",
    "\n",
    "for year in np.arange(1993, 2000):\n",
    "    label_df = pd.read_feather(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\"))\\\n",
    "               ['Retx_60d_label']\n",
    "    \n",
    "    trainY = pd.concat([trainY, label_df])\n",
    "    \n",
    "trainY.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# testX\n",
    "images = {}\n",
    "\n",
    "for year in np.arange(2000, 2020):\n",
    "    images[str(year)] = np.memmap(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_images.dat\"), dtype=np.uint8, mode='r').reshape(\n",
    "                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "\n",
    "testX = np.r_[images['2000'], images['2001'], images['2002'], images['2003'], images['2004'], images['2005'], \n",
    "               images['2006'], images['2007'], images['2008'], images['2009'], images['2010'], images['2011'], \n",
    "               images['2012'], images['2013'], images['2014'], images['2015'], images['2016'], images['2017'],\n",
    "               images['2018'], images['2019']]\n",
    "\n",
    "# testY\n",
    "testY = pd.Series()\n",
    "\n",
    "for year in np.arange(2000, 2020):\n",
    "    label_df = pd.read_feather(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\"))\\\n",
    "               ['Retx_60d_label']\n",
    "    \n",
    "    testY = pd.concat([testY, label_df])\n",
    "\n",
    "testY.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = {5: 15, 20: 60, 60: 180}\n",
    "IMAGE_HEIGHT = {5: 32, 20: 64, 60: 96}   \n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/clicw/data/Aaron/trainX.dat.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12009/3966133906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/trainX.dat.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_HEIGHT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_WIDTH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/trainY.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtwoIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/liangxin/lib/python3.9/site-packages/numpy/core/memmap.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mf_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mf_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mf_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/clicw/data/Aaron/trainX.dat.npy'"
     ]
    }
   ],
   "source": [
    "X = np.memmap(os.getcwd() + '/trainX.dat.npy', dtype=np.uint8, mode='r')[128:].\\\n",
    "    reshape((-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "Y = pd.read_csv(os.getcwd() + '/trainY.csv', index_col=0)\n",
    "\n",
    "twoIds = Y[Y['0'] == 2].index\n",
    "Ids = [x for x in range(0, X.shape[0]) if x not in twoIds]\n",
    "\n",
    "X = torch.Tensor(X[Ids])\n",
    "Y = torch.Tensor(Y.loc[Ids, :].values.reshape(1, -1))\n",
    "\n",
    "ids = [x for x in range(0, X.shape[0])]\n",
    "random.shuffle(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[ids[int(len(ids)*0.3):]]\n",
    "train_y = Y[0][ids[int(len(ids)*0.3):]]\n",
    "\n",
    "validate_X = X[ids[:int(len(ids)*0.3)]]\n",
    "validate_y = Y[0][ids[:int(len(ids)*0.3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_X = np.memmap(os.getcwd() + '/testX.dat.npy', dtype=np.uint8, mode='r')[128:].\\\n",
    "         reshape((-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "test_y = pd.read_csv(os.getcwd() + '/testY.csv', index_col=0)\n",
    "\n",
    "twoIds = test_y[test_y['0'] == 2].index\n",
    "Ids = [x for x in range(0, test_X.shape[0]) if x not in twoIds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(dataset=TensorDataset(X_train, train_y), \\\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = Data.DataLoader(dataset=TensorDataset(validate_X, validate_y), \\\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = Data.DataLoader(dataset=TensorDataset(torch.Tensor(test_X[Ids]), torch.Tensor(test_y.loc[Ids].values.reshape(1, -1)[0])), \\\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, Dropout, BN, Xavier, Activation, Pool, Filter, Dilation, Stride):\n",
    "        super(CNN, self).__init__()\n",
    "        self.Dropout = Dropout\n",
    "        self.BN = BN\n",
    "        self.Xavier = Xavier\n",
    "        self.Activation = Activation\n",
    "        self.Pool = Pool\n",
    "        self.Filter = Filter\n",
    "        self.Dilation = Dilation\n",
    "        self.Stride = Stride\n",
    "        \n",
    "        if self.BN:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=1,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self.Filter,\n",
    "                    stride=self.Stride,\n",
    "                    dilation=self.Dilation,\n",
    "                    padding=(67,1)\n",
    "                    ),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool),\n",
    "            )\n",
    "\n",
    "            self.conv2 = nn.Sequential( \n",
    "                nn.Conv2d(\n",
    "                    in_channels=64,    \n",
    "                    out_channels=128,  \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "\n",
    "            self.conv3 = nn.Sequential(  \n",
    "                nn.Conv2d(\n",
    "                    in_channels=128,  \n",
    "                    out_channels=256, \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "        \n",
    "        if self.BN == False:\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=1,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self.Filter,\n",
    "                    stride=self.Stride,\n",
    "                    dilation=self.Dilation,\n",
    "                    padding=(67,1)\n",
    "                    ),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool),\n",
    "            )\n",
    "\n",
    "            self.conv2 = nn.Sequential( \n",
    "                nn.Conv2d(\n",
    "                    in_channels=64,    \n",
    "                    out_channels=128,  \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "\n",
    "            self.conv3 = nn.Sequential(  \n",
    "                nn.Conv2d(\n",
    "                    in_channels=128,  \n",
    "                    out_channels=256, \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "        \n",
    "        if self.Activation == 'LReLU':\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=1,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self.Filter,\n",
    "                    stride=self.Stride,\n",
    "                    dilation=self.Dilation,\n",
    "                    padding=(67,1)\n",
    "                    ),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool),\n",
    "            )\n",
    "\n",
    "            self.conv2 = nn.Sequential( \n",
    "                nn.Conv2d(\n",
    "                    in_channels=64,    \n",
    "                    out_channels=128,  \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "\n",
    "            self.conv3 = nn.Sequential(  \n",
    "                nn.Conv2d(\n",
    "                    in_channels=128,  \n",
    "                    out_channels=256, \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "        \n",
    "        if self.Activation == 'ReLU':\n",
    "            self.conv1 = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=1,\n",
    "                    out_channels=64,\n",
    "                    kernel_size=self.Filter,\n",
    "                    stride=self.Stride,\n",
    "                    dilation=self.Dilation,\n",
    "                    padding=(67,1)\n",
    "                    ),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool),\n",
    "            )\n",
    "\n",
    "            self.conv2 = nn.Sequential( \n",
    "                nn.Conv2d(\n",
    "                    in_channels=64,    \n",
    "                    out_channels=128,  \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "\n",
    "            self.conv3 = nn.Sequential(  \n",
    "                nn.Conv2d(\n",
    "                    in_channels=128,  \n",
    "                    out_channels=256, \n",
    "                    kernel_size=self.Filter,\n",
    "                    padding=(2,1)\n",
    "                ),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=self.Pool), \n",
    "            )\n",
    "        \n",
    "        if self.Xavier:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "        if self.Xavier == False:\n",
    "            pass\n",
    "        \n",
    "        self.FC1 = nn.Linear(in_features=256*8*60, out_features=2)\n",
    "        self.dropout = nn.Dropout(self.Dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.FC1(x)\n",
    "        x = self.dropout(x)\n",
    "        output = nn.functional.softmax(x,dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oldold:10000000.000\n",
      "old:10000000.000\n",
      "Validation times:  1 | validation loss: 0.706 | validation accuracy: 0.507\n",
      "--------------------------------------------------\n",
      "oldold:10000000.000\n",
      "old:0.706\n",
      "Validation times:  2 | validation loss: 0.698 | validation accuracy: 0.502\n",
      "--------------------------------------------------\n",
      "oldold:0.706\n",
      "old:0.698\n",
      "Validation times:  3 | validation loss: 0.698 | validation accuracy: 0.500\n",
      "--------------------------------------------------\n",
      "oldold:0.698\n",
      "old:0.698\n",
      "Validation times:  4 | validation loss: 0.698 | validation accuracy: 0.504\n",
      "--------------------------------------------------\n",
      "oldold:0.698\n",
      "old:0.698\n",
      "Validation times:  5 | validation loss: 0.698 | validation accuracy: 0.508\n",
      "--------------------------------------------------\n",
      "TERMINAL\n",
      "Accuracy :0.507\n",
      "oldold:0.698\n",
      "old:0.698\n",
      "new:0.708\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(Dropout=0.5, BN=True, Xavier=True, Activation='LReLU', Pool=(2, 1), Filter=(5, 3), Dilation=(2,1), Stride=(3,1))\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.00001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "validate_loss_oldold = 10000000\n",
    "validate_loss_old = 10000000\n",
    "\n",
    "# Training and validating\n",
    "for step,(b_x,b_y) in enumerate(train_loader):\n",
    "    cnn.train()\n",
    "    b_x = b_x.to(device)\n",
    "    b_y = b_y.to(device)\n",
    "    b_x = torch.unsqueeze(b_x,dim=1).float()\n",
    "    output = cnn(b_x)\n",
    "    loss = loss_func(output, b_y.long())\n",
    "    optimizer.zero_grad()           # clear gradients for this training step\n",
    "    loss.backward()                 # backpropagation, compute gradients\n",
    "    optimizer.step()                # apply gradients\n",
    "    \n",
    "    if ((step % 220 == 0) & (step != 0)):\n",
    "        loss_val = 0\n",
    "        total_val = 0\n",
    "        correct_val = 0\n",
    "        cnn.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_step,(v_x,v_y) in enumerate(valid_loader):\n",
    "                v_x = v_x.to(device)\n",
    "                v_y = v_y.to(device)\n",
    "                v_x = torch.unsqueeze(v_x,dim=1).float()\n",
    "                validate_output = cnn(v_x)\n",
    "                validate_loss_new = loss_func(validate_output,v_y.long())\n",
    "                loss_val += validate_loss_new.item()\n",
    "                \n",
    "                pred_y = torch.max(validate_output,1)[1].data\n",
    "                total_val += v_y.size(0)\n",
    "                correct_val += (pred_y == v_y).squeeze().sum()\n",
    "            loss_val /= len(valid_loader)\n",
    "                \n",
    "        if (validate_loss_oldold <= validate_loss_old) and (validate_loss_old <= loss_val):\n",
    "            \n",
    "            print('TERMINAL')\n",
    "            print('Accuracy :{:.3f}'.format(correct_val / total_val))\n",
    "            print('oldold:{:.3f}'.format(validate_loss_oldold))\n",
    "            print('old:{:.3f}'.format(validate_loss_old))\n",
    "            print('new:{:.3f}'.format(loss_val))\n",
    "            break\n",
    "            \n",
    "        print('oldold:{:.3f}'.format(validate_loss_oldold))\n",
    "        print('old:{:.3f}'.format(validate_loss_old))\n",
    "\n",
    "        validate_loss_oldold = validate_loss_old\n",
    "        validate_loss_old = loss_val\n",
    "\n",
    "        print('Validation times: ', step//220, '| validation loss: %.3f' % loss_val, '| validation accuracy: %.3f' % (correct_val / total_val))\n",
    "        print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.510\n",
      "Loss:0.702\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "total_val = 0\n",
    "correct_val = 0\n",
    "\n",
    "for step,(t_x,t_y) in enumerate(test_loader):\n",
    "    \n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        t_x = t_x.to(device)\n",
    "        t_y = t_y.to(device)\n",
    "        t_x = torch.unsqueeze(t_x,dim=1).float()\n",
    "        test_output = cnn(t_x)\n",
    "        pred_y = torch.max(test_output,1)[1].data\n",
    "        total_val += t_y.size(0)\n",
    "        correct_val += (pred_y == t_y).squeeze().sum()\n",
    "        \n",
    "        validate_loss_new = loss_func(test_output,t_y.long())\n",
    "        loss_val += validate_loss_new.item()\n",
    "        \n",
    "loss_val /= len(test_loader)\n",
    "\n",
    "print('Accuracy:{:.3f}'.format(correct_val / total_val))\n",
    "print('Loss:{:.3f}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6851/3871748409.py:16: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  trainY = pd.Series()\n",
      "/tmp/ipykernel_6851/3871748409.py:40: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  testY = pd.Series()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oldold:10000000.000\n",
      "old:10000000.000\n",
      "Validation times:  1 | validation loss: 0.698 | validation accuracy: 0.503\n",
      "--------------------------------------------------\n",
      "oldold:10000000.000\n",
      "old:0.698\n",
      "Validation times:  2 | validation loss: 0.698 | validation accuracy: 0.501\n",
      "--------------------------------------------------\n",
      "oldold:0.698\n",
      "old:0.698\n",
      "Validation times:  3 | validation loss: 0.698 | validation accuracy: 0.501\n",
      "--------------------------------------------------\n",
      "oldold:0.698\n",
      "old:0.698\n",
      "Validation times:  4 | validation loss: 0.697 | validation accuracy: 0.501\n",
      "--------------------------------------------------\n",
      "oldold:0.698\n",
      "old:0.697\n",
      "Validation times:  5 | validation loss: 0.704 | validation accuracy: 0.506\n",
      "--------------------------------------------------\n",
      "TERMINAL\n",
      "Accuracy :0.495\n",
      "oldold:0.697\n",
      "old:0.704\n",
      "new:0.711\n",
      "Accuracy:0.486\n",
      "Loss:0.717\n"
     ]
    }
   ],
   "source": [
    "## 60_day_label\n",
    "\n",
    "# trainX\n",
    "images = {}\n",
    "\n",
    "for year in np.arange(1993, 2000):\n",
    "    images[str(year)] = np.memmap(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_images.dat\"), dtype=np.uint8, mode='r').reshape(\n",
    "                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "\n",
    "\n",
    "trainX = np.r_[images['1993'], images['1994'], images['1995'], images['1996'], images['1997'], images['1998'], \n",
    "               images['1999']]\n",
    "\n",
    "\n",
    "# trainY\n",
    "trainY = pd.Series()\n",
    "\n",
    "for year in np.arange(1993, 2000):\n",
    "    label_df = pd.read_feather(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\"))\\\n",
    "               ['Retx_60d_label']\n",
    "    \n",
    "    trainY = pd.concat([trainY, label_df])\n",
    "    \n",
    "trainY.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# testX\n",
    "images = {}\n",
    "\n",
    "for year in np.arange(2000, 2020):\n",
    "    images[str(year)] = np.memmap(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_images.dat\"), dtype=np.uint8, mode='r').reshape(\n",
    "                        (-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "\n",
    "testX = np.r_[images['2000'], images['2001'], images['2002'], images['2003'], images['2004'], images['2005'], \n",
    "               images['2006'], images['2007'], images['2008'], images['2009'], images['2010'], images['2011'], \n",
    "               images['2012'], images['2013'], images['2014'], images['2015'], images['2016'], images['2017'],\n",
    "               images['2018'], images['2019']]\n",
    "\n",
    "# testY\n",
    "testY = pd.Series()\n",
    "\n",
    "for year in np.arange(2000, 2020):\n",
    "    label_df = pd.read_feather(op.join(os.getcwd()[:17] + 'MAFS6010Z_Project2/data/', f\"monthly_20d/20d_month_has_vb_[20]_ma_{year}_labels_w_delay.feather\"))\\\n",
    "               ['Retx_60d_label']\n",
    "    \n",
    "    testY = pd.concat([testY, label_df])\n",
    "\n",
    "testY.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = trainX[128:].reshape((-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "Y = trainY\n",
    "\n",
    "twoIds = Y[Y == 2].index\n",
    "Ids = [x for x in range(0, X.shape[0]) if x not in twoIds]\n",
    "\n",
    "X = torch.Tensor(X[Ids])\n",
    "Y = torch.Tensor(Y[Ids].values.reshape(1, -1))\n",
    "\n",
    "ids = [x for x in range(0, X.shape[0])]\n",
    "random.shuffle(ids)\n",
    "\n",
    "X_train = X[ids[int(len(ids)*0.3):]]\n",
    "train_y = Y[0][ids[int(len(ids)*0.3):]]\n",
    "\n",
    "validate_X = X[ids[:int(len(ids)*0.3)]]\n",
    "validate_y = Y[0][ids[:int(len(ids)*0.3)]]\n",
    "\n",
    "# test_X = np.memmap(os.getcwd() + '/testX.dat.npy', dtype=np.uint8, mode='r')[128:].\\\n",
    "#          reshape((-1, IMAGE_HEIGHT[20], IMAGE_WIDTH[20]))\n",
    "# test_y = pd.read_csv(os.getcwd() + '/testY.csv', index_col=0)\n",
    "\n",
    "test_X = testX\n",
    "test_y = testY\n",
    "\n",
    "twoIds = test_y[test_y == 2].index\n",
    "Ids = [x for x in range(0, test_X.shape[0]) if x not in twoIds]\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=TensorDataset(X_train, train_y), \\\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = Data.DataLoader(dataset=TensorDataset(validate_X, validate_y), \\\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = Data.DataLoader(dataset=TensorDataset(torch.Tensor(test_X[Ids]), torch.Tensor(test_y.loc[Ids].values.reshape(1, -1)[0])), \\\n",
    "                               batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "cnn = CNN(Dropout=0.5, BN=True, Xavier=True, Activation='LReLU', Pool=(2, 1), Filter=(5, 3), Dilation=(2,1), Stride=(3,1))\n",
    "cnn = cnn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.00001)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "validate_loss_oldold = 10000000\n",
    "validate_loss_old = 10000000\n",
    "\n",
    "# Training and validating\n",
    "for step,(b_x,b_y) in enumerate(train_loader):\n",
    "    cnn.train()\n",
    "    b_x = b_x.to(device)\n",
    "    b_y = b_y.to(device)\n",
    "    b_x = torch.unsqueeze(b_x,dim=1).float()\n",
    "    output = cnn(b_x)\n",
    "    loss = loss_func(output, b_y.long())\n",
    "    optimizer.zero_grad()           # clear gradients for this training step\n",
    "    loss.backward()                 # backpropagation, compute gradients\n",
    "    optimizer.step()                # apply gradients\n",
    "    \n",
    "    if ((step % 220 == 0) & (step != 0)):\n",
    "        loss_val = 0\n",
    "        total_val = 0\n",
    "        correct_val = 0\n",
    "        cnn.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_step,(v_x,v_y) in enumerate(valid_loader):\n",
    "                v_x = v_x.to(device)\n",
    "                v_y = v_y.to(device)\n",
    "                v_x = torch.unsqueeze(v_x,dim=1).float()\n",
    "                validate_output = cnn(v_x)\n",
    "                validate_loss_new = loss_func(validate_output,v_y.long())\n",
    "                loss_val += validate_loss_new.item()\n",
    "                \n",
    "                pred_y = torch.max(validate_output,1)[1].data\n",
    "                total_val += v_y.size(0)\n",
    "                correct_val += (pred_y == v_y).squeeze().sum()\n",
    "            loss_val /= len(valid_loader)\n",
    "                \n",
    "        if (validate_loss_oldold <= validate_loss_old) and (validate_loss_old <= loss_val):\n",
    "            \n",
    "            print('TERMINAL')\n",
    "            print('Accuracy :{:.3f}'.format(correct_val / total_val))\n",
    "            print('oldold:{:.3f}'.format(validate_loss_oldold))\n",
    "            print('old:{:.3f}'.format(validate_loss_old))\n",
    "            print('new:{:.3f}'.format(loss_val))\n",
    "            break\n",
    "            \n",
    "        print('oldold:{:.3f}'.format(validate_loss_oldold))\n",
    "        print('old:{:.3f}'.format(validate_loss_old))\n",
    "\n",
    "        validate_loss_oldold = validate_loss_old\n",
    "        validate_loss_old = loss_val\n",
    "\n",
    "        print('Validation times: ', step//220, '| validation loss: %.3f' % loss_val, '| validation accuracy: %.3f' % (correct_val / total_val))\n",
    "        print('--------------------------------------------------')\n",
    "        \n",
    "# Testing\n",
    "total_val = 0\n",
    "correct_val = 0\n",
    "\n",
    "for step,(t_x,t_y) in enumerate(test_loader):\n",
    "    \n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        t_x = t_x.to(device)\n",
    "        t_y = t_y.to(device)\n",
    "        t_x = torch.unsqueeze(t_x,dim=1).float()\n",
    "        test_output = cnn(t_x)\n",
    "        pred_y = torch.max(test_output,1)[1].data\n",
    "        total_val += t_y.size(0)\n",
    "        correct_val += (pred_y == t_y).squeeze().sum()\n",
    "        \n",
    "        validate_loss_new = loss_func(test_output,t_y.long())\n",
    "        loss_val += validate_loss_new.item()\n",
    "        \n",
    "loss_val /= len(test_loader)\n",
    "\n",
    "print('Accuracy:{:.3f}'.format(correct_val / total_val))\n",
    "print('Loss:{:.3f}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:liangxin]",
   "language": "python",
   "name": "conda-env-liangxin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
