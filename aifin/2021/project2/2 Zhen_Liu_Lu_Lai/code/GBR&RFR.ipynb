{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a9cb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIU Jinghui\n",
    "# 20745644\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "# ---\n",
    "# data prepare\n",
    "# ---\n",
    "# train: 1961 -- 1978\n",
    "# validation: 1979 -- 1990\n",
    "# test: 1991 -- 2020\n",
    "\n",
    "# train_data = all_data[all_data.DATE <= 19781231][19601231 < all_data.DATE]\n",
    "# validation_data = all_data[19781231 < all_data.DATE][all_data.DATE <= 19901231]\n",
    "# test_data = all_data[all_data.DATE > 19911231]\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels import regression\n",
    "import pickle\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "\n",
    "from six import StringIO\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dfdcc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# initialization\n",
    "# ---\n",
    "data_path = 'C:\\\\Users\\\\Claudia\\\\Desktop\\\\philostudy\\\\hkust\\\\21-22 term\\\\6010Z\\\\project2-1112\\\\DATA\\\\DATA\\\\'\n",
    "model_path = 'C:\\\\Users\\\\Claudia\\\\Desktop\\\\philostudy\\\\hkust\\\\21-22 term\\\\6010Z\\\\project2-1112\\\\MODEL\\\\'\n",
    "\n",
    "# train_data = pd.read_csv(data_path+'train_data.csv',index_col=0)\n",
    "# validation_data= pd.read_csv(data_path+'validation_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# GBR\n",
    "# ---\n",
    "loss_list = ['huber'] #, 'squared_error']\n",
    "learning_rate = [0.01, 0.03, 0.05, 0.1]\n",
    "max_depth = [3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f017f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "\n",
    "v_R = dict()\n",
    "\n",
    "for i in range(1,31):\n",
    "    print(i)\n",
    "    test = pd.read_csv(data_path+'test_'+str(i)+'.csv',index_col=0)\n",
    "    \n",
    "    # ---\n",
    "    # preprocess\n",
    "    # ---\n",
    "    check_nan_train = ((train_data.isnull().sum())/train_data.shape[0]).sort_values(ascending=False)\n",
    "    check_nan_validation = ((validation_data.isnull().sum())/validation_data.shape[0]).sort_values(ascending=False)\n",
    "    \n",
    "    train_ = preprocess(train_data,check_nan_train)\n",
    "    x_train = train_.drop(['permno','DATE','RET'],axis=1)\n",
    "    y_train = train_.RET\n",
    "    del train_\n",
    "    \n",
    "    validation_ = preprocess(validation_data,check_nan_validation)\n",
    "    x_validate = validation_.drop(['permno','DATE','RET'],axis=1)\n",
    "    y_validate = validation_.RET\n",
    "    del validation_\n",
    "    \n",
    "    print('finish preprocess!')\n",
    "    \n",
    "    for l in loss_list:\n",
    "        for lr in learning_rate:\n",
    "            for max_dep in max_depth:\n",
    "                time0 = datetime.datetime.now()\n",
    "                print('start training:', time0)\n",
    "                \n",
    "                # ---\n",
    "                # model training\n",
    "                # ---                \n",
    "                gbr = GBR(max_depth=max_dep,learning_rate=lr,loss=l)\n",
    "                gbr.fit(x_train,y_train)\n",
    "                \n",
    "                train_time = datetime.datetime.now()\n",
    "                print('training time：',int((train_time - time0).seconds/60),'min')\n",
    "                \n",
    "                # model saving\n",
    "                modelName = str(i) + '_' + l + '_' + str(lr) + '_'+ str(max_dep)\n",
    "                modelFile = model_path + modelName + '.sav'\n",
    "                pickle.dump(gbr, open(modelFile, 'wb'))\n",
    "    \n",
    "                # ---\n",
    "                # validation\n",
    "                # ---\n",
    "                validate_R_oos = R_oos(gbr,x_validate,y_validate)\n",
    "                v_R[modelName] = validate_R_oos\n",
    "                del gbr\n",
    "                \n",
    "                validation_time = datetime.datetime.now()\n",
    "                print('validating time：',int((validation_time - train_time).seconds/60),'min')\n",
    "    \n",
    "    best_model = min(v_R, key=v_R.get)\n",
    "    \n",
    "    # ---\n",
    "    # load model\n",
    "    # ---\n",
    "    #loaded_model = pickle.load(open(model_path + best_model  '.sav', 'rb'))\n",
    "    \n",
    "    # ---\n",
    "    # test\n",
    "    # ---\n",
    "    x_test = test.drop(['permno','DATE','RET'],axis=1)\n",
    "    y_test = test.RET\n",
    "    del test\n",
    "    \n",
    "    test_R_oos = R_oos(x_test, y_test)\n",
    "    print(test_R_oos)\n",
    "    \n",
    "    # update data\n",
    "    train_end = train_data.DATE.iloc[-1]+10\n",
    "    mid =  validation_data[validation_data.DATE <= (train_end+10000)]\n",
    "    train_data = train_data.append(mid)\n",
    "    validation_data = validation_data[validation_data.DATE > (train_end+10000)]\n",
    "    validation_data = validation_data.append(test)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    del test\n",
    "    \n",
    "endtime = datetime.datetime.now()\n",
    "print ('运行时长：',int((endtime - starttime).seconds/60),'分钟') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# RFR\n",
    "# ---\n",
    "n_estimators = [300,400,500,600]\n",
    "max_depth = [3,4,5,6]\n",
    "min_samples_split = [20,50,100,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc99e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "v_R = dict()\n",
    "\n",
    "for i in range(1,31):\n",
    "    print(i)\n",
    "    test = pd.read_csv(data_path+'test_'+str(i)+'.csv',index_col=0)\n",
    "    \n",
    "    # ---\n",
    "    # preprocess\n",
    "    # ---\n",
    "    check_nan_train = ((train_data.isnull().sum())/train_data.shape[0]).sort_values(ascending=False)\n",
    "    check_nan_validation = ((validation_data.isnull().sum())/validation_data.shape[0]).sort_values(ascending=False)\n",
    "    \n",
    "    train_ = preprocess(train_data,check_nan_train)\n",
    "    x_train = train_.drop(['permno','DATE','RET'],axis=1)\n",
    "    y_train = train_.RET\n",
    "    del train_\n",
    "    \n",
    "    validation_ = preprocess(validation_data,check_nan_validation)\n",
    "    x_validate = validation_.drop(['permno','DATE','RET'],axis=1)\n",
    "    y_validate = validation_.RET\n",
    "    del validation_\n",
    "    \n",
    "    print('finish preprocess!')\n",
    "    \n",
    "    for n in n_estimators:\n",
    "        for s in min_samples_split:\n",
    "            for max_dep in max_depth:\n",
    "                \n",
    "                time0 = datetime.datetime.now()\n",
    "                # ---\n",
    "                # model training\n",
    "                # ---                \n",
    "                rfr = RFR(n_estimators=n,min_samples_split=s,max_depth=max_dep)\n",
    "                rfr.fit(x_train,y_train)\n",
    "                \n",
    "                train_time = datetime.datetime.now()\n",
    "                print('training time：',int((train_time - time0).seconds/60),'min')\n",
    "                \n",
    "                # model saving\n",
    "                modelName = str(n) + '_' + str(s) + '_'+ str(max_dep)\n",
    "                modelFile = model_path + modelName + '.sav'\n",
    "                pickle.dump(rfr, open(modelFile, 'wb'))\n",
    "    \n",
    "                # ---\n",
    "                # validation\n",
    "                # ---\n",
    "                validate_R_oos = R_oos(rfr,x_validate,y_validate)\n",
    "                k = 'test_'+str(i)+modelName\n",
    "                v_R[k] = validate_R_oos\n",
    "                del rfr\n",
    "                \n",
    "                validation_time = datetime.datetime.now()\n",
    "                print('validating time：',int((validation_time - train_time).seconds/60),'min')\n",
    "    \n",
    "    best_model = min(v_R, key=v_R.get)\n",
    "    \n",
    "    # ---\n",
    "    # load model\n",
    "    # ---\n",
    "    # loaded_model = pickle.load(open(model_path + best_model  '.sav', 'rb'))\n",
    "    \n",
    "    # ---\n",
    "    # test\n",
    "    # ---\n",
    "    x_test = test.drop(['permno','DATE','RET'],axis=1)\n",
    "    y_test = test.RET\n",
    "    del test\n",
    "    \n",
    "    test_R_oos = R_oos(x_test, y_test)\n",
    "    print(test_R_oos)\n",
    "    \n",
    "    # update data\n",
    "    train_end = train_data.DATE.iloc[-1]+10\n",
    "    mid =  validation_data[validation_data.DATE <= (train_end+10000)]\n",
    "    train_data = train_data.append(mid)\n",
    "    validation_data = validation_data[validation_data.DATE > (train_end+10000)]\n",
    "    validation_data = validation_data.append(test)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    del test\n",
    "    \n",
    "endtime = datetime.datetime.now()\n",
    "print ('running time：',int((endtime - starttime).seconds/60),'min') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0cd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_oos(model,x,y):\n",
    "    y_hat = model.predict(x)\n",
    "    fenmu = sum(y**2)\n",
    "    fenzi = sum((y-y_hat)**2)\n",
    "    return 1-(fenzi/fenmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32e61e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def preprocess(df, check_nan):\n",
    "    #均值：\n",
    "    df_mean = SimpleImputer(missing_values=np.nan, strategy='mean',copy=False)\n",
    "    #中位数：\n",
    "    df_median = SimpleImputer(missing_values=np.nan, strategy='median',copy=False)\n",
    "    #常数0：\n",
    "    df_0 = SimpleImputer(strategy=\"constant\",fill_value=0,copy=False)\n",
    "    #众数：\n",
    "    df_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent',copy=False)\n",
    "    \n",
    "    # missing rate < 50% ---- fill median\n",
    "    # missing rate > 50% ---- fill 0\n",
    "    available_factors = check_nan[check_nan < 0.5].index\n",
    "    unavailable_factors = check_nan[check_nan >= 0.5].index\n",
    "    \n",
    "    ava_df = df[available_factors]\n",
    "    a = df_most_frequent.fit_transform(ava_df)\n",
    "    a = pd.DataFrame(a)\n",
    "    a.columns = ava_df.columns\n",
    "    \n",
    "    un_df = df[unavailable_factors]\n",
    "    u = df_0.fit_transform(un_df)\n",
    "    u = pd.DataFrame(u)\n",
    "    u.columns = un_df.columns\n",
    "    \n",
    "    df = pd.concat([a,u],axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47a7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
