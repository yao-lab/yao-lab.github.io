{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "realistic-constitutional",
   "metadata": {},
   "source": [
    "# Ke et al.(2020) ‘Predicting Returns with Text Data’ paper replication\n",
    "## Zhongchen WANG (20745072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caroline-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-mentor",
   "metadata": {},
   "source": [
    "# Data Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-vegetable",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf = pd.read_parquet('./data/dsf.parquet.gzip')\n",
    "# Column specret: for the market reaction\n",
    "dsf = dsf[['SecuCode', 'date', 'ret', 'specret']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "anatxt = pd.read_parquet('./data/anatxt.parquet.gzip')\n",
    "anatxt = anatxt.drop(columns=['FYEAR'])\n",
    "anatxt = anatxt.drop_duplicates()\n",
    "data = anatxt[['ID','SecuCode','content','create_date']]\n",
    "data.drop_duplicates(inplace = True)\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "iret = pd.read_csv('./data/iret.csv')\n",
    "iret = iret[['date', 'csi300t']]\n",
    "iret.date = pd.to_datetime(iret.date, format='%Y-%m-%d', errors='ignore')\n",
    "#iret.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-abraham",
   "metadata": {},
   "source": [
    "## Pre-process 1 -- get two return labels: [t:t+1], [t+2:t+6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-spoke",
   "metadata": {},
   "source": [
    "### merge dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_rt = pd.merge(dsf, iret, how = 'left', on = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtracting the csi300t from the return of the same period\n",
    "DF_rt['adj_return'] = DF_rt.ret - DF_rt.csi300t\n",
    "DF_rt = DF_rt.drop(['ret', 'csi300t'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = SecuCode , date, specret\n",
    "DF_rt_spe = DF_rt.drop(['adj_return'],1)\n",
    "DF_rt_spe.dropna(inplace = True)\n",
    "DF_rt_spe.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-apollo",
   "metadata": {},
   "source": [
    "### dealing with the trading date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = DF_rt_spe[['date']].sort_values('date').astype(str)\n",
    "dt_list = dt_list.drop_duplicates()\n",
    "dt_list = dt_list.date.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeindex(lt,delta = 1):\n",
    "    lt_new = lt[delta:]\n",
    "    for i in range(delta):\n",
    "        lt_new.append('NaN')\n",
    "    df = pd.DataFrame(lt)\n",
    "    df.columns = ['date']\n",
    "    df.date = pd.to_datetime(df.date, format='%Y-%m-%d', errors='ignore')\n",
    "    df['t+'+str(delta)] = pd.to_datetime(lt_new, format='%Y-%m-%d', errors='ignore')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following 6 trading day\n",
    "dt_df_t1 = timeindex(dt_list,1)\n",
    "dt_df_t2 = timeindex(dt_list,2)\n",
    "dt_df_t3 = timeindex(dt_list,3)\n",
    "dt_df_t4 = timeindex(dt_list,4)\n",
    "dt_df_t5 = timeindex(dt_list,5)\n",
    "dt_df_t6 = timeindex(dt_list,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final = pd.merge(DF_rt_spe,dt_df_t1,how = 'left',on = 'date')\n",
    "DF_final = pd.merge(DF_final,dt_df_t2,how = 'left',on = 'date')\n",
    "DF_final = pd.merge(DF_final,dt_df_t3,how = 'left',on = 'date')\n",
    "DF_final = pd.merge(DF_final,dt_df_t4,how = 'left',on = 'date')\n",
    "DF_final = pd.merge(DF_final,dt_df_t5,how = 'left',on = 'date')\n",
    "DF_final = pd.merge(DF_final,dt_df_t6,how = 'left',on = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final = DF_final.rename(columns = {'date':'create_date'})\n",
    "DF_final = pd.merge(data, DF_final, how = 'left', on = ['SecuCode','create_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final = DF_final.dropna().sort_values('create_date')\n",
    "DF_final.reset_index(drop = True, inplace = True )\n",
    "DF_final = DF_final.rename(columns = {'specret':'specret_t'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfmerge(DF_final,DF_rt_spe, delta):\n",
    "    df_new = pd.merge(DF_final,DF_rt_spe,how = 'left',left_on = ['SecuCode','t+'+str(delta)], right_on = ['SecuCode','date'])\n",
    "    df_new.drop(['date'],1,inplace = True)\n",
    "    df_new = df_new.rename(columns = {'specret':'specret_t+'+str(delta)})\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_all_spe = dfmerge(DF_final,DF_rt_spe, 1)\n",
    "DF_all_spe = dfmerge(DF_all_spe,DF_rt_spe, 2)\n",
    "DF_all_spe = dfmerge(DF_all_spe,DF_rt_spe, 3)\n",
    "DF_all_spe = dfmerge(DF_all_spe,DF_rt_spe, 4)\n",
    "DF_all_spe = dfmerge(DF_all_spe,DF_rt_spe, 5)\n",
    "DF_all_spe = dfmerge(DF_all_spe,DF_rt_spe, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_all_spe['t:t+1'] = (DF_all_spe['specret_t'] + DF_all_spe['specret_t+1']).tolist()\n",
    "DF_all_spe['t+2:t+6'] = (DF_all_spe['specret_t+2'] + DF_all_spe['specret_t+3']\n",
    "                        + DF_all_spe['specret_t+4'] + + DF_all_spe['specret_t+5']\n",
    "                        + DF_all_spe['specret_t+6']\n",
    "                        ).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-heating",
   "metadata": {},
   "source": [
    "### get dataframe with two labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_data = DF_all_spe[['ID', 'SecuCode', 'content', 'create_date', 't:t+1', 't+2:t+6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF_data.to_csv('./data_preprocess/DF_data_spe.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-groove",
   "metadata": {},
   "source": [
    "## Pre-process 2 -- jieba cut for the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./data_preprocess/DF_data_spe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-virginia",
   "metadata": {},
   "source": [
    "### Divide content into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_sent = []\n",
    "for i in tqdm(range(len(Data))):\n",
    "    st = re.sub('[\\d]', '', Data.content[i]) # remove the number\n",
    "    st = re.sub('[\\s]', '', st)\n",
    "    st = re.sub('[a-zA-Z]', '', st)\n",
    "    st = st.replace('\\n','')\n",
    "    st = st.replace(' ','')\n",
    "    st = st.replace('％','')\n",
    "    st = st.replace('%','')\n",
    "    st = st.replace('.','')\n",
    "    st = st.replace('（）','')\n",
    "    st = st.replace('()','')\n",
    "    st = re.split('。|！|\\！|？|\\?',st)\n",
    "    del st[-1]\n",
    "    content_sent.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['content_sent'] = content_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-carry",
   "metadata": {},
   "source": [
    "### Divide train_data and test_data\n",
    "- train_data: 2010-01-01 -- 2014-12-31\n",
    "- test_data: after 2015-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Data[(Data['create_date']<='2014-12-31')&(Data['create_date']>='2010-01-01')].reset_index(drop = True)\n",
    "test_data = Data[Data['create_date']>='2015-01-01'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-office",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words and punctuations\n",
    "stopwords = [line.strip() for line in open('./data/cn_stopwords_only.txt', 'r', encoding = 'utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_removal(test_data):\n",
    "    no_stop = []\n",
    "    for i in tqdm(range(len(test_data['content_sent']))):\n",
    "        m = test_data['content_sent'][i]\n",
    "        for j in range(len(stopwords)):\n",
    "            a = stopwords[j]\n",
    "            if a in m:\n",
    "                m = m.replace(a,'')\n",
    "        no_stop.append(m)\n",
    "    return no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop = stop_removal(test_data)\n",
    "no_stop_tr = stop_removal(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['no_stop_cont'] = no_stop\n",
    "train_data['no_stop_cont'] = no_stop_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-pilot",
   "metadata": {},
   "source": [
    "### Content_split into word with POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "def wd_split(test_data):\n",
    "    lt_test = []\n",
    "    dou = ','\n",
    "    for j in tqdm(range(len(test_data['no_stop_cont']))):\n",
    "        cont = test_data['no_stop_cont'][j]\n",
    "        a = pseg.cut(cont,use_paddle=True) #paddle mode\n",
    "        b = [dou.join([q,d]) for (q,d) in a]\n",
    "        lt_test.append(b)\n",
    "    return lt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = wd_split(test_data)\n",
    "content_tr = wd_split(train_data)\n",
    "test_data['content_split'] = content\n",
    "train_data['content_split'] = content_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.to_parquet('./data_preprocess/train_data.parquet.gzip')\n",
    "#test_data.to_parquet('./data_preprocess/test_data.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-greenhouse",
   "metadata": {},
   "source": [
    "## Pre-process 3 -- content refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = pd.read_parquet('./data_preprocess/train_data.parquet.gzip')\n",
    "#test_data = pd.read_parquet('./data_preprocess/test_data.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-madness",
   "metadata": {},
   "source": [
    "### sgn lables for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sgn_labels,t:t+1'] = train_data['t:t+1'].map(lambda x: 1 if x>0 \n",
    "                                                         else (-1 if x<0 else (0 if x == 0 else 'NaN')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-theology",
   "metadata": {},
   "source": [
    "### data cleansing -- delete punctuation (eg: ,) (POS tag == 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(a):\n",
    "    lt = []\n",
    "    for i in range(len(a)):\n",
    "        if 'x' not in a[i]:\n",
    "            if len(a[i]) != 1:\n",
    "                lt.append(a[i])\n",
    "    return lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_split(train_data):\n",
    "    new_split = []\n",
    "    for i in tqdm(range(len(train_data))):\n",
    "        a = train_data.content_split[i].tolist()\n",
    "        m = select(a)\n",
    "        new_split.append(m)\n",
    "    return new_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tr = word_split(train_data)\n",
    "word_test = word_split(test_data)\n",
    "train_data['words'] = word_tr\n",
    "test_data['words'] = word_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-pharmacy",
   "metadata": {},
   "source": [
    "### A new column: get words only, without POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatwd(data):\n",
    "    sen = ' '.join(data)\n",
    "    sen = sen.replace(',','')\n",
    "    sen = re.sub('[a-zA-Z]','',sen)  # remove POS\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_noPOS(train_data):\n",
    "    sen_all = []\n",
    "    for i in tqdm(range(len(train_data))):\n",
    "        a = train_data.words[i].tolist()\n",
    "        sen = concatwd(a)\n",
    "        sen_all.append(sen)\n",
    "    return sen_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "noPOS_tr = words_noPOS(train_data)\n",
    "noPOS_test = words_noPOS(test_data)\n",
    "train_data['str_words'] = noPOS_tr\n",
    "train_data['str_words'] = noPOS_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.to_parquet('./data_preprocess/train_data.parquet.gzip')\n",
    "#test_data.to_parquet('./data_preprocess/test_data.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-royalty",
   "metadata": {},
   "source": [
    "## Pre-process 4 -- bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-three",
   "metadata": {},
   "source": [
    "### train_data_out2: for train data with return label outside of [-2%,2%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_out2 = train_data[(train_data['t:t+1']>=0.02)|(train_data['t:t+1']<=-0.02)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-directive",
   "metadata": {},
   "source": [
    "### construct bag of words from all training sample\n",
    "+ following procedures are the same for `train_data_out2` -- omit here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_words_func(train_data):\n",
    "    bag = []\n",
    "    for i in tqdm(range(len(train_data))):\n",
    "        one = train_data.words[i].tolist()\n",
    "        bag.extend(one)\n",
    "    counts = pd.value_counts(bag)\n",
    "    counts = pd.DataFrame(counts)\n",
    "    counts.columns = ['counts']\n",
    "    bag_words = list(set(bag))\n",
    "    bag_words = pd.DataFrame(bag_words)\n",
    "    bag_words.columns = ['original']\n",
    "    bag_words['word'] = bag_words['original'].map(lambda x:x.split(',')[0])\n",
    "    bag_words['POS'] = bag_words['original'].map(lambda x:x.split(',')[1])\n",
    "    bag_df = bag_words.join(counts, on = 'original').sort_values('counts',ascending = False)\n",
    "    ct = bag_df.groupby('word').agg('sum').sort_values('counts',ascending = False)\n",
    "    ct.columns = ['#words']\n",
    "    bag_df_final = bag_df.drop_duplicates('word').reset_index(drop = True)\n",
    "    bag_df_final = bag_df_final.join(ct,on = 'word')\n",
    "    return bag_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_df_final = bag_words_func(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag_df_final.to_csv('./data/word count/bag_df_final.csv',index=False,header=True, encoding = 'utf_8_sig')\n",
    "bag_df_final = pd.read_csv('./data/word count/bag_df_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-explanation",
   "metadata": {},
   "source": [
    "### divide bag of words by different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cut(data, length = 2):\n",
    "    if length == 4:\n",
    "        df = data[data.word.str.len()>=length]\n",
    "    else:\n",
    "        df = data[data.word.str.len()==length]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcut(bag_df_final,whole = ''):\n",
    "    cf_1 = word_cut(bag_df_final,1)\n",
    "    cf_2 = word_cut(bag_df_final,2)\n",
    "    cf_3 = word_cut(bag_df_final,3)\n",
    "    cf_4 = word_cut(bag_df_final,4)\n",
    "    cf_1.sort_values(\"#words\",inplace=True,ascending = False)\n",
    "    cf_2.sort_values(\"#words\",inplace=True,ascending = False)\n",
    "    cf_3.sort_values(\"#words\",inplace=True,ascending = False)\n",
    "    cf_4.sort_values(\"#words\",inplace=True,ascending = False)\n",
    "    cf_1.to_csv('./data/word count/DF_word_len1'+whole+'.csv',index=False,header=True, encoding = 'utf_8_sig') \n",
    "    cf_2.to_csv('./data/word count/DF_word_len2'+whole+'.csv',index=False,header=True, encoding = 'utf_8_sig') \n",
    "    cf_3.to_csv('./data/word count/DF_word_len3'+whole+'.csv',index=False,header=True, encoding = 'utf_8_sig') \n",
    "    cf_4.to_csv('./data/word count/DF_word_len4'+whole+'.csv',index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcut(bag_df_final)\n",
    "# wordcut(bag_df_final_out2,whole = '_out2') for train_data_out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-immigration",
   "metadata": {},
   "source": [
    "### detailed POS exclusion rule of different word length\n",
    "**two methods are adopted**\n",
    "- method one: only select adj. and v. words\n",
    "- method two: delete different POS of words for different word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdcount_df = pd.read_csv('./data/word count/bag_df_final.csv')\n",
    "cf_1 = pd.read_csv('./data/word count/DF_word_len1.csv')\n",
    "cf_2 = pd.read_csv('./data/word count/DF_word_len2.csv')\n",
    "cf_3 = pd.read_csv('./data/word count/DF_word_len3.csv')\n",
    "cf_4 = pd.read_csv('./data/word count/DF_word_len4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only a,v\n",
    "wdcount_df_av = wdcount_df[wdcount_df['POS'].isin(['a','v'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop type of POS that do not contain sentiment information -- detailed exclusion\n",
    "cf_1_ex = cf_1[~cf_1['POS'].isin(['w','r','ns','f','p','c','q','n','nr','m','u'])].reset_index(drop = True)\n",
    "cf_2_ex = cf_2[~cf_2['POS'].isin(['nw','u','m','t','TIME','nr','PER','nz','n','q','ORG','f','LOC','r','s'])].reset_index(drop = True)\n",
    "cf_3_ex = cf_3[~cf_3['POS'].isin(['s','r','LOC','f','ORG','nt','n','nz','PER','nr','TIME','t','m','nw'])].reset_index(drop = True)\n",
    "cf_4_ex = cf_4[~cf_4['POS'].isin(['s','LOC','f','ORG','n','nz','PER','nr','TIME','t','m','nw'])].reset_index(drop = True)\n",
    "wdcount_df_ex = pd.concat([cf_1_ex,cf_2_ex,cf_3_ex,cf_4_ex],axis = 0).reset_index(drop = True)\n",
    "wdcount_df_ex.reset_index(drop = True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdcount_df_av.to_csv('./data/word count/wdcount_df_av.csv',index=False,header=True, encoding = 'utf_8_sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdcount_df_ex.to_csv('./data/word count/wdcount_df_ex.csv',index=False,header=True, encoding = 'utf_8_sig') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-edition",
   "metadata": {},
   "source": [
    "# Benchmark construction -- two github links\n",
    "- benchmark 1: https://github.com/dictionaries2020/SentimentDictionaries\n",
    "- benchmark 2: https://github.com/MengLingchao/Chinese_financial_sentiment_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-repair",
   "metadata": {},
   "source": [
    "## Read dataset -- benchmark 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual report only\n",
    "git1 = pd.read_excel('./data/金融领域中文情绪词典.xlsx', sheet_name='年报负面',header = None)\n",
    "git2 = pd.read_excel('./data/金融领域中文情绪词典.xlsx', sheet_name='年报正面',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet('./data_preprocess/test_data.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "git1.columns = ['word']\n",
    "git1['sentiment'] = [-1]*len(git1)\n",
    "git2.columns = ['word']\n",
    "git2['sentiment'] = [1]*len(git2)\n",
    "git_report = pd.concat([git1,git2],axis = 0)\n",
    "git_report.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-wheel",
   "metadata": {},
   "source": [
    "## 12 scores definition:\n",
    "- Score 1 = # positive words/# of words excluding stop words and symbols etc.\n",
    "- Score 2 = (-1) * # negative words/# of words excluding stop words and symbols etc.\n",
    "- Score 3 = Score 1+score 2\n",
    "- Score 4-6 = change the denominator to (#positive+#negative words) in score 1-3.\n",
    "- Score 7-9 = use # of sentences instead of # words in Score 1-3\n",
    "- Score 10-12 = use # of sentences instead of # words in Score 4-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-viewer",
   "metadata": {},
   "source": [
    "## count of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_count(train_data, dic):\n",
    "    lt_p = []\n",
    "    lt_n = []\n",
    "    \n",
    "    word_list = dic.word.tolist()\n",
    "    s_list = dic.sentiment.tolist()\n",
    "    content_list = train_data.content_sent.tolist()\n",
    "    \n",
    "    for j in tqdm(range(len(content_list)), position=0, leave=True): \n",
    "        p = 0\n",
    "        n = 0\n",
    "        for i in range(len(word_list)):\n",
    "            ct = content_list[j].count(word_list[i])\n",
    "            if s_list[i]>0:\n",
    "                p+=ct\n",
    "            else:\n",
    "                n+=ct\n",
    "        lt_p.append(p)\n",
    "        lt_n.append(n)\n",
    "    return lt_p, lt_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_test_p, lt_test_n = sentiment_count(test_data, git_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_count(data):\n",
    "    lt_all = []\n",
    "    str_list = data.str_words.tolist()\n",
    "    for j in tqdm(range(len(str_list)), position=0, leave=True):\n",
    "        lt_all.append(len(str_list[j].split(' ')))\n",
    "    return lt_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_all = words_count(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-balance",
   "metadata": {},
   "source": [
    "## score1 - socre6 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_df(lt_test_p, lt_test_n, lt_all):\n",
    "    lt_test_p = pd.DataFrame(lt_test_p)\n",
    "    lt_test_n = pd.DataFrame(lt_test_n)\n",
    "    lt_all = pd.DataFrame(lt_all)\n",
    "    lt_test_df = pd.concat([lt_test_p,lt_test_n,lt_all],axis = 1)\n",
    "    lt_test_df.columns = ['#p','#n','#all']\n",
    "    lt_test_df['score1'] = lt_test_df['#p']/lt_test_df['#all']\n",
    "    lt_test_df['score2'] = (-1) * lt_test_df['#n']/lt_test_df['#all']\n",
    "    lt_test_df.fillna(0)\n",
    "    lt_test_df['score3'] = lt_test_df['score1'] + lt_test_df['score2']\n",
    "    lt_test_df['score4'] = lt_test_df['#p']/(lt_test_df['#p']+lt_test_df['#n'])\n",
    "    lt_test_df['score5'] = (-1) * lt_test_df['#n']/(lt_test_df['#p']+lt_test_df['#n'])\n",
    "    lt_test_df.fillna(0)\n",
    "    lt_test_df['score6'] = lt_test_df['score4'] + lt_test_df['score5']\n",
    "    return lt_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_test_df = score_df(lt_test_p, lt_test_n, lt_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-minneapolis",
   "metadata": {},
   "source": [
    "## sentence level count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_level(a, dic):\n",
    "    flag_lt = []\n",
    "    word_list = dic.word.tolist()\n",
    "    s_list = dic.sentiment.tolist()\n",
    "    ct = len(a)\n",
    "    for j in range(ct): \n",
    "        p = 0\n",
    "        n = 0\n",
    "        for i in range(len(word_list)):\n",
    "            ct_wd = a[j].count(word_list[i])\n",
    "            if s_list[i]>0:\n",
    "                p+=ct_wd\n",
    "            else:\n",
    "                n+=ct_wd\n",
    "        if p>n:\n",
    "            flag = 1\n",
    "        elif p<n:\n",
    "            flag = -1\n",
    "        else:\n",
    "            flag = 0\n",
    "        \n",
    "        flag_lt.append(flag)\n",
    "    return flag_lt,ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t = []\n",
    "sen_all = []\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    a = test_data.no_stop_cont[i].split(',')\n",
    "    f,sen = sent_level(a,git_report)\n",
    "    f_t.append(f)\n",
    "    sen_all.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_all = []\n",
    "n_all = []\n",
    "for i in tqdm(range(len(f_t))):\n",
    "    p = f_t[i].count(1)\n",
    "    n = f_t[i].count(-1)\n",
    "    p_all.append(p)\n",
    "    n_all.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-check",
   "metadata": {},
   "source": [
    "## score7 - socre12 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sen_df(lt_test_p, lt_test_n, lt_all):\n",
    "    lt_test_p = pd.DataFrame(lt_test_p)\n",
    "    lt_test_n = pd.DataFrame(lt_test_n)\n",
    "    lt_all = pd.DataFrame(lt_all)\n",
    "    lt_test_df = pd.concat([lt_test_p,lt_test_n],axis = 1)\n",
    "    lt_test_df = pd.concat([lt_test_df,lt_all],axis = 1)\n",
    "    lt_test_df.columns = ['#sen_p','#sen_n','#sen_all']\n",
    "    lt_test_df['score7'] = lt_test_df['#sen_p']/lt_test_df['#sen_all']\n",
    "    lt_test_df['score8'] = (-1) * lt_test_df['#sen_n']/lt_test_df['#sen_all']\n",
    "    lt_test_df.fillna(0)\n",
    "    lt_test_df['score9'] = lt_test_df['score7'] + lt_test_df['score8']\n",
    "    lt_test_df['score10'] = lt_test_df['#sen_p']/(lt_test_df['#sen_p']+lt_test_df['#sen_n'])\n",
    "    lt_test_df['score11'] = (-1) * lt_test_df['#sen_n']/(lt_test_df['#sen_p']+lt_test_df['#sen_n'])\n",
    "    lt_test_df.fillna(0)\n",
    "    lt_test_df['score12'] = lt_test_df['score10'] + lt_test_df['score11']\n",
    "    return lt_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_df = score_sen_df(p_all, n_all, sen_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.concat([lt_test_df,sen_df],axis = 1)\n",
    "master_df = pd.concat([test_data,score_df],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_csv('./data_preprocess/test_score1-12_df.csv',index=False,header=True)\n",
    "master_df.to_parquet('./data_preprocess/master_df.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-packing",
   "metadata": {},
   "source": [
    "## correlation calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df[['t:t+1','score1','score2','score3','score4','score5','score6','score7','score8','score9',\n",
    " 'score10','score11','score12']].corr().iloc[0:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df[['t:t+1','score1','score2','score3','score4','score5','score6','score7','score8','score9',\n",
    " 'score10','score11','score12']].corr(method = 'spearman').iloc[0:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df[['t+2:t+6','score1','score2','score3','score4','score5','score6','score7','score8','score9',\n",
    " 'score10','score11','score12']].corr().iloc[0:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_df[['t+2:t+6','score1','score2','score3','score4','score5','score6','score7','score8','score9',\n",
    " 'score10','score11','score12']].corr(method = 'spearman').iloc[0:1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-trainer",
   "metadata": {},
   "source": [
    "## Read dataset -- benchmark 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "git1_meng = pd.read_excel('./data/中文金融情感词典_姜富伟等(2020).xlsx', sheet_name='negative')\n",
    "git2_meng = pd.read_excel('./data/中文金融情感词典_姜富伟等(2020).xlsx', sheet_name='positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-ancient",
   "metadata": {},
   "source": [
    "### following procedure is the same code as the previous for benchmark 1\n",
    "- omit here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-science",
   "metadata": {},
   "source": [
    "## benchmark 1 correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_csv('/Users/evaking/desktop/NLP Project/data_preprocess/test_score1-12_df.csv')\n",
    "# test data\n",
    "test_data = pd.read_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/test_data_simple.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['score6'] = score_df.score6.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is written at 3.10 -- Correlation calculation in the time window\n",
    "corr_6 = corre_df(test_data,'score6')\n",
    "corr_6.to_csv('/Users/evaking/desktop/NLP Project/data/correlation/corr_benchmarkscore6.csv',\n",
    "            index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-elite",
   "metadata": {},
   "source": [
    "# SESTM method\n",
    "**Three different variations:**\n",
    "- `av method`: only choose word marked with a and v in POS tag: adj and verb\n",
    "- `ex method`: detailed POS tag word exclusion method for different word length\n",
    "- `av_2p method`: chooes word length >= 2 based on words in av method\n",
    "\n",
    "`The following codes are the same for different variations, so only show in one variation`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-manitoba",
   "metadata": {},
   "source": [
    "## Dataset simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('/Users/evaking/Desktop/NLP Project/data_preprocess/train_data.parquet.gzip')\n",
    "train_data = train_data.drop(['content','content_sent','no_stop_cont','content_split','words'],axis = 1)\n",
    "train_data.to_parquet('./data_preprocess/train_data_simple.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-commerce",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data\n",
    "train_data = pd.read_parquet('/Users/evaking/Desktop/NLP Project/data_preprocess/train_data_simple.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words from all training sample\n",
    "wdcount_df_av = pd.read_csv('/Users/evaking/Desktop/NLP Project/data/word count/wdcount_df_av.csv')\n",
    "wdcount_df_ex = pd.read_csv('/Users/evaking/Desktop/NLP Project/data/word count/wdcount_df_ex.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-development",
   "metadata": {},
   "source": [
    "## pi_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi_hat_func(train_data, p = True):\n",
    "    pi_hat = train_data[train_data['sgn_labels,t:t+1']>0]['sgn_labels,t:t+1'].sum() / train_data.shape[0]\n",
    "    if p:\n",
    "        print('pi_hat is:', pi_hat)\n",
    "    return pi_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_hat = pi_hat_func(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-persian",
   "metadata": {},
   "source": [
    "## Step 1 -- Screening for Sentiment-Charged words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-determination",
   "metadata": {},
   "source": [
    "### 1.3 Choose one kappa -- threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of words distribution\n",
    "def kappaFunc(data,quant = 0.94):\n",
    "    kappa = data['#words'].quantile(quant)\n",
    "    print('The value of kappa is: ', kappa)\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_av = kappaFunc(wdcount_df_av)\n",
    "# for wdcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise the dataset according to kappa\n",
    "wdcount_df_av = wdcount_df_av[wdcount_df_av['#words'] > kappa_av].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-project",
   "metadata": {},
   "source": [
    "### 1.1 calculates the frequency with which word j co-occurs with a positive return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wd_freq_variant(df,train_data):\n",
    "    word_list = df.word.tolist()\n",
    "    content_list = train_data.str_words.tolist()\n",
    "    labels_list = train_data['sgn_labels,t:t+1'].tolist()\n",
    "    \n",
    "    ct = [0] * len(word_list)\n",
    "    ct_sgn1 = [0] * len(word_list)\n",
    "    \n",
    "    for i in tqdm(range(len(word_list))):\n",
    "        for j in range(len(content_list)): \n",
    "            if (word_list[i] in content_list[j]):\n",
    "                ct[i] += 1\n",
    "                if labels_list[j] > 0:\n",
    "                    ct_sgn1[i] += 1\n",
    "    return word_list, ct, ct_sgn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_av,ct_av,ct_sgn1_av = wd_freq_variant(wdcount_df_av,train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variant method showed in the paper\n",
    "def Freqfunc(word_list_ex,ct_sgn1_ex,ct_ex):\n",
    "    Freq_ex = pd.DataFrame([word_list_ex, ct_sgn1_ex, ct_ex]).T\n",
    "    Freq_ex.columns = ['word', 'k_j & y=1', 'k_j']\n",
    "    Freq_ex = Freq_ex[~Freq_ex['k_j'].isin([0])]\n",
    "    Freq_ex.reset_index(drop = True,inplace = True)\n",
    "    Freq_ex['f_j'] = (Freq_ex['k_j & y=1'] / Freq_ex.k_j)#.replace(np.inf,0)\n",
    "    return Freq_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq_av = Freqfunc(word_list_av,ct_sgn1_av,ct_av)\n",
    "Freq_av.to_csv('/Users/evaking/Desktop/NLP Project/data/Step1/Freq_av.csv',index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-dakota",
   "metadata": {},
   "source": [
    "### 1.3 Choose one kappa -- threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freq_av = pd.read_csv('./data/Step1/Freq_av.csv')\n",
    "Freq_av = Freq_av[Freq_av.k_j>kappa_av].reset_index(drop = True)\n",
    "#Freq_ex = Freq_ex[Freq_ex.k_j>kappa_ex].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-blond",
   "metadata": {},
   "source": [
    "### 1.2 Compare f_j with proper threshold: alpha+ and alpha-\n",
    "`Here are also two variations: 500 and 100 words respectively for both positive and negative words`\n",
    "\n",
    "`The following codes are the same for different variations, so only show in one variation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def S_hat(Freq,pi_hat,wds=100):\n",
    "    S_hat_p = Freq[Freq.f_j >= (pi_hat)].sort_values('k_j',ascending = False)[:wds].reset_index(drop = True)\n",
    "    S_hat_n = Freq[Freq.f_j <= (pi_hat)].sort_values('k_j',ascending = False)[:wds].reset_index(drop = True)\n",
    "    S_hat = pd.concat([S_hat_p, S_hat_n], axis = 0)\n",
    "    S_hat = S_hat.reset_index(drop = True)\n",
    "    S_hat['sentiment'] = ['positive']*wds + ['negative']*wds\n",
    "    return S_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_hat_av_500 = S_hat(Freq_av, pi_hat, wds = 500)\n",
    "S_hat_av_500.to_csv('/Users/evaking/Desktop/NLP Project/data/Step1/S_hat_av_500.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-humanitarian",
   "metadata": {},
   "source": [
    "## Step 2 -- Learning Sentiment Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-masters",
   "metadata": {},
   "source": [
    "### 2.1 estimate H_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentwdbag(sentlist,train_data):\n",
    "    SentCharged = sentlist.word.tolist()\n",
    "    SentCharged_bag = []\n",
    "    content = train_data.str_words.tolist()\n",
    "    for i in tqdm(range(len(SentCharged))):\n",
    "        a = SentCharged[i]\n",
    "        ct_wd = []\n",
    "        for j in range(len(content)):\n",
    "            ct = content[j].count(a)\n",
    "            ct_wd.append(ct)\n",
    "        SentCharged_bag.append(ct_wd)\n",
    "    return SentCharged_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "SentCharged_bag_av_500 = sentwdbag(S_hat_av_500,train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hhat_func(sentwdbag, index, method = 'zero'):\n",
    "    d_S_hat = pd.DataFrame(sentwdbag)\n",
    "    d_S_hat.index = index\n",
    "    \n",
    "    if method == 'zero': # setting hi to 0 if si_hat==0\n",
    "        H_hat = d_S_hat.apply(lambda x: x/x.sum(), axis = 0)\n",
    "        H_hat = H_hat.fillna(0)\n",
    "    elif method == 'drop': # remove articles with s_i_hat = 0\n",
    "        H_hat = d_S_hat.apply(lambda x: x/x.sum(), axis = 0)\n",
    "        H_hat = H_hat.dropna(axis = 1)\n",
    "    else: # replace s_i_hat with count of all words (instead of sentiment charged words) for all articles (including those with s_i_hat > 0)\n",
    "        d_S_hat.loc['all'] = ct_a.values.tolist()[0]\n",
    "        H_hat = d_S_hat.apply(lambda x: x/x['all'], axis = 0)\n",
    "        d_S_hat.drop(['all'],axis = 0,inplace = True)\n",
    "        H_hat.drop(['all'],axis = 0,inplace = True)\n",
    "        H_hat = H_hat.fillna(0)\n",
    "    \n",
    "    return H_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_av_500 = S_hat_av_500.word.tolist()\n",
    "H_hat_av_500 = hhat_func(SentCharged_bag_av_500, index = index_av_500)\n",
    "np.save('/Users/evaking/Desktop/NLP Project/Data/Step2/array_H_hat_av_500.npy',H_hat_av_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-reputation",
   "metadata": {},
   "source": [
    "### 2.2 calculate W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_hat_func(train_data, rank = 'big',label = 't:t+1'):\n",
    "    ln = len(train_data)\n",
    "    if rank == 'small': # the higher the return, the smaller the number of the rank (i.e. highest return ranked as 1)\n",
    "        rk = train_data[label].rank(ascending = False)\n",
    "    else: # this is the correct one\n",
    "        rk = train_data[label].rank(ascending = True)\n",
    "    p_hat = list(rk/ln)\n",
    "    p_hat_1 = list(1-rk/ln)\n",
    "    lt = [p_hat,p_hat_1]\n",
    "    return np.array(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat_big = W_hat_func(train_data,rank = 'big')\n",
    "np.save('/Users/evaking/desktop/NLP Project/Data/Step2/W_hat_big.npy',W_hat_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-newport",
   "metadata": {},
   "source": [
    "### 2.3 calculate O_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohatfunc(H_hat, W_hat):\n",
    "    O_hat = np.dot(H_hat, W_hat.T)\n",
    "    O_hat = np.dot(O_hat, np.linalg.inv(np.dot(W_hat,W_hat.T)))\n",
    "    O_hat = pd.DataFrame(O_hat.tolist())\n",
    "    O_hat[O_hat < 0] = 0\n",
    "    O_hat = O_hat.apply(lambda x: x/x.sum(), axis = 0)\n",
    "    return O_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "O_hat_av_500 = ohatfunc(H_hat_av_500, W_hat_big).T # transpose matters\n",
    "\n",
    "np.save('/Users/evaking/desktop/NLP Project/Data/Step2/list_O_hat_av_500.npy',O_hat_av_500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-receptor",
   "metadata": {},
   "source": [
    "## Step 3 -- Scoring New Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/test_data_simple.parquet.gzip')\n",
    "#S_hat_av_500 = pd.read_csv('./data/Step1/S_hat_av_500.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-perfume",
   "metadata": {},
   "source": [
    "## get d first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_func(sentlist,data):\n",
    "    SentCharged = sentlist.word.tolist()\n",
    "    SentCharged_bag = []\n",
    "    content = data.str_words.tolist()\n",
    "    for j in tqdm(range(len(content))):\n",
    "        ct_wd = []\n",
    "        for i in range(len(SentCharged)):\n",
    "            ct = content[j].count(SentCharged[i])\n",
    "            ct_wd.append(ct)\n",
    "        SentCharged_bag.append(ct_wd)\n",
    "    return SentCharged_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_av_500 = d_func(S_hat_av_500,test_data) # list\n",
    "np.save('/Users/evaking/desktop/NLP Project/data/Step3/array_d_av_500',d_av_500) # 保存为.npy格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-mouse",
   "metadata": {},
   "source": [
    "## likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llh(p):\n",
    "    summ = sum(np.log(p * O[0] + (1-p) * O[1])*d[flag])\n",
    "    if sum(d[flag]):\n",
    "        likelihood = (summ / sum(d[flag]) + lamb * np.log(p*(1-p)))*(-1) # times (-1) for later minimize func\n",
    "    else:\n",
    "        likelihood = (lamb * np.log(p*(1-p)))*(-1) # times (-1) for later minimize func\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-district",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_score(bnds=(0,1)):\n",
    "    import scipy.optimize as opt # use optimization function\n",
    "    pre = []\n",
    "    for _ in tqdm(range(len(d))):\n",
    "        # get the independent variable \n",
    "        p_optimal = opt.minimize_scalar(fun=llh, bounds=bnds,method = 'bounded').x \n",
    "        global flag\n",
    "        flag +=1 # go through all the articles in the testing sample\n",
    "        pre.append(p_optimal)\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load('/Users/evaking/Desktop/NLP Project/data/Step3/array_d_av_500.npy')\n",
    "O = np.load('/Users/evaking/Desktop/NLP Project/data/Step2/list_O_hat_av_500.npy')\n",
    "flag = 0\n",
    "lamb = 5\n",
    "pre = pre_score()\n",
    "np.save('/Users/evaking/desktop/NLP Project/data/Step3/p_pre/array_p_pre_av500',pre) # 保存为.npy格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-storm",
   "metadata": {},
   "source": [
    "## Correlation calculation in the time window\n",
    "`two kinds of correlation`\n",
    "- pearson correlation\n",
    "- spearman correlation\n",
    "\n",
    "`two time window`\n",
    "- the correlation between predicted score at time t and return label time [t:t+1]\n",
    "- the correlation between predicted score at time t and return label time [t+2:t+6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pre_av_500 = np.load('/Users/evaking/desktop/NLP Project/data/Step3/p_pre/array_p_pre_av500.npy').tolist()\n",
    "test_data['p_pre_av_500'] = p_pre_av_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corre_df(data,name):\n",
    "    lt_p = []\n",
    "    lt_s = []\n",
    "    date_list = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        if (i>1 and not i%100) or i==len(data)-1:\n",
    "            cor_p = data[[name,'t:t+1','t+2:t+6']][:i].corr().iloc[0][1:].tolist()\n",
    "            cor_s = data[[name,'t:t+1','t+2:t+6']][:i].corr(method = 'spearman').iloc[0][1:].tolist()\n",
    "            lt_p.append(cor_p)\n",
    "            lt_s.append(cor_s)\n",
    "            date_list.append(data.create_date[i-1])\n",
    "    dt = pd.DataFrame(date_list)\n",
    "    lt_p = pd.DataFrame(lt_p)\n",
    "    lt_s = pd.DataFrame(lt_s)\n",
    "    df = pd.concat([dt,lt_p,lt_s],axis = 1)\n",
    "    df.columns = ['date','p:[t:t+1]','p:[t+2:t+6]','s:[t:t+1]','s:[t+2:t+6]']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = corre_df(test_data,'p_pre_av_500')\n",
    "corr.to_csv('/Users/evaking/desktop/NLP Project/data/correlation/corr_av500.csv',\n",
    "            index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-flood",
   "metadata": {},
   "source": [
    "# Extention -- pre-trained word embedding corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-facility",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-flash",
   "metadata": {},
   "source": [
    "### Data preprocess for test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_data = pd.read_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/test_data_simple.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsfortest(test_data):\n",
    "    words = []\n",
    "    lt = test_data.str_words.tolist()\n",
    "    for i in tqdm(range(len(lt))):\n",
    "        wd = []\n",
    "        ltt = lt[i].split(' ')\n",
    "        for j in range(len(ltt)):\n",
    "            if ltt[j]:\n",
    "                wd.append(eval(ltt[j]))\n",
    "        words.append(wd)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test = wordsfortest(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_all = []\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    sen_all.append(' '.join(test_data.new[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned words in testing data (the format is the same as train data)\n",
    "test_data['str_words'] = sen_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/test_data_simple.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-november",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('/Users/evaking/Desktop/NLP Project/data_preprocess/train_data_simple.parquet.gzip')\n",
    "test_data = pd.read_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/test_data_simple.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-bench",
   "metadata": {},
   "source": [
    "### get the pretrained data\n",
    "1. Ngram2vec pre-trained word vectors\n",
    "    - https://github.com/Embedding/Chinese-Word-Vectors/blob/master/README_zh.md\n",
    "    - Word2vec / Skip-Gram with Negative Sampling (SGNS) : Financial News 金融新闻\n",
    "2. DSG pre-trained word vectors\n",
    "    - https://ai.tencent.com/ailab/nlp/en/embedding.html\n",
    "    \n",
    "`The following procedure is the same for above two methods so only show one here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open('/Users/evaking/Desktop/NLP Project/data/sgns.financial.word.txt', 'r',encoding=\"utf-8\") as file_to_read:\n",
    "     for line in file_to_read:\n",
    "            line = line.strip('\\n')\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = embeddings_dict.pop('467370')\n",
    "print('The number of words in the file:', len(embeddings_dict), 'and the dimension size:', dim[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-miracle",
   "metadata": {},
   "source": [
    "### get the whole words in the training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bag=[]\n",
    "tt = train_data.str_words.tolist()\n",
    "for i in range(len(tt)):\n",
    "    word_bag.extend(tt[i].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = set(word_bag)\n",
    "#len(bag) = 248043"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-truck",
   "metadata": {},
   "source": [
    "### find the overlapped words for training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_words = list(embeddings_dict)\n",
    "embed_words = set(embed_words)\n",
    "overlap = embed_words&bag\n",
    "#len(overlap) = 71475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "oolist = list(overlap)\n",
    "np.save('/Users/evaking/Desktop/NLP Project/data/Step1/overlap_w2v.npy',oolist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-trick",
   "metadata": {},
   "source": [
    "### new word vectors only for the overlapped words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {key: value for key, value in embeddings_dict.items() if key in overlap}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-opportunity",
   "metadata": {},
   "source": [
    "## construct article level vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_vec(overlap,words):\n",
    "    words = words.split(' ')\n",
    "    vec = np.zeros(int(dim[0]))\n",
    "    for i in range(len(words)):\n",
    "        \n",
    "        if words[i] in overlap:\n",
    "            vec += new_dict[words[i]]\n",
    "    return vec.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = train_data.str_words.tolist()\n",
    "article_vec_list = all_article_vec(overlap,words)\n",
    "#np.save('/Users/evaking/desktop/NLP Project/Data/model extension/array_train.npy',article_vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_hat_w2v = np.array(article_vec_list).T\n",
    "#np.save('/Users/evaking/Desktop/NLP Project/Data/Step2/array_H_hat_w2v.npy',H_hat_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-seminar",
   "metadata": {},
   "source": [
    "## calculate O_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat_big = np.load('/Users/evaking/desktop/NLP Project/Data/Step2/W_hat_big.npy')\n",
    "H_hat_w2v = np.load('/Users/evaking/Desktop/NLP Project/Data/Step2/array_H_hat_w2v.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "oolist = np.load('/Users/evaking/Desktop/NLP Project/data/Step1/overlap_w2v.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohatfunc is the same as Step 2 -- 3.5.3 2.3 calculate O_hat\n",
    "O_hat_w2v = ohatfunc(H_hat_w2v, W_hat_big).T # transpose matters\n",
    "#np.save('/Users/evaking/desktop/NLP Project/Data/Step2/list_O_hat_w2v.npy',O_hat_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "O_hat_w2v = np.load('/Users/evaking/desktop/NLP Project/Data/Step2/list_O_hat_w2v.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-postcard",
   "metadata": {},
   "source": [
    "## get article level vector for testing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-wound",
   "metadata": {},
   "source": [
    "### get the whole words in the testing sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bag_test =[]\n",
    "tt_test = test_data.str_words.tolist()\n",
    "for i in range(len(tt)):\n",
    "    word_bag_test.extend(tt_test[i].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_test = set(word_bag_test)\n",
    "#len(bag_test) = 266567\n",
    "lt = list(bag_test)\n",
    "del(lt[0])\n",
    "bag_test_new = lt\n",
    "# len(bag_test_new) = 266566"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-accused",
   "metadata": {},
   "source": [
    "### find the overlapped words with training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_words = list(embeddings_dict)\n",
    "embed_words = set(embed_words)\n",
    "overlap_test = set(bag_test_new)&overlap\n",
    "#len(overlap_test) = 58799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict_test = {key: value for key, value in embeddings_dict.items() if key in overlap_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_t = test_data.str_words.tolist()\n",
    "article_vec_list_test = all_article_vec(overlap_test,words_t)\n",
    "#np.save('/Users/evaking/desktop/NLP Project/Data/model extension/array_test.npy',article_vec_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_test = np.load('/Users/evaking/desktop/NLP Project/Data/model extension/array_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-fundamentals",
   "metadata": {},
   "source": [
    "## SESTM method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here no sum(d) -- seems make sense but the result is strange\n",
    "def llh_extension(p):\n",
    "    summ = sum(np.log(p * O[0] + (1-p) * O[1])*d[flag])\n",
    "   \n",
    "    likelihood = (summ  + lamb * np.log(p*(1-p)))*(-1) # times (-1) for later minimize func\n",
    "    \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_score_extension(bnds=(0,1)):\n",
    "    import scipy.optimize as opt # use optimization function\n",
    "    pre = []\n",
    "    for _ in tqdm(range(len(d))):\n",
    "        # get the independent variable \n",
    "        p_optimal = opt.minimize_scalar(fun=llh_extension, bounds=bnds,method = 'bounded').x \n",
    "        global flag\n",
    "        flag +=1 # go through all the articles in the testing sample\n",
    "        pre.append(p_optimal)\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load('/Users/evaking/desktop/NLP Project/Data/model extension/array_test.npy')\n",
    "O = np.load('/Users/evaking/desktop/NLP Project/Data/Step2/list_O_hat_w2v.npy')\n",
    "flag = 0\n",
    "lamb = 5\n",
    "pre = pre_score_extension()\n",
    "#np.save('/Users/evaking/desktop/NLP Project/data/Step3/p_pre/array_p_pre_av500',pre) # 保存为.npy格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pre_sestm_w2v'] = pre # the result is strange -- many are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-partition",
   "metadata": {},
   "source": [
    "## Directly method\n",
    "- calculate the distance between new article vector and learned O+ and O-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_pos = np.linalg.norm(array_test-O_hat_w2v[0],axis = 1)\n",
    "dis_neg = np.linalg.norm(array_test-O_hat_w2v[1],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['pre_w2v_diff'] = (dis_pos-dis_neg).tolist()\n",
    "test_data['pre_w2v_diff_np'] = (dis_neg-dis_pos).tolist()\n",
    "test_data['pre_w2v_pos'] = (dis_pos).tolist()\n",
    "test_data['pre_w2v_neg'] = (dis_neg).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.corr().loc[['t:t+1','t+2:t+6']]\n",
    "# correlation \n",
    "#pre_w2v_diff  t:t+1 -0.058777 t+2:t+6 -0.000974\n",
    "#pre_w2v_diff_np  t:t+1 0.058777 t+2:t+6 0.000974  this one is selected for the final comparison\n",
    "#pre_w2v_pos t:t+1 0.005595 t+2:t+6 0.001548\n",
    "#pre_w2v_neg t:t+1 0.005701 t+2:t+6 0.001551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_np = corre_df(test_data,'pre_w2v_diff_np')\n",
    "corr_np.to_csv('/Users/evaking/desktop/NLP Project/data/correlation/corr_w2v_dis.csv',\n",
    "            index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-assurance",
   "metadata": {},
   "source": [
    "## Directly jump to prediction\n",
    "### Model selection summary\n",
    "`Regression`\n",
    "\n",
    "    - Linear Regression\n",
    "    - SVR\n",
    "    - Random Forest -- too slow -- ignore\n",
    "    \n",
    "`Classification`\n",
    "\n",
    "    - Logistic Regression\n",
    "    - KNN\n",
    "    - SVM -- twoo slow -- ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-bench",
   "metadata": {},
   "source": [
    "### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "regr.fit(article_vec_list, return_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(article_vec_list_test)\n",
    "test_data['pre_w2v_LR'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation: t:t+1 0.076263  t+2:t+6 0.018424\n",
    "corr_LR = corre_df(test_data,'pre_w2v_LR')\n",
    "corr_LR.to_csv('/Users/evaking/desktop/NLP Project/data/correlation/corr_n2v_LR.csv',\n",
    "            index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-corporation",
   "metadata": {},
   "source": [
    "### SVM regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model_SVR = svm.SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the training sets\n",
    "model_SVR.fit(article_vec_list, return_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing set\n",
    "y_pred_SVR = model_SVR.predict(article_vec_list_test)\n",
    "test_data['pre_w2v_SVR'] = y_pred_SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation: t:t+1 0.082435  t+2:t+6 0.027413\n",
    "corr_SVR = corre_df(test_data,'pre_w2v_SVR')\n",
    "corr_SVR.to_csv('/Users/evaking/desktop/NLP Project/data/correlation/corr_n2v_SVR.csv',\n",
    "            index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-rabbit",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "`threshold change`\n",
    "- threshold 0\n",
    "- three categories\n",
    "- threshold top 30% and bottom 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression(max_iter=1000)\n",
    "logistic_30 = linear_model.LogisticRegression(max_iter=1000)\n",
    "logistic_out = linear_model.LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold 0\n",
    "return_y_classification_0 = [1  if x >=0 else 0 for x in return_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.fit(article_vec_list, return_y_classification_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pred = logistic.predict_proba(article_vec_list_test)\n",
    "test_data['pre_w2v_logit_1'] = logit_pred.T[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_logit_1 = corre_df(test_data,'pre_w2v_logit_1')\n",
    "corr_logit_1.to_csv('/Users/evaking/desktop/NLP Project/data/correlation/corr_n2v_logit.csv',\n",
    "            index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 30%\n",
    "top30 = np.quantile(np.array(return_y), .70)\n",
    "# bottom 30%\n",
    "bottom30 = np.quantile(np.array(return_y), .30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three categories\n",
    "return_y_classification_30 = [1 if x >=top30 else -1 if x<=bottom30 else 0 for x in return_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_30.fit(article_vec_list, return_y_classification_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pred_30 = logistic_30.predict_proba(article_vec_list_test)\n",
    "test_data['pre_w2v_logit_1_30'] = logit_pred_30.T[2].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top and bottom\n",
    "only30 = [i for i in range(len(return_y_classification_30)) if return_y_classification_30[i] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_train = np.load('/Users/evaking/desktop/NLP Project/Data/model extension/array_train.npy')\n",
    "array_test = np.load('/Users/evaking/desktop/NLP Project/Data/model extension/array_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_train = [array_train[i] for i in tqdm(range(0, len(array_train), 1)) if i not in only30]\n",
    "return_y_out = [return_y_classification_30[i] for i in tqdm(range(0, len(return_y_classification_30), 1)) if i not in only30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_out.fit(array_train, return_y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pred_out = logistic_out.predict_proba(array_test)\n",
    "test_data['pre_w2v_logit_out'] = logit_pred_out.T[1].tolist()\n",
    "np.save('/Users/evaking/desktop/NLP Project/data/Step3/p_pre/array_p_pre_logitc_out',logit_pred_out.T[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_logit_out = corre_df(test_data,'pre_w2v_logit_out')\n",
    "corr_logit_out.to_csv('/Users/evaking/desktop/NLP Project/data/correlation/corr_n2v_logit_out.csv',\n",
    "            index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation: \n",
    "# threshold 0\n",
    "    # t:t+1 0.092801  t+2:t+6 0.019043\n",
    "# three categories\n",
    "    # t:t+1 0.094214  t+2:t+6 0.021277\n",
    "# top and bottom 30%\n",
    "    # t:t+1 0.095062  t+2:t+6 0.019215"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-bhutan",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "kneighbor = KNeighborsClassifier(500)\n",
    "# threshold 0\n",
    "time_start=time.time()\n",
    "kneighbor.fit(article_vec_list, return_y_classification_0)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start=time.time()\n",
    "kneighbor_pred = kneighbor.predict_proba(article_vec_list_test)\n",
    "test_data['pre_w2v_kneighbor_0'] = kneighbor_pred.T[1].tolist()\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_kneighbor_0 = corre_df(test_data,'pre_w2v_kneighbor_0')\n",
    "corr_kneighbor_0.to_csv('/Users/evaking/desktop/NLP Project/data/correlation/corr_n2v_kneighbor.csv',\n",
    "            index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "kneighbor = KNeighborsClassifier(500)\n",
    "# threshold top30, bottom 30\n",
    "time_start=time.time()\n",
    "kneighbor.fit(array_train, return_y_out)\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start=time.time()\n",
    "kneighbor_pred_out = kneighbor.predict_proba(array_test)\n",
    "test_data['pre_w2v_kneighbor_out'] = kneighbor_pred_out.T[1].tolist()\n",
    "time_end=time.time()\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_kneighbor_out = corre_df(test_data,'pre_w2v_kneighbor_out')\n",
    "corr_kneighbor_out.to_csv('/Users/evaking/desktop/NLP Project/data/correlation/corr_n2v_kneighbor_out.csv',\n",
    "            index=False,header=True, encoding = 'utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation: \n",
    "# threshold 0\n",
    "    # t:t+1 0.065391  t+2:t+6 0.006842\n",
    "# top and bottom 30%\n",
    "    # t:t+1 0.071280  t+2:t+6 0.006468"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-tourist",
   "metadata": {},
   "source": [
    "# Performance -- Correlation plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-pacific",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['av100','av500','av100_2p','av500_2p','ex100','ex500',\n",
    "        'ex100_2p','ex500_2p','git','benchmarkscore6',\n",
    "        'n2v_dis','n2v_LR','n2v_SVR','n2v_logit','n2v_logit_out','n2v_kneighbor','n2v_kneighbor_out',\n",
    "       'DSG_dis','DSG_LR','DSG_SVR','DSG_logit','DSG_logit_out','DSG_kneighbor','DSG_kneighbor_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "sestm = ['av100','av500','av100_2p','av500_2p','ex100','ex500',\n",
    "        'ex100_2p','ex500_2p','git','benchmarkscore6']\n",
    "extension_w2v = ['av500_2p','git',\n",
    "             'n2v_dis','n2v_LR','n2v_SVR','n2v_logit','n2v_logit_out','n2v_kneighbor','n2v_kneighbor_out']\n",
    "extension_glove = ['av500_2p','git',\n",
    "             'DSG_dis','DSG_LR','DSG_SVR','DSG_logit','DSG_logit_out','DSG_kneighbor','DSG_kneighbor_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-folks",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for n in name:\n",
    "    # original and git\n",
    "    exec('corr_{} = pd.read_csv(\"/Users/evaking/desktop/NLP Project/data/correlation/corr_{}.csv\",\\\n",
    "         parse_dates = [\"date\"],infer_datetime_format = True)'.format(n,n))\n",
    "    # for training data: range outside of [-2%,2%]\n",
    "    if '2p' in n:\n",
    "        exec('corr_{}_out2 = pd.read_csv(\"/Users/evaking/desktop/NLP Project/data/correlation/corr_{}_out2.csv\",\\\n",
    "         parse_dates = [\"date\"],infer_datetime_format = True)'.format(n,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-final",
   "metadata": {},
   "source": [
    "## Plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parms(plt,interval = 0.02):\n",
    "    plt.xlabel('Year',fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "    plt.ylabel('Correlation',fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "    plt.xticks(size = 15)\n",
    "    plt.yticks(size = 15)\n",
    "    y_major_locator = MultipleLocator(interval)\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_locator(y_major_locator)\n",
    "    import matplotlib.dates as mdates\n",
    "    ax.xaxis.set_major_locator(locator=mdates.YearLocator())\n",
    "    plt.grid(linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_plot(name, git = True, method = 'p'):\n",
    "    colors = ['b','g','r','c','m','y','darkorange','lightpink','k','darkgrey']\n",
    "    for i in range(len(name)):\n",
    "        n = name[i]\n",
    "        if git == True:\n",
    "            nn = 'corr_{}'.format(n)\n",
    "        else:\n",
    "            if '2p' in n:\n",
    "                nn_out2 = 'corr_{}_out2'.format(n)\n",
    "                nn = 'corr_{}'.format(n)\n",
    "                plot_data_out2 = eval(nn_out2)\n",
    "                ax.plot(plot_data_out2['date'],plot_data_out2[method+':[t:t+1]'],colors[i],linestyle=':',label = nn+',[t:t+1],out2')\n",
    "                ax.plot(plot_data_out2['date'],plot_data_out2[method+':[t+2:t+6]'],colors[i],linestyle='-.',label = nn+',[t+2:t+6],out2')\n",
    "            else:\n",
    "                continue\n",
    "        plot_data = eval(nn)\n",
    "        ax.plot(plot_data['date'],plot_data[method+':[t:t+1]'],colors[i],label = nn+',[t:t+1]')\n",
    "        ax.plot(plot_data['date'],plot_data[method+':[t+2:t+6]'],colors[i],linestyle = 'dashed',label = nn+',[t+2:t+6]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-paris",
   "metadata": {},
   "source": [
    "## SESTM method variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  (Pearson Correlation)\n",
    "fig = plt.figure(figsize=(16, 10), dpi=80)\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plt.title('Pearson Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt)\n",
    "plt.ylim(0,0.32)\n",
    "# plot sestm method\n",
    "corr_plot(sestm)\n",
    "plt.legend()\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plt.title('close look at Pearson Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt,interval = 0.01)\n",
    "plt.ylim(0,0.10)\n",
    "corr_plot(sestm)\n",
    "#plt.legend(loc = 7, prop = {'size':7})\n",
    "plt.legend(bbox_to_anchor=(0.6,0.4,0.3,0),ncol=1,loc=7,mode='expand',borderaxespad=2, prop = {'size':8})\n",
    "\n",
    "plt.suptitle('SESTM method variation (Pearson Correlation)',fontsize=22)\n",
    "plt.savefig('/Users/evaking/desktop/NLP Project/pics/SESTM method variation_pearson.png')\n",
    "\n",
    "########################\n",
    "# (Spearman Correlation)\n",
    "fig = plt.figure(figsize=(16, 10), dpi=80)\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plt.title('Spearman Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt)\n",
    "plt.ylim(-0.03,0.285)\n",
    "corr_plot(sestm,method = 's')\n",
    "plt.legend(prop = {'size':9})\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plt.title('close look at Spearman Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt,interval = 0.01)\n",
    "plt.ylim(-0.03,0.12)\n",
    "corr_plot(sestm,method = 's')\n",
    "plt.legend(bbox_to_anchor=(0.57,0.5,0.3,0),ncol=1,loc=7,mode='expand',borderaxespad=2, prop = {'size':8})\n",
    "\n",
    "plt.suptitle('SESTM method variation (Spearman Correlation)',fontsize=22)\n",
    "plt.savefig('/Users/evaking/desktop/NLP Project/pics/SESTM method variation_spearman.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-glass",
   "metadata": {},
   "source": [
    "## Model Extention -- n2v and DSG implementation\n",
    "- only show n2v implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Pearson Correlation)\n",
    "fig = plt.figure(figsize=(16, 10), dpi=80)\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plt.title('Pearson Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt)\n",
    "plt.ylim(-0.018,0.32)\n",
    "# plot extension method\n",
    "corr_plot(extension_w2v)\n",
    "plt.legend(loc=1,prop = {'size':10})\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plt.title('close look at Pearson Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt,interval = 0.01)\n",
    "plt.ylim(-0.015,0.10)\n",
    "corr_plot(extension_w2v)\n",
    "#plt.legend(loc = 7, prop = {'size':7})\n",
    "plt.legend(bbox_to_anchor=(0.6,0.48,0.3,0),ncol=1,loc=7,mode='expand',borderaxespad=2, prop = {'size':7})\n",
    "\n",
    "plt.suptitle('Algorithm Extention -- ngram2vec implementation (Pearson Correlation)',fontsize=22)\n",
    "plt.savefig('/Users/evaking/desktop/NLP Project/pics/Model Extention_pearson.png')\n",
    "\n",
    "########################\n",
    "# (Spearman Correlation)\n",
    "fig = plt.figure(figsize=(16, 10), dpi=80)\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plt.title('Spearman Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt)\n",
    "plt.ylim(-0.034,0.285)\n",
    "corr_plot(extension_w2v,method = 's')\n",
    "plt.legend(prop = {'size':9})\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plt.title('close look at Spearman Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt,interval = 0.01)\n",
    "plt.ylim(-0.034,0.14)\n",
    "corr_plot(extension_w2v,method = 's')\n",
    "plt.legend(bbox_to_anchor=(0.1,0.5,0.3,0),ncol=1,loc=7,mode='expand',borderaxespad=2, prop = {'size':8})\n",
    "\n",
    "plt.suptitle('Algorithm Extention -- ngram2vec implementation (Spearman Correlation)',fontsize=22)\n",
    "plt.savefig('/Users/evaking/desktop/NLP Project/pics/Model Extention_spearman.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-torture",
   "metadata": {},
   "source": [
    "## SESTM method variation -- range outside of [-2%,2%]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Pearson Correlation)\n",
    "fig = plt.figure(figsize=(16, 8), dpi=80)\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plt.title('Pearson Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt)\n",
    "plt.ylim(0,0.32)\n",
    "corr_plot(name, git = False)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plt.title('close look at Pearson Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt,interval = 0.01)\n",
    "plt.ylim(0,0.10)\n",
    "corr_plot(name, git = False)\n",
    "plt.legend(loc = 7, prop = {'size':9})\n",
    "\n",
    "plt.suptitle('SESTM method variation -- range outside of [-2%,2%] (Pearson Correlation)',fontsize=22)\n",
    "plt.savefig('/Users/evaking/desktop/NLP Project/pics/out2_pearson.png')\n",
    "\n",
    "########################\n",
    "# (Spearman Correlation)\n",
    "fig = plt.figure(figsize=(16, 8), dpi=80)\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "plt.title('Spearman Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt)\n",
    "plt.ylim(-0.03,0.3)\n",
    "corr_plot(name, git = False,method = 's')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "plt.title('close look at Spearman Correlation change', fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "plot_parms(plt,interval = 0.01)\n",
    "plt.ylim(-0.03,0.12)\n",
    "corr_plot(name, git = False,method = 's')\n",
    "plt.legend(bbox_to_anchor=(0.7,0.45,0.3,0),loc = 7, prop = {'size':9})\n",
    "\n",
    "plt.suptitle('SESTM method variation -- range outside of [-2%,2%] (Spearman Correlation)',fontsize=22)\n",
    "plt.savefig('/Users/evaking/desktop/NLP Project/pics/out2_spearman.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-marketplace",
   "metadata": {},
   "source": [
    "# Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-massage",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "- get the average predicted score for each stock at each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data\n",
    "test_data = pd.read_parquet('/Users/evaking/Desktop/NLP Project/data_preprocess/test_data_simple.parquet.gzip')\n",
    "# predicted score\n",
    "pre_score = np.load('/Users/evaking/desktop/NLP Project/data/Step3/p_pre/array_p_pre_logitc_out.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['extension_w2v_logit_out'] = pre_score # also add other predicted score from different methods, omit here\n",
    "test_data = test_data.drop(['ID','t:t+1','t+2:t+6','str_words'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.groupby(['SecuCode','create_date']).mean().reset_index()\n",
    "test_data.create_date = pd.to_datetime(test_data.create_date, format='%Y-%m-%d', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is previous result\n",
    "test = pd.read_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/test_data_score.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.merge(test,test_data,how = 'left',on=['SecuCode','create_date'])\n",
    "order = ['SecuCode', 'create_date', 'extension_w2v_logit_out','av_500_2p', 'av_100_2p', 'ex_500_2p',\n",
    "       'ex_100_2p', 'year', 'month']\n",
    "test_data = test_data[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data.to_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/test_data_score.parquet.gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-jimmy",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "msf = pd.read_csv('/Users/evaking/desktop/NLP Project/data/msf.csv')\n",
    "msf_new = msf[['SecuCode','SecuAbbr','date','ret']]\n",
    "msf_new.date = pd.to_datetime(msf_new.date, format='%Y-%m-%d', errors='ignore')\n",
    "msf_new['year'] = msf_new.date.dt.year\n",
    "msf_new['month'] = msf_new.date.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose score 6 as the sentiment score for dictionay based method\n",
    "git1 = pd.read_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/git1_score.parquet.gzip')\n",
    "git2 = pd.read_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/git2_score.parquet.gzip')\n",
    "git1 = git1[['SecuCode','create_date','year','month','score6']]\n",
    "git2 = git2[['SecuCode','create_date','year','month','score6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose av_500_2p and extension_w2v_logit_out score as the sentiment score for SESTM method and its extension\n",
    "sestm = pd.read_parquet('/Users/evaking/desktop/NLP Project/data_preprocess/test_data_score.parquet.gzip')\n",
    "av500_2p = sestm[['SecuCode','create_date','year','month','av_500_2p']]\n",
    "extension_w2v_logit_out = sestm[['SecuCode','create_date','year','month','extension_w2v_logit_out']]\n",
    "av500_2p.rename(columns={\"av_500_2p\": \"score6\"},inplace = True)\n",
    "extension_w2v_logit_out.rename(columns={\"extension_w2v_logit_out\": \"score6\"},inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-wound",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = data.sort_values(['SecuCode','create_date']).reset_index(drop = True)\n",
    "    data.create_date = pd.to_datetime(data.create_date, format='%Y-%m-%d', errors='ignore')\n",
    "    data.loc[:,'SecuCode'] = data.loc[:,'SecuCode'].astype('object')\n",
    "    # average score for one day\n",
    "    data = data.groupby(['SecuCode','create_date']).mean()\n",
    "    data.reset_index(inplace = True)\n",
    "    data['year'] = data['create_date'].dt.year\n",
    "    data['month'] = data['create_date'].dt.month\n",
    "    df = data[['SecuCode','create_date','year','month']]\n",
    "    df = df.drop_duplicates(['SecuCode','year','month'],keep = 'last') # to track the last date of one month\n",
    "    # average score for one month\n",
    "    new = data.groupby(['SecuCode','year','month']).mean()\n",
    "    new = pd.merge(new,df,on = ['SecuCode','year','month'])\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "git1 = preprocess(git1)\n",
    "git2 = preprocess(git2)\n",
    "av500_2p = preprocess(av500_2p)\n",
    "extension_w2v_logit_out = preprocess(extension_w2v_logit_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_calendar(data,day = 30):\n",
    "    month = day//30\n",
    "    data['pre_'+str(day)+'d_month'] = data.month-month\n",
    "    data['pre_'+str(day)+'d_year'] = data.year\n",
    "    data.loc[data['pre_'+str(day)+'d_month']<=0,'pre_'+str(day)+'d_year'] = data.loc[data['pre_'+str(day)+'d_month']<=0]['year']-1\n",
    "    data.loc[data['pre_'+str(day)+'d_month']==0,'pre_'+str(day)+'d_month'] = 12\n",
    "    data.loc[data['pre_'+str(day)+'d_month']==-1,'pre_'+str(day)+'d_month'] = 11\n",
    "    data.loc[data['pre_'+str(day)+'d_month']==-2,'pre_'+str(day)+'d_month'] = 10\n",
    "    return data[['pre_'+str(day)+'d_month','pre_'+str(day)+'d_year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(data,score_list = ['score6']):\n",
    "    previous_calendar(data,day = 30)\n",
    "    previous_calendar(data,day = 60)\n",
    "    lt = ['SecuCode','year','month']\n",
    "    lt.extend(score_list)\n",
    "    lt_o = [a+'_o' for a in lt]\n",
    "    git1_original = data[lt]\n",
    "    git1_original.columns = lt_o\n",
    "    return git1_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "git1_original = df(git1)\n",
    "git2_original = df(git2)\n",
    "av500_2p_original = df(av500_2p)\n",
    "extension_w2v_logit_out_original = df(extension_w2v_logit_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(git1,git1_original):\n",
    "    git1_df = pd.merge(git1,git1_original,how = 'left',\n",
    "             left_on = ['SecuCode','pre_30d_month','pre_30d_year'],\n",
    "             right_on = ['SecuCode_o','month_o','year_o']).dropna()\n",
    "    git1_df.rename(columns={\"score6_o\": \"score6_pre1m\"},inplace = True)\n",
    "    git1_df = git1_df[['SecuCode','year','month','score6_pre1m']]\n",
    "    \n",
    "    git1_df_60 = pd.merge(git1,git1_original,how = 'left',\n",
    "             left_on = ['SecuCode','pre_60d_month','pre_60d_year'],\n",
    "             right_on = ['SecuCode_o','month_o','year_o']).dropna()\n",
    "    git1_df_60.rename(columns={\"score6_o\": \"score6_pre2m\"},inplace = True)\n",
    "    git1_df_60 = git1_df_60[['SecuCode','year','month','score6_pre2m']]\n",
    "    \n",
    "    df = pd.merge(git1,git1_df,how = 'left',on = ['SecuCode','year','month'])\n",
    "    df = pd.merge(df,git1_df_60,how = 'left',on = ['SecuCode','year','month'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_df(git1,git1_original):\n",
    "    git1_df = match(git1,git1_original)\n",
    "    git1_final = git1_df[['SecuCode','create_date','year','month','score6','score6_pre1m','score6_pre2m']]\n",
    "    git1_final['score6_pre1m_ave'] = git1_final[['score6','score6_pre1m']].mean(axis = 1)\n",
    "    git1_final['score6_pre2m_ave'] = git1_final[['score6','score6_pre1m','score6_pre2m']].mean(axis = 1)\n",
    "    git1_final['1m+month'] = git1_final.month+1\n",
    "    git1_final['1m+year'] = git1_final.year\n",
    "    git1_final.loc[git1_final['1m+month']==13,\n",
    "               '1m+year'] = git1_final.loc[git1_final['1m+month']==13]['1m+year']+1\n",
    "    git1_final.loc[git1_final['1m+month']==13,\n",
    "               '1m+month'] = 1\n",
    "    git1_final = pd.merge(git1_final,msf_new,how='left',\n",
    "                          left_on = ['SecuCode','1m+year','1m+month'],right_on = ['SecuCode','year','month'])\n",
    "    git1_final = git1_final.drop(['year_y','month_y','year_x','month_x','1m+month','1m+year'],axis = 1)\n",
    "    return git1_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "git1_final = final_df(git1,git1_original)\n",
    "git2_final = final_df(git2,git2_original)\n",
    "av500_2p_final = final_df(av500_2p,av500_2p_original)\n",
    "extension_w2v_logit_out_final = final_df(extension_w2v_logit_out,extension_w2v_logit_out_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_df(data):\n",
    "    data['year'] = data['create_date'].dt.year\n",
    "    data['month'] = data['create_date'].dt.month\n",
    "    data = data.dropna(subset=['ret']).reset_index(drop=True)\n",
    "    data = data.drop(['score6_pre1m','score6_pre2m','SecuAbbr','create_date','date'],axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "git1_final_ = simple_df(git1_final)\n",
    "git2_final_ = simple_df(git2_final)\n",
    "av500_2p_final_ = simple_df(av500_2p_final)\n",
    "extension_w2v_logit_out_final_ = simple_df(extension_w2v_logit_out_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-jewel",
   "metadata": {},
   "source": [
    "## Cross-sectional Correlation calculatioin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_df(data,method = 'pearson'):\n",
    "    # previous 30 days\n",
    "    df = data.groupby(['year','month'])[['score6','ret']].corr(method = method).reset_index()\n",
    "    corr30 = df[ ~ df['level_2'].str.contains('ret') ].drop(['score6','level_2','year','month'],axis = 1).mean()[0]\n",
    "    df = data.groupby(['year','month'])[['score6_pre1m_ave','ret']].corr(method = method).reset_index()\n",
    "    corr60 = df[ ~ df['level_2'].str.contains('ret') ].drop(['score6_pre1m_ave','level_2','year','month'],axis = 1).mean()[0]\n",
    "    df = data.groupby(['year','month'])[['score6_pre2m_ave','ret']].corr(method = method).reset_index()\n",
    "    corr90 = df[ ~ df['level_2'].str.contains('ret') ].drop(['score6_pre2m_ave','level_2','year','month'],axis = 1).mean()[0]\n",
    "    pearson = pd.DataFrame([corr30,corr60,corr90]).T\n",
    "    pearson.columns = ['pre_30days','pre_60days','pre_90days']\n",
    "    return pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_df(data):\n",
    "    df_p = cross_df(data)\n",
    "    df_s = cross_df(data,'spearman')\n",
    "    df = pd.concat([df_p,df_s])\n",
    "    df.index = pd.Series(['pearson', 'spearman'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SESTM method: extension_w2v_logit_out\n",
    "corr_extension_w2v_logit_out = corr_df(extension_w2v_logit_out_final_)\n",
    "corr_extension_w2v_logit_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SESTM method: av_500_2p\n",
    "corr_av500_2p = corr_df(av500_2p_final_)\n",
    "corr_av500_2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# github link 1\n",
    "corr_git1 = corr_df(git1_final)\n",
    "corr_git1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# github link 2\n",
    "corr_git2 = corr_df(git2_final)\n",
    "corr_git2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-basis",
   "metadata": {},
   "source": [
    "## 11 portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-wagon",
   "metadata": {},
   "source": [
    "### 11 portfolios construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cum_return(av500_2p_30_hedge,name): # to get the compound return for the hedge portfolio\n",
    "    import calendar\n",
    "    lt = []\n",
    "    for i in range(len(av500_2p_30_hedge)):\n",
    "        lt.append(calendar.monthrange(av500_2p_30_hedge.year[i],av500_2p_30_hedge.month[i])[1])\n",
    "        \n",
    "    av500_2p_30_hedge['day'] = lt\n",
    "    av500_2p_30_hedge['date'] = av500_2p_30_hedge['year'].map(str)+\"/\"\\\n",
    "                                +av500_2p_30_hedge['month'].map(str)+\"/\"\\\n",
    "                                +av500_2p_30_hedge['day'].map(str)\n",
    "    av500_2p_30_hedge = av500_2p_30_hedge.set_index('date')\n",
    "    av500_2p_30_hedge = av500_2p_30_hedge.drop(['year','month','day'],1)\n",
    "    av500_2p_30_hedge = av500_2p_30_hedge+1 # for the use of compound calculation\n",
    "    av500_2p_30_hedge = av500_2p_30_hedge.cumprod(axis = 0)\n",
    "    av500_2p_30_hedge = av500_2p_30_hedge.reset_index()\n",
    "    av500_2p_30_hedge.columns = ['date',name]\n",
    "    return av500_2p_30_hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-archive",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def portfolio(test_final,name,score = 'score'):\n",
    "    lt = [score[:6]+'_pre1m',score[:6]+'_pre2m']\n",
    "    test_ls = test_final.drop(lt,axis = 1)\n",
    "    test_ls['year'] = test_ls.create_date.dt.year\n",
    "    test_ls['month'] = test_ls.create_date.dt.month\n",
    "    \n",
    "    test_ls = test_ls.dropna(subset = ['ret',score]).reset_index(drop = True)\n",
    "    \n",
    "    test_ls_1m = test_ls[['SecuCode',score,'ret','year','month']]\n",
    "    \n",
    "    #higher the score,higher the pct\n",
    "    test_ls_1m_pct = test_ls_1m.groupby(['year','month']).rank(ascending=True,method='dense',pct = True)[[score]]\n",
    "    test_ls_1m_pct.columns = [score+'_pct']\n",
    "    # allocate group\n",
    "    test_ls_1m_pct['group'] = test_ls_1m_pct.apply(lambda x: x*10)\n",
    "    \n",
    "    test_ls_1m_pct['group'] = [math.ceil(a) for a in test_ls_1m_pct['group']]\n",
    "    test_ls_1m_pct['group'] = test_ls_1m_pct['group'].astype('int')\n",
    "    \n",
    "    # group for each stock at each month\n",
    "    df = pd.merge(test_ls_1m,test_ls_1m_pct,left_index = True,right_index = True)\n",
    "    #df = df.dropna(subset = ['ret']).reset_index(drop = True)\n",
    "    \n",
    "    return_df = df.groupby(['year','month','group']).mean()[['ret']].reset_index()\n",
    "    \n",
    "    # hedged portfolio\n",
    "    hedge = (return_df.groupby(['year','month']).last()-return_df.groupby(['year','month']).first())[['ret']]\n",
    "    ret = hedge.ret.mean()*12\n",
    "    v = hedge.ret.std()*math.sqrt(12) #volatility\n",
    "    sr = ret/v  #sharpe ratio\n",
    "    \n",
    "    # cumulative compound return\n",
    "    hedge = hedge.reset_index()\n",
    "    if score == 'score6':\n",
    "        day = 30\n",
    "    elif score == 'score6_pre1m_ave':\n",
    "        day = 60\n",
    "    else:\n",
    "        day = 90\n",
    "    hedge = cum_return(hedge,name[1:]+'_'+str(day)+'_hedge')\n",
    "    \n",
    "    # annualized return, risk, sharpe ratio\n",
    "    annual = return_df.groupby('group').mean()[['ret']]*12\n",
    "    annual.rename(columns = {'ret':'annual_return'+name},inplace = True)\n",
    "    vol = return_df.groupby('group').std()[['ret']]*math.sqrt(12)\n",
    "    vol.rename(columns = {'ret':'annual_risk'+name},inplace = True)\n",
    "    result = pd.merge(annual,vol,left_index = True, right_index = True)\n",
    "    result['Sharpe Ratio'+name] = result['annual_return'+name]/result['annual_risk'+name]\n",
    "    \n",
    "    result.loc[11] = [ret,v,sr]\n",
    "    \n",
    "\n",
    "    return result,hedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(av500_2p_final,name):\n",
    "    av500_2p_30 = portfolio(av500_2p_final,name = name,score = 'score6')\n",
    "    av500_2p_60 = portfolio(av500_2p_final,name = name,score = 'score6_pre1m_ave')\n",
    "    av500_2p_90 = portfolio(av500_2p_final,name = name,score = 'score6_pre2m_ave')\n",
    "    return av500_2p_30,av500_2p_60,av500_2p_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SESTM method\n",
    "av500_2p_30,av500_2p_60,av500_2p_90, av500_2p_30_hedge,av500_2p_60_hedge,av500_2p_90_hedge= get_result(av500_2p_final,name = '_av500_2p')\n",
    "extension_w2v_logit_out_30,extension_w2v_logit_out_60,extension_w2v_logit_out_90,extension_w2v_logit_out_30_hedge,extension_w2v_logit_out_60_hedge,extension_w2v_logit_out_90_hedge = get_result(extension_w2v_logit_out_final,name = '_extension_n2v_logit_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# github link 1\n",
    "git1_30,git1_60,git1_90,git1_30_hedge,git1_60_hedge,git1_90_hedge = get_result(git1_final,name = '_git1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# github link 2\n",
    "git2_30,git2_60,git2_90,git2_30_hedge,git2_60_hedge,git2_90_hedge = get_result(git2_final,name = '_git2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-packaging",
   "metadata": {},
   "source": [
    "### Result plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-weather",
   "metadata": {},
   "source": [
    "#### cumulative return for hedged portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedge_plot(av500_2p_30_hedge,extension_w2v_logit_out_30_hedge,git1_30_hedge,git2_30_hedge):\n",
    "    data = pd.merge(av500_2p_30_hedge,extension_w2v_logit_out_30_hedge,how='left', on=['date'])\n",
    "    data = pd.merge(data,git1_30_hedge,how='left', on=['date'])\n",
    "    data = pd.merge(data,git2_30_hedge,how='left', on=['date'])\n",
    "    data.date = pd.to_datetime(data.date, format='%Y-%m-%d', errors='ignore')\n",
    "    return data#.set_index(['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_30 =hedge_plot(av500_2p_30_hedge,extension_w2v_logit_out_30_hedge,git1_30_hedge,git2_30_hedge)\n",
    "data_60 =hedge_plot(av500_2p_60_hedge,extension_w2v_logit_out_60_hedge,git1_30_hedge,git2_60_hedge)\n",
    "data_90 =hedge_plot(av500_2p_90_hedge,extension_w2v_logit_out_90_hedge,git1_90_hedge,git2_90_hedge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parms():\n",
    "    plt.xlabel('Year',fontdict={'family' : 'Times New Roman', 'size'   : 30})\n",
    "    plt.ylabel('Portfolio Value',fontdict={'family' : 'Times New Roman', 'size'   : 30})\n",
    "    plt.xticks(size = 15)\n",
    "    plt.yticks(size = 15)\n",
    "    ax = plt.gca()\n",
    "    plt.grid(linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_func(title,data,lt,size = 18):\n",
    "    plt.title(title, fontdict={'family' : 'Times New Roman', 'size'   : 30})\n",
    "    plot_parms()\n",
    "    for i in range(len(lt)):\n",
    "        ax.plot(data['date'],data[lt[i]],linestyle='-',label = lt[i])\n",
    "    plt.legend(prop = {'size':size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "extension_compare = pd.merge(data_30[['date','extension_n2v_logit_out_30_hedge']],\n",
    "        data_60[['date','extension_n2v_logit_out_60_hedge']],\n",
    "        how='left', on=['date'])\n",
    "extension_compare = pd.merge(extension_compare,\n",
    "        data_90[['date','extension_n2v_logit_out_90_hedge']],\n",
    "        how='left', on=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 18), dpi=80)\n",
    "plt.suptitle('Cumulative compound return for hedged portfolios',fontsize=30)\n",
    "\n",
    "ax = fig.add_subplot(2,2,1)\n",
    "plot_func('previous 30 days',data_30,\n",
    "          ['av500_2p_30_hedge','extension_n2v_logit_out_30_hedge','git1_30_hedge','git2_30_hedge'])\n",
    "\n",
    "ax = fig.add_subplot(2,2,2)\n",
    "plot_func('previous 60 days',data_60,\n",
    "          ['av500_2p_60_hedge','extension_n2v_logit_out_60_hedge','git1_30_hedge','git2_60_hedge'])\n",
    "\n",
    "ax = fig.add_subplot(2,2,3)\n",
    "plot_func('previous 90 days',data_90,\n",
    "          ['av500_2p_90_hedge','extension_n2v_logit_out_90_hedge','git1_90_hedge','git2_90_hedge'])\n",
    "\n",
    "ax = fig.add_subplot(2,2,4)\n",
    "plot_func('extension method previous days comparison',extension_compare,\n",
    "          ['extension_n2v_logit_out_30_hedge','extension_n2v_logit_out_60_hedge','extension_n2v_logit_out_90_hedge'])\n",
    "plt.savefig('/Users/evaking/desktop/NLP Project/pics/cumulative.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df1,df2,df3,df4,category = 'Sharpe Ratio'):\n",
    "    df_1 = pd.merge(df1[[category+'_extension_n2v_logit_out']],df2[[category+'_av500_2p']],\n",
    "         left_index = True, right_index = True)\n",
    "    df_2 = pd.merge(df3[[category+'_git1']],df4[[category+'_git2']],\n",
    "         left_index = True, right_index = True)\n",
    "    df = pd.merge(df_1,df_2,left_index = True, right_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df,category,day = 30):\n",
    "    df.plot(figsize=(12, 4),xticks=df.index.tolist())\n",
    "    tick_spacing = 1\n",
    "    plt.title(category+' of different portfolios ('+str(day)+'days)')\n",
    "    plt.xlabel('Group',fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "    plt.ylabel(category,fontdict={'family' : 'Times New Roman', 'size'   : 20})\n",
    "    plt.xticks(size = 15)\n",
    "    plt.yticks(size = 15)\n",
    "    plt.grid(linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.savefig('/Users/evaking/desktop/NLP Project/pics/'+category+str(day)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-mobility",
   "metadata": {},
   "source": [
    "#### Sharpe Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = plot_df(extension_w2v_logit_out_30,av500_2p_30,git1_30,git2_30)\n",
    "plot(df_sp,'Sharpe Raio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp_60 = plot_df(extension_w2v_logit_out_60,av500_2p_60,git1_60,git2_60)\n",
    "plot(df_sp_60,'Sharpe Raio',day = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp_90 = plot_df(extension_w2v_logit_out_90,av500_2p_90,git1_60,git2_90)\n",
    "plot(df_sp_90,'Sharpe Raio',day = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-notice",
   "metadata": {},
   "source": [
    "#### Annual Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = plot_df(extension_w2v_logit_out_30,av500_2p_30,git1_30,git2_30,category = 'annual_return')\n",
    "plot(df_r,'annual_return')\n",
    "# 60, 90 days are similar and omit here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-commerce",
   "metadata": {},
   "source": [
    "#### Annual Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_risk = plot_df(extension_w2v_logit_out_30,av500_2p_30,git1_30,git2_30,category = 'annual_risk')\n",
    "plot(df_risk,'annual_risk')\n",
    "# 60, 90 days are similar and omit here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-chrome",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Result Description\n",
    "\n",
    "**`method variation comparison`**\n",
    "1. *500* words for positive and negative word each is better than *100* words each\n",
    "2. adj + v is better than detailed exclusion\n",
    "3. word length >=*2* is better than not selecting word length (for SESTM method only)\n",
    "4. _500_ words each with a. and v. whose word length larger or equal to two performs best\n",
    "5. github fixed dictionary + SESTM method performs worst\n",
    "\n",
    "**`correlation result analysis`**\n",
    "\n",
    "The correlation varies as time goes by.\n",
    "- for the correlation between predicted score at time t and return label time *[t:t+1]*\n",
    "    - At the begining of the testing sample, the pearson correlation can be up to around *0.31*.\n",
    "    - While after around half a year, the pearson correlation sharply drop to around *0.08* in average for all methods and become stable afterwards.\n",
    "    - The spearman correlation follows the same trend, but with a slightly lower peak at the beginning (around *0.28*) and a slightly higher correlation when the trend is stable (around *0.09*).\n",
    "\n",
    "- for the correlation between predicted score at time t and return label time *[t+2:t+6]*\n",
    "    - Follow the same trend as *[t:t+1]*, but the correlation drops ealier.\n",
    "    - At the begining of the testing sample, the pearson correlation can also be up to around *0.31*.\n",
    "    - While in the following months, the pearson correlation gradually drop to around *0.01* in average for all methods and become stable afterwards.\n",
    "    - The spearman correlation follows the same trend, with a similar peak at the beginning (around *0.28*) but a relative lower correlation when the trend is stable (around *-0.01*).\n",
    "    \n",
    "**`algorithm extension`**    \n",
    "1. Comparison between using the whole training dataset and using only those with return outside range of *[-2%,2%]*\n",
    "- for pearson correlation\n",
    "    - Not much difference as in some periods the whole training dataset one is slightly higher while in other periods, the outside range one is slightly higher for all methods and all time window.\n",
    "\n",
    "- for spearman correlation\n",
    "    - In general, the correlation score of the outside range one is slightly lower than the whole one.\n",
    "\n",
    "2. ngram2vec method implementation / DSG method implementation\n",
    "- Both regression and classification methods are applied and logistic regression performs best\n",
    "    \n",
    "**`portfolio result analysis`**\n",
    "- Adopt four methods in total to test the correlation between previous *1-3* month average predicted sentiment score and the following one month return for each stock.\n",
    "- Four methods are as followed:\n",
    "    - n2v logistic regression threshold _30%_\n",
    "    - av_500_2p\n",
    "    - github link _1_ with score _6_ (dictionary based method)\n",
    "    - github link _2_ with score _6_ (dictionary based method)\n",
    "- Construct _10_ portfolios and one hedged portfolio (notated as portfolio _11_) based on the previous *1-3* month average predicted sentiment score rank\n",
    "    - The portfolio is rebalanced at each month.\n",
    "    - The higher the number of portfolio, the more positive for the sentiment score.\n",
    "    - Therefore, the hedged portfolio _11_ is constructed by (portfolio _10_ - portfolio _1_) at each rebalance date.\n",
    "\n",
    "1. The correlation results show that the word2vec method performs best among different methods and different previous time window.\n",
    "2. The sharpe ratio doesn't vary too much among the first _10_ portfolios for different methods and different previous time window with an average score at around _0.6_.\n",
    "3. However, the sharpe ratio all drops at the hedged portfolio for all different methods and different previous time window with varied values for different combination of method and time window.\n",
    "4. The trend of annual return is the same as sharpe ratio and the average annual return among the first _10_ portfolios is around _0.16_.\n",
    "5. Interestingly, the trend of annual risk is very different from the previous two. Among the first _10_ portfolios, the bigger the number of the portfolio, the higher the annual risk, which means the more positive sentiment of the stocks in blasket the higher the risk of the portfolio. While for hedged portfolio, since it adopts hedged strategy, the risk is supposed to be the lowest, and the result proves the expectation. The annual risk of the hedged portfolio is the lowest among all the _11_ portfolios.\n",
    "\n",
    "    \n",
    "## Result Conlusion\n",
    "1. The algorithm extension applying ngram2vec method performs best.\n",
    "2. The predicted score is most powerful in the very short period of time *[t:t+1]*, while not significant in the relative longer time window *[t+2:t+6]*\n",
    "3. The portfolio based on the sentiment score doesn't perform well which may make sense, since we are back-testing the portfolios on a monthly scale which far beyond the predict ability of the sentiment score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
