{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from downcast import reduce\n",
    "from numba import jit\n",
    "import time\n",
    "import gc\n",
    "from downcast import reduce\n",
    "import random\n",
    "import warnings\n",
    "from math import ceil\n",
    "from tqdm import trange\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e618f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large dataset, use 2 methods to save memory\n",
    "@jit\n",
    "def reduce_mem_usage(df, verbose=True, method='downcast'):\n",
    "    start = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if method == 'downcast':\n",
    "        df = reduce(df)\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)\n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction) and time is '.format(end_mem, 100 * (\n",
    "                    start_mem - end_mem) / start_mem) + str(time.time() - start))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_DAY = 846\n",
    "# use the latest 3 years data only\n",
    "\n",
    "remove_feature = ['id',\n",
    "                  'state_id',\n",
    "                  'store_id',\n",
    "#                   'item_id',\n",
    "                  'dept_id',\n",
    "                  'cat_id',\n",
    "                  'date','wm_yr_wk','d','sales']\n",
    "\n",
    "cat_var = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "cat_var = list(set(cat_var) - set(remove_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2_colnm = ['sell_price', 'price_max', 'price_min', 'price_std',\n",
    "               'price_mean', 'price_norm', 'price_nunique', 'item_nunique',\n",
    "               'price_momentum', 'price_momentum_m', 'price_momentum_y']\n",
    "\n",
    "grid3_colnm = ['event_name_1', 'event_type_1', 'event_name_2',\n",
    "               'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m',\n",
    "               'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end']\n",
    "\n",
    "lag_colnm = [ 'sales_lag_28', 'sales_lag_29', 'sales_lag_30',\n",
    "             'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34',\n",
    "             'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38',\n",
    "             'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42',\n",
    "             \n",
    "             'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14',\n",
    "             'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60',\n",
    "             'rolling_std_60']\n",
    "\n",
    "mean_enc_colnm = [\n",
    "    \n",
    "    'enc_store_id_dept_id_mean', 'enc_store_id_dept_id_std', \n",
    "    'enc_item_id_state_id_mean', 'enc_item_id_state_id_std',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "#################################################################################\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.015,\n",
    "                    'num_leaves': 2**8-1,\n",
    "                    'min_data_in_leaf': 2**8-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 3000,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = ['public']\n",
    "STORES = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n",
    "CATS = ['HOBBIES', 'HOUSEHOLD', 'FOODS']\n",
    "DEPTS = ['HOBBIES_1', 'HOBBIES_2', 'HOUSEHOLD_1', 'HOUSEHOLD_2', 'FOODS_1', 'FOODS_2', 'FOODS_3']\n",
    "validation = {\n",
    "    'cv' : [1885,1913],\n",
    "    'public' : [1913, 1941],\n",
    "    'private' : [1941, 1969]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "traindata = pd.read_pickle(\"E:/traindata.pkl\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Predict \n",
    "#################################################################################\n",
    "\n",
    "rmsse_bycv = dict()\n",
    "\n",
    "for cv in cvs:\n",
    "    print('cv : day', validation[cv])\n",
    "    \n",
    "    pred_list = []\n",
    "    for store in STORES:\n",
    "        for state in DEPTS:\n",
    "\n",
    "            print(store,state, 'start')\n",
    "            start = time.time()\n",
    "            grid_df = traindata[(traindata['store_id'] == store) & (traindata['dept_id'] == state)]\n",
    "\n",
    "            model_var = grid_df.columns[~grid_df.columns.isin(remove_feature)]\n",
    "\n",
    "            tr_mask = (grid_df['d'] <= validation[cv][0]) & (grid_df['d'] >= FIRST_DAY)\n",
    "            vl_mask = (grid_df['d'] > validation[cv][0]) & (grid_df['d'] <= validation[cv][1])\n",
    "            pr_mask = (grid_df['d'] > validation[cv][1])\n",
    "\n",
    "            train_data = lgb.Dataset(grid_df[tr_mask][model_var], \n",
    "                           label=grid_df[tr_mask]['sales'])\n",
    "\n",
    "            valid_data = lgb.Dataset(grid_df[vl_mask][model_var], \n",
    "                               label=grid_df[vl_mask]['sales'])\n",
    "\n",
    "            m_lgb = lgb.train(lgb_params, train_data, valid_sets = [valid_data, train_data], verbose_eval=50, \n",
    "                              num_boost_round = 1000, early_stopping_rounds = 50)\n",
    "            print('time: ' + str(time.time() - start))\n",
    "            model_name = 'non_recur_model_'+store+'_'+state+'.bin'\n",
    "            pickle.dump(m_lgb, open(model_name, 'wb'))\n",
    "            \n",
    "            indice_valid = grid_df[vl_mask].index.tolist()\n",
    "            indice_pred = grid_df[pr_mask].index.tolist()\n",
    "            prediction_valid = pd.DataFrame({'y_pred': m_lgb.predict(grid_df[vl_mask][model_var])})\n",
    "            prediction_pred = pd.DataFrame({'y_pred': m_lgb.predict(grid_df[pr_mask][model_var])})\n",
    "            prediction_valid.index = indice_valid\n",
    "            prediction_pred.index = indice_pred\n",
    "\n",
    "\n",
    "            del grid_df, train_data, valid_data, m_lgb, tr_mask, vl_mask\n",
    "            gc.collect()\n",
    "\n",
    "            grid_1 = pd.read_pickle('E:/grid_part_1.pkl')\n",
    "            pd.concat([grid_1.iloc[indice_valid], prediction_valid], axis=1).pivot(index='id', columns='d', values='y_pred').reset_index()\\\n",
    "            .set_index('id').to_csv(f'submission_storeandcat_{store}_{state}_{cv}_valid.csv')\n",
    "            pd.concat([grid_1.iloc[indice_pred], prediction_pred], axis=1).pivot(index='id', columns='d', values='y_pred') \\\n",
    "            .reset_index().set_index('id').to_csv(f'submission_onlystore_{store}_{cv}_pred.csv')\n",
    "\n",
    "\n",
    "            del grid_1\n",
    "            gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "submission_pred = pd.read_csv('D:/HKUST/MAFS6010Z AI in Fintech/project3/sample_submission.csv').set_index('id').iloc[30490:]\n",
    "submisssion_valid = pd.read_csv('D:/HKUST/MAFS6010Z AI in Fintech/project3/sample_submission.csv').set_index('id').iloc[:30490]\n",
    "sub_id_valid = pd.DataFrame({'id':submisssion_valid.index.tolist()})\n",
    "sub_id_pred = pd.DataFrame({'id':submission_pred.index.tolist()})\n",
    "\n",
    "pri = [a for a in os.listdir() if 'onlystore' in a]\n",
    "valid = [a for a in pri if 'valid' in a]\n",
    "pred = [a for a in pri if 'pred' in a]\n",
    "\n",
    "fcol = [f'F{i}' for i in range(1,29)]\n",
    "sub_copy = submisssion_valid.copy()\n",
    "for file in valid:\n",
    "    temp = pd.read_csv(file).iloc[:,:29]\n",
    "    temp.columns = ['id']+fcol\n",
    "    sub_copy += sub_id_valid.merge(temp, how='left', on='id').set_index('id').fillna(0)\n",
    "sub_copy.columns = fcol\n",
    "\n",
    "sub_copy1 = submission_pred.copy()\n",
    "for file in pred:\n",
    "    temp = pd.read_csv(file).iloc[:,:29]\n",
    "    temp.columns = ['id']+fcol\n",
    "    sub_copy1 += sub_id_pred.merge(temp, how='left', on='id').set_index('id').fillna(0)\n",
    "sub_copy1.columns = fcol\n",
    "\n",
    "df = pd.concat([sub_copy, sub_copy1])\n",
    "df.to_csv('E:/submission_nonrecursive_store_cat.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
