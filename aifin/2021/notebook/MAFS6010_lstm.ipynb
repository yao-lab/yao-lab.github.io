{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a character-based LSTM to generate sonnets\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.layers import LSTM,SimpleRNN\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename=\"shakespeare.txt\", seq_length=40, step=5):\n",
    "    '''\n",
    "    returns semi-redundant sequences their outputs \n",
    "    seq_length: number of characters in each sequence\n",
    "    step: gets every [step] sequence  \n",
    "    '''\n",
    "\n",
    "    # puts all data into text string  \n",
    "    file = open(filename, \"r\")\n",
    "    text = \"\"\n",
    "    for line in file:\n",
    "        line = line.lstrip(' ').rstrip(' ')\n",
    "        if line != '\\n' and not line[0].isdigit():\n",
    "            line.translate(str.maketrans('', '', string.punctuation))\n",
    "            text += line.lower()\n",
    "\n",
    "    # make char to index and index to char dictionary \n",
    "    characters = sorted(list(set(text)))\n",
    "    char_indices_dict = dict((c, i) for i, c in enumerate(characters))\n",
    "    indices_char_dict = dict((i, c) for i, c in enumerate(characters))\n",
    "    #print(char_indices_dict)\n",
    "\n",
    "    # makes every [step] char sequences of length seq_length and their outputs\n",
    "    sequences = []\n",
    "    next_chars = [] # next char that seq in sequences generates\n",
    "    #print(repr(text[len(text) - 200:]))\n",
    "    for i in range(0, len(text) - seq_length, step):\n",
    "        #print(i, seq, text[i : i + seq_length])\n",
    "        sequences.append(text[i : i + seq_length])\n",
    "        next_chars.append(text[i + seq_length])\n",
    "\n",
    "    # put sequences and outputs into np array\n",
    "    x = np.zeros((len(sequences), seq_length, len(characters)))\n",
    "    y = np.zeros((len(sequences), len(characters)), dtype=np.bool)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for t, char in enumerate(sequence):\n",
    "            x[i, t, char_indices_dict[char]] = 1\n",
    "        y[i, char_indices_dict[next_chars[i]]] = 1\n",
    "\n",
    "    return x, y, sequences, indices_char_dict, char_indices_dict, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(temperature=1.0):\n",
    "    x, y, sequences, indices_char_dict, char_indices_dict, text = preprocess()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100))\n",
    "    # add temperature (controls variance)\n",
    "    model.add(Lambda(lambda x: x / temperature))\n",
    "    model.add(Dense(len(indices_char_dict), activation='softmax'))  \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor='loss', patience=3, verbose=1, mode='auto')\n",
    "    model.fit(x, y, epochs=50, verbose=1, callbacks=[earlyStopping])\n",
    "    model.save('lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sonnet():\n",
    "    x, y, sequences, indices_char_dict, char_indices_dict, text = preprocess()\n",
    "\n",
    "    model = load_model('lstm.h5')\n",
    "    sonnet = []\n",
    "    #f = open('output.txt', 'a')\n",
    "\n",
    "    seq = \"shall i compare thee to a summer's day?\\n\"\n",
    "    sonnet.append(seq)\n",
    "    for _ in range(13):\n",
    "        line = \"\"\n",
    "        for i in range(40):\n",
    "            x = np.zeros((1, len(seq), len(indices_char_dict)))\n",
    "            for t, index in enumerate(seq):\n",
    "                x[0, t, char_indices_dict[index]] = 1.\n",
    "\n",
    "            prediction = model.predict(x, verbose=0)[0]\n",
    "            index = np.argmax(prediction)\n",
    "            char = indices_char_dict[index]\n",
    "            line += char\n",
    "            seq = seq[1:] + char\n",
    "\n",
    "        sonnet.append(line)\n",
    "    return sonnet\n",
    "\n",
    "    #for line in sonnet:\n",
    "        #print(line)\n",
    "        #f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "586/586 [==============================] - 6s 7ms/step - loss: 2.8068 - accuracy: 0.2218\n",
      "Epoch 2/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 2.2331 - accuracy: 0.3471\n",
      "Epoch 3/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 2.0911 - accuracy: 0.3785\n",
      "Epoch 4/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.9816 - accuracy: 0.4100\n",
      "Epoch 5/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.8943 - accuracy: 0.4286\n",
      "Epoch 6/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.8083 - accuracy: 0.4557\n",
      "Epoch 7/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.7465 - accuracy: 0.4616\n",
      "Epoch 8/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.6640 - accuracy: 0.4886\n",
      "Epoch 9/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.5874 - accuracy: 0.5057\n",
      "Epoch 10/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.4875 - accuracy: 0.5337\n",
      "Epoch 11/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.3682 - accuracy: 0.5783\n",
      "Epoch 12/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.2696 - accuracy: 0.6066\n",
      "Epoch 13/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.1353 - accuracy: 0.6437\n",
      "Epoch 14/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 1.0325 - accuracy: 0.6848\n",
      "Epoch 15/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.9054 - accuracy: 0.7241\n",
      "Epoch 16/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.7651 - accuracy: 0.7695\n",
      "Epoch 17/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.6495 - accuracy: 0.8083\n",
      "Epoch 18/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.5379 - accuracy: 0.8460\n",
      "Epoch 19/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.4409 - accuracy: 0.8762\n",
      "Epoch 20/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.3491 - accuracy: 0.9074\n",
      "Epoch 21/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.2885 - accuracy: 0.9273\n",
      "Epoch 22/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.2304 - accuracy: 0.9450\n",
      "Epoch 23/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.1854 - accuracy: 0.9583\n",
      "Epoch 24/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.1655 - accuracy: 0.9622\n",
      "Epoch 25/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.1387 - accuracy: 0.9689\n",
      "Epoch 26/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.1211 - accuracy: 0.9716\n",
      "Epoch 27/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.1136 - accuracy: 0.9746\n",
      "Epoch 28/50\n",
      "586/586 [==============================] - 3s 6ms/step - loss: 0.0978 - accuracy: 0.9772\n",
      "Epoch 29/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0933 - accuracy: 0.9791\n",
      "Epoch 30/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.0831 - accuracy: 0.9813\n",
      "Epoch 31/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.0805 - accuracy: 0.9824\n",
      "Epoch 32/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.0722 - accuracy: 0.9832\n",
      "Epoch 33/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0772 - accuracy: 0.9810\n",
      "Epoch 34/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.0726 - accuracy: 0.9838\n",
      "Epoch 35/50\n",
      "586/586 [==============================] - 3s 6ms/step - loss: 0.0631 - accuracy: 0.9847\n",
      "Epoch 36/50\n",
      "586/586 [==============================] - 3s 6ms/step - loss: 0.0630 - accuracy: 0.9840\n",
      "Epoch 37/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0600 - accuracy: 0.9862\n",
      "Epoch 38/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0558 - accuracy: 0.9863\n",
      "Epoch 39/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.0521 - accuracy: 0.9869\n",
      "Epoch 40/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.0509 - accuracy: 0.9877\n",
      "Epoch 41/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0496 - accuracy: 0.9882\n",
      "Epoch 42/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0490 - accuracy: 0.9878\n",
      "Epoch 43/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0521 - accuracy: 0.9855\n",
      "Epoch 44/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.0427 - accuracy: 0.9909\n",
      "Epoch 45/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0448 - accuracy: 0.9896\n",
      "Epoch 46/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0435 - accuracy: 0.9895\n",
      "Epoch 47/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0415 - accuracy: 0.9898\n",
      "Epoch 48/50\n",
      "586/586 [==============================] - 4s 7ms/step - loss: 0.0384 - accuracy: 0.9905\n",
      "Epoch 49/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0401 - accuracy: 0.9901\n",
      "Epoch 50/50\n",
      "586/586 [==============================] - 4s 6ms/step - loss: 0.0384 - accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "make_model(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "\n",
      "when food my sell be thy have my self fr\n",
      "oungentoon anoo\n",
      "hhat thy self thou art c\n",
      "anse, in hairechouss,\n",
      "by thiummere i for\n",
      " thee have not skee sgone,\n",
      "and hearting \n",
      "woth mise rine, you a toon.\n",
      "and out bros\n",
      "e sto me dead bearty wid of thee,\n",
      "and th\n",
      "eref ind, that thy self thoughts mise,\n",
      "w\n",
      "hen i ill my self thee may the may,\n",
      "but \n",
      "this swill eme sande me the reioll sake,\n",
      "\n",
      "the efen weth frienting mawot is thing \n",
      "onptess,\n",
      "the analowed mud i not seef ing\n",
      "warth dess to live,\n",
      "yet ded lime the lov\n",
      "el make the thow whore thee still,\n",
      "whe e\n"
     ]
    }
   ],
   "source": [
    "sonnet=generate_sonnet()\n",
    "for line in sonnet:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "772508630a427619dd49b8cfe9c2ae7d9dbaeae5c7cd5fb2ab9db30424e7636a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('python36': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
