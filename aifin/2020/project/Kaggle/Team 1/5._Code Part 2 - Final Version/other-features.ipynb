{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os, sys, gc, warnings, psutil, random\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2960025 entries, 0 to 2960024\n",
      "Data columns (total 34 columns):\n",
      "id                  category\n",
      "item_id             category\n",
      "dept_id             category\n",
      "cat_id              category\n",
      "store_id            category\n",
      "state_id            category\n",
      "d                   int16\n",
      "sales               float64\n",
      "release             int16\n",
      "sell_price          float16\n",
      "price_max           float16\n",
      "price_min           float16\n",
      "price_std           float16\n",
      "price_mean          float16\n",
      "price_norm          float16\n",
      "price_nunique       float16\n",
      "item_nunique        int16\n",
      "price_momentum      float16\n",
      "price_momentum_m    float16\n",
      "price_momentum_y    float16\n",
      "event_name_1        category\n",
      "event_type_1        category\n",
      "event_name_2        category\n",
      "event_type_2        category\n",
      "snap_CA             category\n",
      "snap_TX             category\n",
      "snap_WI             category\n",
      "tm_d                int8\n",
      "tm_w                int8\n",
      "tm_m                int8\n",
      "tm_y                int8\n",
      "tm_wm               int8\n",
      "tm_dw               int8\n",
      "tm_w_end            int8\n",
      "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
      "memory usage: 159.7 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_df = pd.concat([pd.read_pickle('../input/m5-simple-fe/grid_part_1.pkl'),\n",
    "                     pd.read_pickle('../input/m5-simple-fe/grid_part_2.pkl').iloc[:,2:],\n",
    "                     pd.read_pickle('../input/m5-simple-fe/grid_part_3.pkl').iloc[:,2:]],\n",
    "                     axis=1)\n",
    "\n",
    "# \n",
    "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
    "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
    "\n",
    "# Let's \"inspect\" our grid DataFrame\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[318]\ttraining's rmse: 2.82659\tvalid_1's rmse: 2.38847\n"
     ]
    }
   ],
   "source": [
    "########################### Baseline model\n",
    "#################################################################################\n",
    "\n",
    "# We will need some global VARS for future\n",
    "\n",
    "SEED = 42             # Our random seed for everything\n",
    "random.seed(SEED)     # to make all tests \"deterministic\"\n",
    "np.random.seed(SEED)\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "TARGET = 'sales'      # Our Target\n",
    "END_TRAIN = 1913      # And we will use last 28 days as validation\n",
    "\n",
    "# Drop some items from \"TEST\" set part (1914...)\n",
    "grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n",
    "\n",
    "# Features that we want to exclude from training\n",
    "remove_features = ['id','d',TARGET]\n",
    "\n",
    "# Our baseline model serves\n",
    "# to do fast checks of\n",
    "# new features performance \n",
    "\n",
    "# We will use LightGBM for our tests\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
    "                    'objective': 'regression',       # Standart loss for RMSE\n",
    "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
    "                    'subsample': 0.8,                \n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
    "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
    "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
    "                    'feature_fraction': 0.8,\n",
    "                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n",
    "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
    "                    'seed': SEED,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "## RMSE\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
    "\n",
    "# Small function to make fast features tests\n",
    "# estimator = make_fast_test(grid_df)\n",
    "# it will return lgb booster for future analisys\n",
    "def make_fast_test(df):\n",
    "\n",
    "    features_columns = [col for col in list(df) if col not in remove_features]\n",
    "\n",
    "    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]              \n",
    "    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n",
    "    \n",
    "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
    "    \n",
    "    estimator = lgb.train(\n",
    "                            lgb_params,\n",
    "                            train_data,\n",
    "                            valid_sets = [train_data,valid_data],\n",
    "                            verbose_eval = 500,\n",
    "                        )\n",
    "    \n",
    "    return estimator\n",
    "\n",
    "# Make baseline model\n",
    "baseline_model = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[322]\ttraining's rmse: 2.56871\tvalid_1's rmse: 2.26668\n"
     ]
    }
   ],
   "source": [
    "     \n",
    "########################### Some more info about lags here:\n",
    "\n",
    "\n",
    "# Small helper to make lags creation faster\n",
    "from multiprocessing import Pool                # Multiprocess Runs\n",
    "\n",
    "## Multiprocessing Run.\n",
    "# :t_split - int of lags days                   # type: int\n",
    "# :func - Function to apply on each split       # type: python function\n",
    "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def make_normal_lag(lag_day):\n",
    "    lag_df = grid_df[['id','d',TARGET]] # not good to use df from \"global space\"\n",
    "    col_name = 'sales_lag_'+str(lag_day)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "# Launch parallel lag creation\n",
    "# and \"append\" to our grid\n",
    "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standart RMSE 2.266677619311552\n",
      "release 0.0\n",
      "sell_price 0.0031\n",
      "price_max 0.0002\n",
      "price_min -0.0002\n",
      "price_std 0.0062\n",
      "price_mean 0.003\n",
      "price_norm 0.0129\n",
      "price_nunique -0.0018\n",
      "item_nunique 0.003\n",
      "price_momentum 0.0003\n",
      "price_momentum_m 0.0094\n",
      "price_momentum_y 0.0007\n",
      "tm_d 0.0044\n",
      "tm_w 0.0001\n",
      "tm_m 0.0009\n",
      "tm_y 0.0\n",
      "tm_wm 0.0\n",
      "tm_dw 0.1058\n",
      "tm_w_end 0.0094\n",
      "sales_lag_1 0.4503\n",
      "sales_lag_2 0.0209\n",
      "sales_lag_3 0.0067\n",
      "sales_lag_4 0.013\n",
      "sales_lag_5 0.0147\n",
      "sales_lag_6 0.015\n",
      "sales_lag_7 0.0272\n"
     ]
    }
   ],
   "source": [
    "########################### Permutation importance Test\n",
    "\n",
    "# Let's creat validation dataset and features\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "\n",
    "# Make normal prediction with our model and save score\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "\n",
    "# Now we are looping over all our numerical features\n",
    "for col in features_columns:\n",
    "    \n",
    "    # We will make validation set copy to restore\n",
    "    # features states on each run\n",
    "    temp_df = validation_df.copy()\n",
    "    \n",
    "    # Error here appears if we have \"categorical\" features and can't \n",
    "    # do np.random.permutation without disrupt categories\n",
    "    # so we need to check if feature is numerical\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        \n",
    "        # If our current rmse score is less than base score\n",
    "        # it means that feature most probably is a bad one\n",
    "        # and our model is learning on noise\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "# Remove Temp data\n",
    "del temp_df, validation_df\n",
    "\n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from eli5 documentation (seems it's perfect explanation)\n",
    "\n",
    "The idea is the following: feature importance can be measured by looking at how much the score (accuracy, mse, rmse, mae, etc. - any score we’re interested in) decreases when a feature is not available.\n",
    "\n",
    "To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. Also, it shows what may be important within a dataset, not what is important within a concrete trained model.\n",
    "\n",
    "To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can **replace it with random noise** - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the **same distribution as original feature values** (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
    "\n",
    "---\n",
    "\n",
    "It's not good when feature remove (replaced by noise) but we have better score. Simple and easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[359]\ttraining's rmse: 2.80592\tvalid_1's rmse: 2.39371\n",
      "Standart RMSE 2.393708315090793\n",
      "release 0.0\n",
      "sell_price 0.0036\n",
      "price_max 0.0055\n",
      "price_min 0.0001\n",
      "price_std 0.0064\n",
      "price_mean 0.0028\n",
      "price_norm 0.012\n",
      "price_nunique 0.0188\n",
      "item_nunique 0.0126\n",
      "price_momentum 0.0002\n",
      "price_momentum_m 0.0285\n",
      "price_momentum_y 0.0087\n",
      "tm_d 0.0028\n",
      "tm_w 0.0016\n",
      "tm_m 0.0015\n",
      "tm_y 0.0\n",
      "tm_wm 0.0002\n",
      "tm_dw 0.1056\n",
      "tm_w_end 0.0134\n",
      "sales_lag_56 0.0125\n",
      "sales_lag_57 0.0059\n",
      "sales_lag_58 0.0117\n",
      "sales_lag_59 0.002\n",
      "sales_lag_60 0.0034\n",
      "sales_lag_61 -0.0018\n",
      "sales_lag_62 -0.0011\n"
     ]
    }
   ],
   "source": [
    "########################### Lets test far away Lags (7 days with 56 days shift)\n",
    "########################### and check permutation importance\n",
    "#################################################################################\n",
    "\n",
    "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
    "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
    "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
    "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
    "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
    "print('Standart RMSE', base_score)\n",
    "\n",
    "for col in features_columns:\n",
    "    temp_df = validation_df.copy()\n",
    "    if temp_df[col].dtypes.name != 'category':\n",
    "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
    "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
    "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
    "        print(col, np.round(cur_score - base_score, 4))\n",
    "\n",
    "del temp_df, validation_df\n",
    "        \n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "\n",
    "# Results:\n",
    "## Lags with 56 days shift (far away past) are not as important\n",
    "## as nearest past lags\n",
    "## and at some point will be just noise for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA: id 7\n",
      "[0.72243389 0.06622603 0.05933126 0.04200092 0.0388851  0.03610057\n",
      " 0.03502223]\n",
      "Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[243]\ttraining's rmse: 2.66922\tvalid_1's rmse: 2.27719\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def make_pca(df, pca_col, n_days):\n",
    "    print('PCA:', pca_col, n_days)\n",
    "    \n",
    "    # We don't need any other columns to make pca\n",
    "    pca_df = df[[pca_col,'d',TARGET]]\n",
    "    \n",
    "    # If we are doing pca for other series \"levels\" \n",
    "    # we need to agg first\n",
    "    if pca_col != 'id':\n",
    "        merge_base = pca_df[[pca_col,'d']]\n",
    "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
    "        pca_df[TARGET] = pca_df['sum']\n",
    "        del pca_df['sum']\n",
    "    \n",
    "    # Min/Max scaling\n",
    "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
    "    \n",
    "    # Making \"lag\" in old way (not parallel)\n",
    "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
    "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
    "    pca_df = pca_df.assign(**{\n",
    "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
    "            for l in LAG_DAYS\n",
    "            for col in [TARGET]\n",
    "        })\n",
    "    \n",
    "    pca_columns = list(pca_df)[3:]\n",
    "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
    "    pca = PCA(random_state=SEED)\n",
    "    \n",
    "    # You can use fit_transform here\n",
    "    pca.fit(pca_df[pca_columns])\n",
    "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
    "    \n",
    "    print(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # we will keep only 3 most \"valuable\" columns/dimensions \n",
    "    keep_cols = pca_columns[:3]\n",
    "    print('Columns to keep:', keep_cols)\n",
    "    \n",
    "    # If we are doing pca for other series \"levels\"\n",
    "    # we need merge back our results to merge_base df\n",
    "    # and only than return resulted df\n",
    "    # I'll skip that step here\n",
    "    \n",
    "    return pca_df[keep_cols]\n",
    "\n",
    "\n",
    "# Make PCA\n",
    "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "# As we will compare performance with baseline model for now\n",
    "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding item_id\n",
      "Encoding cat_id\n",
      "Encoding dept_id\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[399]\ttraining's rmse: 2.77878\tvalid_1's rmse: 2.37865\n"
     ]
    }
   ],
   "source": [
    "########################### Mean/std target encoding\n",
    "#################################################################################\n",
    "\n",
    "# We will use these three columns for test\n",
    "# (in combination with store_id)\n",
    "icols = ['item_id','cat_id','dept_id']\n",
    "\n",
    "# But we can use any other column or even multiple groups\n",
    "# like these ones\n",
    "#            'state_id',\n",
    "#            'store_id',\n",
    "#            'cat_id',\n",
    "#            'dept_id',\n",
    "#            ['state_id', 'cat_id'],\n",
    "#            ['state_id', 'dept_id'],\n",
    "#            ['store_id', 'cat_id'],\n",
    "#            ['store_id', 'dept_id'],\n",
    "#            'item_id',\n",
    "#            ['item_id', 'state_id'],\n",
    "#            ['item_id', 'store_id']\n",
    "\n",
    "# There are several ways to do \"mean\" encoding\n",
    "## K-fold scheme\n",
    "## LOO (leave one out)\n",
    "## Smoothed/regularized \n",
    "## Expanding mean\n",
    "## etc \n",
    "\n",
    "# You can test as many options as you want\n",
    "# and decide what to use\n",
    "# Because of memory issues you can't \n",
    "# use many features.\n",
    "\n",
    "# We will use simple target encoding\n",
    "# by std and mean agg\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    temp_df = grid_df[grid_df['d']<=(1913-28)] # to be sure we don't have leakage in our validation set\n",
    "    \n",
    "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
    "    joiner = '_'+col+'_encoding_'\n",
    "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
    "    temp_df = temp_df.reset_index()\n",
    "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
    "    del temp_df\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
    "grid_df = grid_df[keep_cols]\n",
    "\n",
    "# Bad thing that for some items  \n",
    "# we are using past and future values.\n",
    "# But we are looking for \"categorical\" similiarity\n",
    "# on a \"long run\". So future here is not a big problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds\n",
      "[500]\ttraining's rmse: 2.63326\tvalid_1's rmse: 2.2845\n",
      "Early stopping, best iteration is:\n",
      "[525]\ttraining's rmse: 2.62736\tvalid_1's rmse: 2.2825\n"
     ]
    }
   ],
   "source": [
    "########################### Last non O sale\n",
    "#################################################################################\n",
    "\n",
    "def find_last_sale(df,n_day):\n",
    "    \n",
    "    # Limit initial df\n",
    "    ls_df = df[['id','d',TARGET]]\n",
    "    \n",
    "    # Convert target to binary\n",
    "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
    "    \n",
    "    # Make lags to prevent any leakage\n",
    "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
    "\n",
    "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
    "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
    "\n",
    "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
    "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
    "\n",
    "    return ls_df[['last_sale']]\n",
    "\n",
    "\n",
    "# Find last non zero\n",
    "# Need some \"dances\" to fit in memory limit with groupers\n",
    "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
    "\n",
    "# Make features test\n",
    "test_model = make_fast_test(grid_df)\n",
    "\n",
    "# Remove test features\n",
    "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
    "grid_df = grid_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding ['state_id']\n",
      "Encoding ['store_id']\n",
      "Encoding ['cat_id']\n",
      "Encoding ['dept_id']\n",
      "Encoding ['state_id', 'cat_id']\n",
      "Encoding ['state_id', 'dept_id']\n",
      "Encoding ['store_id', 'cat_id']\n",
      "Encoding ['store_id', 'dept_id']\n",
      "Encoding ['item_id']\n",
      "Encoding ['item_id', 'state_id']\n",
      "Encoding ['item_id', 'store_id']\n"
     ]
    }
   ],
   "source": [
    "########################### Apply on grid_df\n",
    "#################################################################################\n",
    "# lets read grid from \n",
    "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "# to be sure that our grids are aligned by index\n",
    "grid_df = pd.read_pickle('../input/m5-simple-fe/grid_part_1.pkl')\n",
    "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
    "base_cols = list(grid_df)\n",
    "\n",
    "icols =  [\n",
    "            ['state_id'],\n",
    "            ['store_id'],\n",
    "            ['cat_id'],\n",
    "            ['dept_id'],\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            ['item_id'],\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "            ]\n",
    "\n",
    "for col in icols:\n",
    "    print('Encoding', col)\n",
    "    col_name = '_'+'_'.join(col)+'_'\n",
    "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
    "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
    "\n",
    "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
    "grid_df = grid_df[['id','d']+keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Mean/Std encoding\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "print('Save Mean/Std encoding')\n",
    "grid_df.to_pickle('mean_encoding_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46881677 entries, 0 to 46881676\n",
      "Data columns (total 24 columns):\n",
      "id                           category\n",
      "d                            int16\n",
      "enc_state_id_mean            float16\n",
      "enc_state_id_std             float16\n",
      "enc_store_id_mean            float16\n",
      "enc_store_id_std             float16\n",
      "enc_cat_id_mean              float16\n",
      "enc_cat_id_std               float16\n",
      "enc_dept_id_mean             float16\n",
      "enc_dept_id_std              float16\n",
      "enc_state_id_cat_id_mean     float16\n",
      "enc_state_id_cat_id_std      float16\n",
      "enc_state_id_dept_id_mean    float16\n",
      "enc_state_id_dept_id_std     float16\n",
      "enc_store_id_cat_id_mean     float16\n",
      "enc_store_id_cat_id_std      float16\n",
      "enc_store_id_dept_id_mean    float16\n",
      "enc_store_id_dept_id_std     float16\n",
      "enc_item_id_mean             float16\n",
      "enc_item_id_std              float16\n",
      "enc_item_id_state_id_mean    float16\n",
      "enc_item_id_state_id_std     float16\n",
      "enc_item_id_store_id_mean    float16\n",
      "enc_item_id_store_id_std     float16\n",
      "dtypes: category(1), float16(22), int16(1)\n",
      "memory usage: 2.1 GB\n"
     ]
    }
   ],
   "source": [
    "########################### Final list of new features\n",
    "#################################################################################\n",
    "grid_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
