{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-based LSTM for generating Shakespeare's poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# implementation of a character-based LSTM to generate sonnets\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import LambdaCallback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename=\"shakespeare.txt\", seq_length=40, step=5):\n",
    "    '''\n",
    "    returns semi-redundant sequences their outputs \n",
    "    seq_length: number of characters in each sequence\n",
    "    step: gets every [step] sequence  \n",
    "    '''\n",
    "\n",
    "    # puts all data into text string  \n",
    "    file = open(filename, \"r\")\n",
    "    text = \"\"\n",
    "    for line in file:\n",
    "        line = line.lstrip(' ').rstrip(' ')\n",
    "        if line != '\\n' and not line[0].isdigit():\n",
    "            line.translate(str.maketrans('', '', string.punctuation))\n",
    "            text += line.lower()\n",
    "\n",
    "    # make char to index and index to char dictionary \n",
    "    characters = sorted(list(set(text)))\n",
    "    char_indices_dict = dict((c, i) for i, c in enumerate(characters))\n",
    "    indices_char_dict = dict((i, c) for i, c in enumerate(characters))\n",
    "    #print(char_indices_dict)\n",
    "\n",
    "    # makes every [step] char sequences of length seq_length and their outputs\n",
    "    sequences = []\n",
    "    next_chars = [] # next char that seq in sequences generates\n",
    "    #print(repr(text[len(text) - 200:]))\n",
    "    for i in range(0, len(text) - seq_length, step):\n",
    "        #print(i, seq, text[i : i + seq_length])\n",
    "        sequences.append(text[i : i + seq_length])\n",
    "        next_chars.append(text[i + seq_length])\n",
    "\n",
    "    # put sequences and outputs into np array\n",
    "    x = np.zeros((len(sequences), seq_length, len(characters)))\n",
    "    y = np.zeros((len(sequences), len(characters)), dtype=np.bool)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for t, char in enumerate(sequence):\n",
    "            x[i, t, char_indices_dict[char]] = 1\n",
    "        y[i, char_indices_dict[next_chars[i]]] = 1\n",
    "\n",
    "    return x, y, sequences, indices_char_dict, char_indices_dict, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(temperature=1.0):\n",
    "    x, y, sequences, indices_char_dict, char_indices_dict, text = preprocess()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200))\n",
    "    # add temperature (controls variance)\n",
    "    model.add(Lambda(lambda x: x / temperature))\n",
    "    model.add(Dense(len(indices_char_dict), activation='softmax'))  \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    earlyStopping = EarlyStopping(monitor='loss', patience=3, verbose=1, mode='auto')\n",
    "    model.fit(x, y, epochs=40, verbose=1, callbacks=[earlyStopping])\n",
    "    model.save('lstm.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sonnet():\n",
    "    x, y, sequences, indices_char_dict, char_indices_dict, text = preprocess()\n",
    "\n",
    "    model = load_model('lstm.h5')\n",
    "    sonnet = []\n",
    "    #f = open('output.txt', 'a')\n",
    "\n",
    "    seq = \"shall i compare thee to a summer's day?\\n\"\n",
    "    for _ in range(13):\n",
    "        line = \"\"\n",
    "        for i in range(40):\n",
    "            x = np.zeros((1, len(seq), len(indices_char_dict)))\n",
    "            for t, index in enumerate(seq):\n",
    "                x[0, t, char_indices_dict[index]] = 1.\n",
    "\n",
    "            prediction = model.predict(x, verbose=0)[0]\n",
    "            index = np.argmax(prediction)\n",
    "            char = indices_char_dict[index]\n",
    "            line += char\n",
    "            seq = seq[1:] + char\n",
    "\n",
    "        sonnet.append(line)\n",
    "    return sonnet\n",
    "\n",
    "    #for line in sonnet:\n",
    "        #print(line)\n",
    "        #f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "18727/18727 [==============================] - 25s 1ms/step - loss: 2.5891 - accuracy: 0.2719\n",
      "Epoch 2/40\n",
      "18727/18727 [==============================] - 25s 1ms/step - loss: 2.2131 - accuracy: 0.3500\n",
      "Epoch 3/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 2.0807 - accuracy: 0.3835\n",
      "Epoch 4/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.9781 - accuracy: 0.4095\n",
      "Epoch 5/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.8961 - accuracy: 0.4309\n",
      "Epoch 6/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.8218 - accuracy: 0.4473\n",
      "Epoch 7/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.7470 - accuracy: 0.4685\n",
      "Epoch 8/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.6737 - accuracy: 0.4846\n",
      "Epoch 9/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.5903 - accuracy: 0.5072\n",
      "Epoch 10/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.4997 - accuracy: 0.5330\n",
      "Epoch 11/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.4017 - accuracy: 0.5608\n",
      "Epoch 12/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.2906 - accuracy: 0.5960\n",
      "Epoch 13/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.1746 - accuracy: 0.6321\n",
      "Epoch 14/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 1.0470 - accuracy: 0.6747\n",
      "Epoch 15/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.9224 - accuracy: 0.7147\n",
      "Epoch 16/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.7976 - accuracy: 0.7551\n",
      "Epoch 17/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.6817 - accuracy: 0.7968\n",
      "Epoch 18/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.5742 - accuracy: 0.8299\n",
      "Epoch 19/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.4754 - accuracy: 0.8630\n",
      "Epoch 20/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.3937 - accuracy: 0.8888\n",
      "Epoch 21/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.3221 - accuracy: 0.9133\n",
      "Epoch 22/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.2689 - accuracy: 0.9290\n",
      "Epoch 23/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.2316 - accuracy: 0.9425\n",
      "Epoch 24/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.1904 - accuracy: 0.9538\n",
      "Epoch 25/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.1679 - accuracy: 0.9594\n",
      "Epoch 26/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.1483 - accuracy: 0.9662\n",
      "Epoch 27/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.1360 - accuracy: 0.9679\n",
      "Epoch 28/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.1237 - accuracy: 0.9698\n",
      "Epoch 29/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.1106 - accuracy: 0.9750\n",
      "Epoch 30/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.1052 - accuracy: 0.9751\n",
      "Epoch 31/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0980 - accuracy: 0.9771\n",
      "Epoch 32/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0964 - accuracy: 0.9776\n",
      "Epoch 33/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0875 - accuracy: 0.9784\n",
      "Epoch 34/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0815 - accuracy: 0.9801\n",
      "Epoch 35/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0760 - accuracy: 0.9822\n",
      "Epoch 36/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0772 - accuracy: 0.9804\n",
      "Epoch 37/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0688 - accuracy: 0.9842\n",
      "Epoch 38/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0748 - accuracy: 0.9813\n",
      "Epoch 39/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0668 - accuracy: 0.9836\n",
      "Epoch 40/40\n",
      "18727/18727 [==============================] - 24s 1ms/step - loss: 0.0678 - accuracy: 0.9837\n"
     ]
    }
   ],
   "source": [
    "make_model(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whe eass not shall sick mend for somend.\n",
      "\n",
      "why owh the lover that a pavw oo to bel\n",
      "oss dintyhind hime.\n",
      "in me thy love, that\n",
      " thou deadoure of live,\n",
      "that dead like t\n",
      "o a bow a vearthed nage:\n",
      "but no tite eye\n",
      ", and the eftlle a how no eam,\n",
      "and love \n",
      "is the bool, hive is sught beart,\n",
      "and th\n",
      "e fill a dad theun thou sholl res rerigh\n",
      "t,\n",
      "is mise mounie, my forsung of his wil\n",
      "l thou be thought,\n",
      "the will in hearty wi\n",
      "th foignad pare but bairy\n",
      "or pating case\n",
      " biig that streng have the.\n",
      "whech fairt \n",
      "not the fostrrcnommanon spll ddest see,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sonnet=generate_sonnet()\n",
    "for line in sonnet:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
