\documentclass[11pt]{article}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}

\def\N{{\mathbb N}}
\def\NN{{\mathcal N}}
\def\R{{\mathbb R}}
\def\E{{\mathbb E}}
\def\tr{{\mathrm{trace}}}
\def\P{{\mathrm{Prob}}}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0.25 in}
\setlength{\parskip}{0.1 in}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \setcounter{section}{0}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf A Mathematical Introduction to Data Science \hfill #4} }
       \vspace{6mm}
       \hbox to 6.28in { {\Large \hfill #1  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\it Instructor: #2\hfill #3} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}


\begin{document}

\lecture{Homework 2. Random Projections}{Yuan Yao}{Due: Open Date}{Sep. 15, 2017}

The problem below marked by $^*$ is optional with bonus credits. % For the experimental problem, include the source codes which are runnable under standard settings. 

\begin{enumerate}


\item {\em SNPs of World-wide Populations:} This dataset contains a data matrix $X\in \R^{n\times p}$ of about $p=650,000$ columns of SNPs (Single Nucleid Polymorphisms) and $n=1064$ rows of peoples around the world (but there are 21 rows mostly with missing values). Each element is of three choices, $0$ (for `AA'), $1$ (for `AC'), $2$ (for `CC'), and some missing values marked by $9$. 

\url{http://math.stanford.edu/~yuany/course/ceph_hgdp_minor_code_XNA.txt.zip}

which is big (151MB in zip and 2GB original txt). Moreover, the following file contains the region where each people comes from, as well as two variables {\texttt{ind1}} and{\texttt{ind2}} such that $X({\texttt{ind1}},{\texttt{ind2}})$ removes all missing values. 

\url{http://www.math.pku.edu.cn/teachers/yaoy/data/HGDP_region.mat}

A good reference for this data can be the following paper in Science, 

\url{http://www.sciencemag.org/content/319/5866/1100.abstract}

Explore the genetic variation of those persons with their geographic variations, by MDS/PCA. Since $p$ is big, explore random projections for dimensionality reduction.  

\item {\em Phase Transition in Compressed Sensing:} Let $A\in \R^{n\times d}$ be a Gaussian random matrix, \emph{i.e.} $A_{ij} \sim \NN(0,1)$. In the following experiments, fix $d=20$. For each $n=1,\ldots,d$, and each $k=1,\ldots, d$, repeat the following procedure 50 times:

\begin{enumerate}
\item Construct a sparse vector $x_0\in\R^d$ with $k$ nonzero entries. The locations of the nonzero entries are selected at random and each nonzero equals $\pm 1$ with equal probability;
\item Draw a standard Gaussian random matrix $A\in \R^{n\times d}$, and set $b=Ax_0$;
\item Solve the following linear programming problem to obtain an optimal point $\hat{x}$,
\begin{eqnarray*}
&\min_x&  \|x\|_1:= \sum |x_i| \\
& s.t. & A x = b,
\end{eqnarray*} 
for example, matlab toolbox {\tt{cvx}} can be an easy solver;
\item Declare success if $\|\hat{x} - x_0\|\leq 10^{-3}$;
\end{enumerate}

After repeating 50 times, compute the success probability $p(n,k)$; draw a figure with x-axis for $k$ and y-axis for $n$, to visualize the success probability. For example, matlab command {\tt{imagesc(p)}} can be a choice. 

Can you try to give an analysis of the phenomenon observed? Tropp's paper mentioned on class may give you a good starting point. 

%\item {\em $^*$Singular Value Decomposition:} The goal of this exercise is to refresh your memory about the singular value decomposition and matrix norms. A good reference to the singular value decomposition is Chapter 2 in this book:\\
%{\em Matrix Computations}, Golub and Van Loan, 3rd edition.\\
%Parts of the book are available online here:\\
%\url{http://www.math.pku.edu.cn/teachers/yaoy/reference/golub.pdf}
%
%\begin{enumerate}
%
%\item {\em Existence:} Prove the existence of the singular value decomposition. That is, show that if $A$ is an $m\times n$ real valued matrix, then $A = U\Sigma V^T$, where $U$ is $m\times m$ orthogonal matrix, $V$ is $n \times n$ orthogonal matrix, and $\Sigma = \text{diag}(\sigma_1,\sigma_2,\ldots,\sigma_p)$ (where $p=\min\{m,n\}$) is an $m\times n$ diagonal matrix. It is customary to order the singular values in decreasing order: $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_p \geq 0$. Determine to what extent the SVD is unique. (See Theorem 2.5.2, page 70 in Golub and Van Loan).
%
%\item {\em Best rank-k approximation - operator norm:} Prove that the ``best" rank-$k$ approximation of a matrix in the operator norm sense is given by its SVD. That is, if $A = U\Sigma V^T$ is the SVD of $A$, then $A_k = U\Sigma_k V^T$ (where $\Sigma_k = \text{diag}(\sigma_1,\sigma_2,\ldots,\sigma_k,0,\ldots,0)$ is a diagonal matrix containing the largest $k$ singular values) is a rank-$k$ matrix that satisfies
%    $$\|A-A_k\| = \min_{\operatorname{rank}(B)=k} \|A-B\|.$$ (Recall that the operator norm of $A$ is $\|A\| = \max_{\|x\|=1} \|Ax\|$. See Theorem 2.5.3 (page 72) in Golub and Van Loan).
%
%\item {\em Best rank-k approximation - Frobenius norm:} Show that the SVD also provides the best rank-$k$ approximation for the Frobenius norm, that is, $A_k = U \Sigma_k V^T$ satisfies $$\|A-A_k\|_F = \min_{\operatorname{rank}(B)=k} \|A-B\|_F.$$

%\item {\em Best rank-k approximation - Schatten p-norms:} A matrix norm $\| \cdot \|$ that satisfies
%$$\|QAZ\| = \|A\|,$$
%for all $Q$ and $Z$ orthogonal matrices is called a unitarily invariant norm. The Schatten $p$-norm of a matrix $A$ is given by the $\ell_p$ norm ($p\geq 1$) of its vector of singular values, namely, $$\|A\|_p = \left(\sum_{i} \sigma_i^p\right)^{1/p}.$$ Show that the Schatten $p$-norm is unitarily invariant. Note that the case $p=1$ is sometimes called the nuclear norm of the matrix, the case $p=2$ is the Frobenius norm, and $p=\infty$ is the operator norm.
%
%\item {\em Best rank-k approximation for unitarily invariant norms:} Show that the SVD provides the best rank-$k$ approximation for any unitarily invariant norm. See also 7.4.51 and 7.4.52 in: {\em Matrix Analysis}, Horn and Johnson, Cambridge University Press, 1985.
%
%\item {\em Closest rotation:} Given a square $n\times n$ matrix $A$ whose SVD is $A=U\Sigma V^T$, show that its closest (in the Frobenius norm) orthogonal matrix $R$ (satisfying $RR^T=R^TR=I$) is given by $R=UV^T$. That is, show that 
%    $$\|A - UV^T\|_F = \min_{RR^T=R^TR=I} \|A-R\|_F,$$ where
%    $A=U\Sigma V^T$.   
%    In other words, $R$ is obtained from the SVD of $A$ by dropping the diagonal matrix $\Sigma$. Use this observation to conclude what is the optimal rotation that aligns two sets of points $p_1,p_2,\ldots,p_n$ and $q_1,\ldots,q_n$ in $\mathbb{R}^d$, that is, find $R$ that minimizes $\sum_{i=1}^n \|Rp_i-q_i\|^2$. See also (the papers are posted on course website):\\
%    
%    $\bullet$ [Arun87] Arun, K. S., Huang, T. S., and Blostein, S. D., ``Least-squares fitting of two 3-D point sets", {\em IEEE Transactions on Pattern Analysis and Machine Intelligence,} {\bf 9} (5), pp. 698--700, 1987.\\
%    
%    $\bullet$ [Keller75] Keller, J. B., ``Closest Unitary, Orthogonal and Hermitian Operators to a Given Operator", {\em Mathematics Magazine}, {\bf 48} (4), pp. 192--197, 1975.\\
%    
%    $\bullet$ [FanHoffman55] Fan, K. and Hoffman, A. J., ``Some Metric Inequalities in the Space of Matrices",
%     {\em Proceedings of the American Mathematical Society},
%     {\bf 6} (1), pp. 111--116, 1955.
%    
%\end{enumerate}
%


\end{enumerate}

\end{document}


